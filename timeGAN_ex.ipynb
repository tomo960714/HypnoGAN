{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neptune'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\98_GitRepo\\00_HypnoGAN\\HypnoGAN\\timeGAN_ex.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/98_GitRepo/00_HypnoGAN/HypnoGAN/timeGAN_ex.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/98_GitRepo/00_HypnoGAN/HypnoGAN/timeGAN_ex.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/98_GitRepo/00_HypnoGAN/HypnoGAN/timeGAN_ex.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mneptune\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnew\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mneptune\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'neptune'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "#local imports\n",
    "\n",
    "#global imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import neptune.new as neptune"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune example:\n",
    "\n",
    "run = neptune.init_run(\n",
    "\n",
    "    project=\"tomo96/HypnoGANex\",\n",
    "    \n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhNGRjNDgzOC04OTk5LTQ0YTktYjQ4Ny1hMTE4NzRjNjBiM2EifQ==\",)  # your credentials\n",
    "\n",
    "params = {\"learning_rate\": 0.001, \"optimizer\": \"Adam\"}\n",
    "\n",
    "run[\"parameters\"] = params\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    run[\"train/loss\"].append(0.9 ** epoch)\n",
    "\n",
    "run[\"eval/f1_score\"] = 0.66\n",
    "\n",
    "run.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomo9\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\neptune\\new\\internal\\utils\\git.py:37: UserWarning: GitPython could not be initialized\n",
      "  warnings.warn(\"GitPython could not be initialized\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/tomo96/HypnoGANex/e/HYPNOG-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info (NVML): NVML Shared Library Not Found. GPU usage metrics may not be reported. For more information, see https://docs.neptune.ai/help/nvml_error/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 22 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 22 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/tomo96/HypnoGANex/e/HYPNOG-1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"tomo96/HypnoGANex\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhNGRjNDgzOC04OTk5LTQ0YTktYjQ4Ny1hMTE4NzRjNjBiM2EifQ==\",\n",
    ")  # your credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeGan implementation\n",
    "# Path: timeGAN_ex.ipynb\n",
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class EmbeddingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    The embedding network (encoder) that maps the input data to a latent space.\n",
    "    \"\"\"\n",
    "    def __init__(self,args):\n",
    "        super(EmbeddingNetwork, self).__init__()\n",
    "        self.feature_dim = args.feature_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.num_layers = args.num_layers\n",
    "        self.padding_value = args.padding_value\n",
    "        self. max_seq_len = args.max_seq_len\n",
    "\n",
    "        #Embedder Architecture\n",
    "        self.emb_rnn = nn.GRU(\n",
    "            input_size=self.feature_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.emb_linear = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.emb_sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "        # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L61\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.emb_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(1)\n",
    "\n",
    "            for name, param in self.emb_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "    def forward(self,X,T):\n",
    "        \"\"\"Forward pass of the embedding features from original space to latent space.\n",
    "        Args:\n",
    "            X: Input time series feature (B x S x F)\n",
    "            T: INput temporal information (B)\n",
    "        Returns:\n",
    "            H: latent space embeddings (B x S x H)\n",
    "        \"\"\"\n",
    "        # Dynamic RNN input for ignoring paddings\n",
    "\n",
    "        X_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input =X,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # 128*100*71\n",
    "        H_o,H_t = self.emb_rnn(X_pack)\n",
    "\n",
    "        #pad RNN output back to sequence length\n",
    "\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "\n",
    "        #128*100*10\n",
    "        logits = self.emb_linear(H_o)\n",
    "        H = self.emb_sigmoid(logits)\n",
    "\n",
    "        return H\n",
    "    \n",
    "class RecoveryNetwork(nn.Module):\n",
    "    \"\"\"The recovery network (decoder) for TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,arg):\n",
    "        super(RecoveryNetwork, self).__init__()\n",
    "        self.hidden_dim = arg.hidden_dim\n",
    "        self.feature_dim = arg.feature_dim\n",
    "        self.num_layers = arg.num_layers\n",
    "        self.padding_value = arg.padding_value\n",
    "        self.max_seq_len = arg.max_seq_len\n",
    "\n",
    "        #Recovery Architecture\n",
    "        self.rec_rnn = nn.GRU(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.rec_linear = nn.Linear(self.hidden_dim, self.feature_dim)\n",
    "\n",
    "        # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name,param in self.rec_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(0)\n",
    "            for name,param in self.rec_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        \n",
    "    def forward(self,H,T):\n",
    "        \"\"\" Forward pass of the recovery features from latent space to original space.\n",
    "        Args:\n",
    "            H: latent representation (B x S x E)\n",
    "            T: input temporal information (B)\n",
    "        Returns:\n",
    "            X_tilde: recovered features (B x S x F)\n",
    "        \"\"\"\n",
    "        #Dynamic RNN input for ignoring paddings\n",
    "        H_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input = H,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        #128 x 100 x 10\n",
    "        H_o,H_t = self.rec_rnn(H_pack)\n",
    "        #pad RNN output back to sequence length\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "        #128 x 100 x 71\n",
    "        X_tilde = self.rec_linear(H_o)\n",
    "        return X_tilde\n",
    "\n",
    "class SupervisorNetwork(nn.Module):\n",
    "        \"\"\"The supervisor network for TimeGAN\n",
    "        \"\"\"\n",
    "        def __init__(self,args):\n",
    "            super(SupervisorNetwork,self).__init__()\n",
    "            self.hidden_dim = args.hidden_dim\n",
    "            self.num_layers = args.num_layers\n",
    "            self.padding_value = args.padding_value\n",
    "            self.max_seq_len = args.max_seq_len\n",
    "\n",
    "            #supervisor architecture\n",
    "            self.sup_rnn = nn.GRU(\n",
    "                input_size=self.hidden_dim,\n",
    "                hidden_size=self.hidden_dim,\n",
    "                num_layers=self.num_layers-1,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            self.sup_linear = nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "            self.sup_sigmoid = nn.Sigmoid()\n",
    "             # Init weights\n",
    "            # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "            # Reference: \n",
    "            # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "            # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "            with torch.no_grad():\n",
    "                for name, param in self.sup_rnn.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'bias_ih' in name:\n",
    "                        param.data.fill_(1)\n",
    "                    elif 'bias_hh' in name:\n",
    "                        param.data.fill_(0)\n",
    "                for name, param in self.sup_linear.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        param.data.fill_(0)\n",
    "        def forward(self,H,T):\n",
    "            \"\"\"Forward pass for the supervisor for predicting next step\n",
    "            Args:\n",
    "                H: latent representation (B x S x E)\n",
    "                T: input temporal information (B)\n",
    "            Returns:\n",
    "                H_hat: predicted next step data (B x S x E)\n",
    "            \"\"\"\n",
    "\n",
    "            #Dynamic RNN input for ignoring paddings\n",
    "            H_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "                input = H,\n",
    "                lengths=T,\n",
    "                batch_first=True,\n",
    "                enforce_sorted=False,\n",
    "            )\n",
    "\n",
    "            H_o,H_t = self.sup_rnn(H_pack)\n",
    "            #pad RNN output back to sequence length\n",
    "            H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "                sequence=H_o,\n",
    "                batch_first=True,\n",
    "                padding_value=self.padding_value,\n",
    "                total_length=self.max_seq_len,\n",
    "            )\n",
    "            logits = self.sup_linear(H_o)\n",
    "            H_hat = self.sup_sigmoid(logits)\n",
    "            return H_hat\n",
    "\n",
    "class GeneratorNetwork(nn.Module):\n",
    "    \"\"\"The generator network for TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,args):\n",
    "        super(GeneratorNetwork,self).__init__()\n",
    "        self.Z_dim = args.Z_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.num_layers = args.num_layers\n",
    "        self.padding_value = args.padding_value\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "\n",
    "        #Generator Architecture\n",
    "        self.gen_rnn = nn.GRU(\n",
    "            input_size=self.Z_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.gen_linear = nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "        self.gen_sigmoid = nn.Sigmoid()\n",
    "                # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.gen_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(0)\n",
    "            for name, param in self.gen_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        \n",
    "    def forward(self,Z,T):\n",
    "        \"\"\" Takes in random noise (features) and generates synthetic features within the last latent space\n",
    "        Args:\n",
    "            Z: input random noise (B x S x Z)\n",
    "            T: input temporal information (B)\n",
    "        Returns:\n",
    "            H: embeddings (B x S x E)\n",
    "        \"\"\"\n",
    "        #Dynamic RNN input for ignoring paddings\n",
    "        Z_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input = Z,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # 128*100*71\n",
    "        H_o,H_t = self.gen_rnn(Z_pack)\n",
    "\n",
    "        #pad RNN output back to sequence length\n",
    "\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "\n",
    "        #128*100*10\n",
    "        logits = self.gen_linear(H_o)\n",
    "        H = self.gen_sigmoid(logits)\n",
    "\n",
    "        return H\n",
    "\n",
    "class DiscriminatorNetwork(nn.Module):\n",
    "    \"\"\"The discriminator network for TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,args):\n",
    "        super(DiscriminatorNetwork,self).__init__()\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.num_layers = args.num_layers\n",
    "        self.padding_value = args.padding_value\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "\n",
    "        #Discriminator Architecture\n",
    "        self.dis_rnn = nn.GRU(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dis_linear = nn.Linear(self.hidden_dim,1)\n",
    "\n",
    "        # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.dis_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(0)\n",
    "            for name, param in self.dis_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "    \n",
    "    def forward(self, H, T):\n",
    "        \"\"\" Forward pass for predicting if the data is real or synthetic\n",
    "        \n",
    "        Args:\n",
    "            H: latent representation (B x S x E)\n",
    "            T: input temporal information (B)\n",
    "        Returns:\n",
    "        logits: prediction logits(B x S x 1)\n",
    "        \"\"\"\n",
    "        # dynamic RNN input for ignoring paddings\n",
    "        H_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input = H,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # 128*100*10\n",
    "        H_o,H_t = self.dis_rnn(H_pack)\n",
    "\n",
    "        # pad RNN output back to sequence length\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "\n",
    "        logits = self.dis_linear(H_o).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "class TimeGAN(nn.Module):\n",
    "    \"\"\" Implementation of TimeGan (Yoon et al., 2019) using PyTorch\n",
    "    \n",
    "    Reference:\n",
    "        - Yoon, J., Jarret, D., van der Schaar, M. (2019). Time-series Generative Adversarial Networks. (https://papers.nips.cc/paper/2019/hash/c9efe5f26cd17ba6216bbe2a7d26d490-Abstract.html)\n",
    "        - https://github.com/jsyoon0823/TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,args):\n",
    "        super(TimeGAN,self).__init__()\n",
    "        self.device =args.device\n",
    "        self.feature_dim = args.feature_dim\n",
    "        self.Z_dim = args.Z_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        self.embedder = EmbeddingNetwork(args)\n",
    "        self.recovery = RecoveryNetwork(args)\n",
    "        self.generator = GeneratorNetwork(args)\n",
    "        self.discriminator = DiscriminatorNetwork(args)\n",
    "        self.supervisor = SupervisorNetwork(args)\n",
    "\n",
    "    def _recovery_forward(self, X, T):\n",
    "        \"\"\" The embedding network forward pass and the embedder network loss\n",
    "        Args:\n",
    "            X: input features\n",
    "            T: input temporal information\n",
    "        Returns:\n",
    "            E_loss: the reconstruction loss\n",
    "            X_tilde: the reconstructed features\n",
    "        \"\"\"\n",
    "\n",
    "        # FOrward pass\n",
    "        H = self.embedder(X,T)\n",
    "        X_tilde = self.recovery(H,T)\n",
    "\n",
    "        #for Joint training\n",
    "        H_hat_supervise = self.supervisor(H,T)\n",
    "        G_loss_S = F.mse_loss(\n",
    "            H_hat_supervise[:,:-1,:],\n",
    "            H[:,1:,:],\n",
    "        ) #Teacher forcing next output\n",
    "\n",
    "        #Reconstruction loss\n",
    "        E_loss_T0 = F.mse_loss(X_tilde,X)\n",
    "        E_loss0 = 10*torch.sqrt(E_loss_T0)\n",
    "        E_loss = E_loss0 + 0.1*G_loss_S\n",
    "        return E_loss, E_loss0,E_loss_T0\n",
    "    def _supervisor_forward(self, X, T):\n",
    "        \"\"\" The supervisor training forward pass\n",
    "        Args:\n",
    "            X: original input features\n",
    "            T: input temporal information\n",
    "        Returns:\n",
    "            S_loss: the supervisor's loss\n",
    "        \"\"\"\n",
    "        #supervisor forward pass\n",
    "        H = self.embedder(X,T)\n",
    "        H_hat_supervise = self.supervisor(H,T)\n",
    "\n",
    "        #supervised loss\n",
    "        S_loss = F.mse_loss(\n",
    "            H_hat_supervise[:,:-1,:],\n",
    "            H[:,1:,:],\n",
    "        ) #Teacher forcing next output\n",
    "        return S_loss\n",
    "    def _discriminator_forward(self, X, T, Z, gamma=1):\n",
    "        \"\"\" The discriminator forward pass and adversarial loss\n",
    "        Args:\n",
    "            X: input features\n",
    "            T: input temporal information\n",
    "            Z: input noise\n",
    "            gamma: the weight for the adversarial loss\n",
    "        Returns:\n",
    "            D_loss: adversarial loss\n",
    "        \"\"\"\n",
    "        #Real\n",
    "        H = self.embedder(X, T).detach()\n",
    "\n",
    "        #generator\n",
    "        E_hat = self.generator(Z,T).detach()\n",
    "        H_hat = self.supervisor(E_hat,T).detach()\n",
    "        \n",
    "        #forward pass\n",
    "        Y_real = self.discriminator(H,T)        #Encode original data\n",
    "        Y_fake = self.discriminator(H_hat,T)    #Output of generator + supervisor\n",
    "        Y_fake_e = self.discriminator(E_hat,T)  #Output of generator\n",
    "\n",
    "        D_loss_real = F.binary_cross_entropy_with_logits(Y_real, torch.ones_like(Y_real))\n",
    "        D_loss_fake = F.binary_cross_entropy_with_logits(Y_fake, torch.zeros_like(Y_fake))\n",
    "        D_loss_fake_e = F.binary_cross_entropy_with_logits(Y_fake_e, torch.zeros_like(Y_fake_e))\n",
    "\n",
    "        D_loss = D_loss_real + D_loss_fake + gamma * D_loss_fake_e\n",
    "\n",
    "        return D_loss\n",
    "    \n",
    "    def _generator_forward(self, X, T, Z, gamma=1):\n",
    "        \"\"\" The generator forward pass\n",
    "        Args:\n",
    "            X: original input features\n",
    "            T: input temporal information\n",
    "            Z: input noise for the generator\n",
    "            gamma: the weight for the adversarial loss\n",
    "        Returns:\n",
    "            G_loss: the generator loss\n",
    "        \"\"\"\n",
    "        #supervisor forward pass\n",
    "        H = self.embedder(X,T)\n",
    "        H_hat_supervise = self.supervisor(H,T)\n",
    "\n",
    "        #generator forward pass\n",
    "        E_hat = self.generator(Z,T)\n",
    "        H_hat = self.supervisor(E_hat,T)\n",
    "\n",
    "        #synthetic data generated\n",
    "        X_hat = self.recovery(H_hat,T)\n",
    "\n",
    "        #generator loss\n",
    "        #Adversarial loss\n",
    "        Y_fake = self.discriminator(H_hat,T)        #Output of supervisor\n",
    "        Y_fake_e = self.discriminator(E_hat,T)      #Output of generator\n",
    "\n",
    "        G_loss_U = F.binary_cross_entropy_with_logits(Y_fake, torch.ones_like(Y_fake))\n",
    "        G_loss_U_e = F.binary_cross_entropy_with_logits(Y_fake_e, torch.ones_like(Y_fake_e))\n",
    "\n",
    "        #Supervised loss\n",
    "        G_loss_S = F.mse_loss(\n",
    "            H_hat_supervise[:,:-1,:],\n",
    "            H[:,1:,:],\n",
    "        ) #Teacher forcing next output\n",
    "\n",
    "        #Two moments losses\n",
    "        G_loss_V1 = torch.mean(\n",
    "            torch.abs(torch.sqrt(X_hat.var(dim=0,unbiased=False)+1e-6) - torch.sqrt(X.var(dim=0,unbiased=False)+1e-6))\n",
    "        )\n",
    "        G_loss_V2 = torch.mean(torch.abs((X_hat.mean(dim=0)) - (X.mean(dim=0))))\n",
    "        G_loss_V = G_loss_V1 + G_loss_V2\n",
    "        \n",
    "        #sum of losses\n",
    "        G_loss = G_loss_U + gamma * G_loss_U_e + 100 * torch.sqrt(G_loss_S) + 100 * G_loss_V\n",
    "    \n",
    "        return G_loss\n",
    "    \n",
    "    def _inference(self, Z,T):\n",
    "        \"\"\" Inference for generating synthetic data\n",
    "        Args:\n",
    "            Z: input noise\n",
    "            T: temporal information\n",
    "        Returns:\n",
    "            X_hat: the generated data\n",
    "        \"\"\"\n",
    "\n",
    "        #generator forward pass\n",
    "        E_hat = self.generator(Z,T)\n",
    "        H_hat = self.supervisor(E_hat,T)\n",
    "\n",
    "        #synthetic data generated\n",
    "        X_hat = self.recovery(H_hat,T)\n",
    "        return X_hat\n",
    "\n",
    "    def forward(self,X,T,Z, obj, gamma=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: input features (B,H,F)\n",
    "            T: The temporal information (B)\n",
    "            Z: the sampled noise (B,H,Z)\n",
    "            obj: the network to be trained ('autoencoder','supervisor','generator','discriminator')\n",
    "            gamma: loss hyperparameter\n",
    "        Returns:\n",
    "            loss: loss for the forward pass\n",
    "            X_hat: the generated data\n",
    "        \"\"\"\n",
    "\n",
    "        #Move variables to device\n",
    "        if obj !='inference':\n",
    "            if X is None:\n",
    "                raise ValueError('X cannot be empty')\n",
    "            \n",
    "            X = torch.FloatTensor(X)\n",
    "            X = X.to(self.device)\n",
    "\n",
    "        if Z is not None:\n",
    "            Z = torch.FloatTensor(Z)\n",
    "            Z = Z.to(self.device)\n",
    "        \n",
    "        if obj == 'autoencoder':\n",
    "            #embedder and recovery forward\n",
    "            loss = self._recovery_forward(X,T)\n",
    "        elif obj == 'supervisor':\n",
    "            loss = self._supervisor_forward(X,T)\n",
    "        elif obj == 'generator':\n",
    "            if Z is None:\n",
    "                raise ValueError('Z cannot be empty')\n",
    "            loss = self._generator_forward(X,T,Z,gamma)\n",
    "        elif obj == 'discriminator':\n",
    "            if Z is None:\n",
    "                raise ValueError('Z cannot be empty')\n",
    "            loss = self._discriminator_forward(X,T,Z,gamma)\n",
    "            return loss\n",
    "        elif obj == 'inference':\n",
    "            X_hat = self._inference(Z,T)\n",
    "            X_hat = X_hat.cpu.detach()\n",
    "\n",
    "            return X_hat\n",
    "        else:\n",
    "            raise ValueError('obj must be autoencoder, supervisor, generator or discriminator')\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class TimeGANDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"TimeGAN Dataset for sampling data with their respective time\n",
    "    Args:\n",
    "        - data (numpy.ndarray): the padded dataset to be fitted (D x S x F)\n",
    "        - time (numpy.ndarray): the length of each data (D)\n",
    "    Parameters:\n",
    "        - x (torch.FloatTensor): the real value features of the data\n",
    "        - t (torch.LongTensor): the temporal feature of the data \n",
    "    \"\"\"\n",
    "    def __init__(self, data, time=None, padding_value=None):\n",
    "        # sanity check\n",
    "        if len(data) != len(time):\n",
    "            raise ValueError(\n",
    "                f\"len(data) `{len(data)}` != len(time) {len(time)}\"\n",
    "            )\n",
    "\n",
    "        if isinstance(time, type(None)):\n",
    "            time = [len(x) for x in data]\n",
    "\n",
    "        self.X = torch.FloatTensor(data)\n",
    "        self.T = torch.LongTensor(time)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.T[idx]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Minibatch sampling\n",
    "        \"\"\"\n",
    "        # Pad sequences to max length\n",
    "        X_mb = [X for X in batch[0]]\n",
    "        \n",
    "        # The actual length of each data\n",
    "        T_mb = [T for T in batch[1]]\n",
    "        \n",
    "        return X_mb, T_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local modules\n",
    "import os\n",
    "import pickle\n",
    "from typing import Dict, Union\n",
    "\n",
    "# 3rd party modules\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Self-written modules\n",
    "from models.dataset import TimeGANDataset\n",
    "\n",
    "def embedding_trainer(\n",
    "    model: torch.nn.Module, \n",
    "    dataloader: torch.utils.data.DataLoader, \n",
    "    e_opt: torch.optim.Optimizer, \n",
    "    r_opt: torch.optim.Optimizer, \n",
    "    args: Dict, \n",
    "    writer: Union[torch.utils.tensorboard.SummaryWriter, type(None)]=None\n",
    ") -> None:\n",
    "    \"\"\"The training loop for the embedding and recovery functions\n",
    "    \"\"\"  \n",
    "    logger = trange(args.emb_epochs, desc=f\"Epoch: 0, Loss: 0\")\n",
    "    for epoch in logger:   \n",
    "        for X_mb, T_mb in dataloader:\n",
    "            # Reset gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Forward Pass\n",
    "            # time = [args.max_seq_len for _ in range(len(T_mb))]\n",
    "            _, E_loss0, E_loss_T0 = model(X=X_mb, T=T_mb, Z=None, obj=\"autoencoder\")\n",
    "            loss = np.sqrt(E_loss_T0.item())\n",
    "\n",
    "            # Backward Pass\n",
    "            E_loss0.backward()\n",
    "\n",
    "            # Update model parameters\n",
    "            e_opt.step()\n",
    "            r_opt.step()\n",
    "\n",
    "        # Log loss for final batch of each epoch (29 iters)\n",
    "        logger.set_description(f\"Epoch: {epoch}, Loss: {loss:.4f}\")\n",
    "        if writer:\n",
    "            writer.add_scalar(\n",
    "                \"Embedding/Loss:\", \n",
    "                loss, \n",
    "                epoch\n",
    "            )\n",
    "            writer.flush()\n",
    "\n",
    "def supervisor_trainer(\n",
    "    model: torch.nn.Module, \n",
    "    dataloader: torch.utils.data.DataLoader, \n",
    "    s_opt: torch.optim.Optimizer, \n",
    "    g_opt: torch.optim.Optimizer, \n",
    "    args: Dict, \n",
    "    writer: Union[torch.utils.tensorboard.SummaryWriter, type(None)]=None\n",
    ") -> None:\n",
    "    \"\"\"The training loop for the supervisor function\n",
    "    \"\"\"\n",
    "    logger = trange(args.sup_epochs, desc=f\"Epoch: 0, Loss: 0\")\n",
    "    for epoch in logger:\n",
    "        for X_mb, T_mb in dataloader:\n",
    "            # Reset gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Forward Pass\n",
    "            S_loss = model(X=X_mb, T=T_mb, Z=None, obj=\"supervisor\")\n",
    "\n",
    "            # Backward Pass\n",
    "            S_loss.backward()\n",
    "            loss = np.sqrt(S_loss.item())\n",
    "\n",
    "            # Update model parameters\n",
    "            s_opt.step()\n",
    "\n",
    "        # Log loss for final batch of each epoch (29 iters)\n",
    "        logger.set_description(f\"Epoch: {epoch}, Loss: {loss:.4f}\")\n",
    "        if writer:\n",
    "            writer.add_scalar(\n",
    "                \"Supervisor/Loss:\", \n",
    "                loss, \n",
    "                epoch\n",
    "            )\n",
    "            writer.flush()\n",
    "\n",
    "def joint_trainer(\n",
    "    model: torch.nn.Module, \n",
    "    dataloader: torch.utils.data.DataLoader, \n",
    "    e_opt: torch.optim.Optimizer, \n",
    "    r_opt: torch.optim.Optimizer, \n",
    "    s_opt: torch.optim.Optimizer, \n",
    "    g_opt: torch.optim.Optimizer, \n",
    "    d_opt: torch.optim.Optimizer, \n",
    "    args: Dict, \n",
    "    writer: Union[torch.utils.tensorboard.SummaryWriter, type(None)]=None, \n",
    ") -> None:\n",
    "    \"\"\"The training loop for training the model altogether\n",
    "    \"\"\"\n",
    "    logger = trange(\n",
    "        args.sup_epochs, \n",
    "        desc=f\"Epoch: 0, E_loss: 0, G_loss: 0, D_loss: 0\"\n",
    "    )\n",
    "    \n",
    "    for epoch in logger:\n",
    "        for X_mb, T_mb in dataloader:\n",
    "            ## Generator Training\n",
    "            for _ in range(2):\n",
    "                # Random Generator\n",
    "                Z_mb = torch.rand((args.batch_size, args.max_seq_len, args.Z_dim))\n",
    "\n",
    "                # Forward Pass (Generator)\n",
    "                model.zero_grad()\n",
    "                G_loss = model(X=X_mb, T=T_mb, Z=Z_mb, obj=\"generator\")\n",
    "                G_loss.backward()\n",
    "                G_loss = np.sqrt(G_loss.item())\n",
    "\n",
    "                # Update model parameters\n",
    "                g_opt.step()\n",
    "                s_opt.step()\n",
    "\n",
    "                # Forward Pass (Embedding)\n",
    "                model.zero_grad()\n",
    "                E_loss, _, E_loss_T0 = model(X=X_mb, T=T_mb, Z=Z_mb, obj=\"autoencoder\")\n",
    "                E_loss.backward()\n",
    "                E_loss = np.sqrt(E_loss.item())\n",
    "                \n",
    "                # Update model parameters\n",
    "                e_opt.step()\n",
    "                r_opt.step()\n",
    "\n",
    "            # Random Generator\n",
    "            Z_mb = torch.rand((args.batch_size, args.max_seq_len, args.Z_dim))\n",
    "\n",
    "            ## Discriminator Training\n",
    "            model.zero_grad()\n",
    "            # Forward Pass\n",
    "            D_loss = model(X=X_mb, T=T_mb, Z=Z_mb, obj=\"discriminator\")\n",
    "\n",
    "            # Check Discriminator loss\n",
    "            if D_loss > args.dis_thresh:\n",
    "                # Backward Pass\n",
    "                D_loss.backward()\n",
    "\n",
    "                # Update model parameters\n",
    "                d_opt.step()\n",
    "            D_loss = D_loss.item()\n",
    "\n",
    "        logger.set_description(\n",
    "            f\"Epoch: {epoch}, E: {E_loss:.4f}, G: {G_loss:.4f}, D: {D_loss:.4f}\"\n",
    "        )\n",
    "        if writer:\n",
    "            writer.add_scalar(\n",
    "                'Joint/Embedding_Loss:', \n",
    "                E_loss, \n",
    "                epoch\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                'Joint/Generator_Loss:', \n",
    "                G_loss, \n",
    "                epoch\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                'Joint/Discriminator_Loss:', \n",
    "                D_loss, \n",
    "                epoch\n",
    "            )\n",
    "            writer.flush()\n",
    "\n",
    "def timegan_trainer(model, data, time, args):\n",
    "    \"\"\"The training procedure for TimeGAN\n",
    "    Args:\n",
    "        - model (torch.nn.module): The model model that generates synthetic data\n",
    "        - data (numpy.ndarray): The data for training the model\n",
    "        - time (numpy.ndarray): The time for the model to be conditioned on\n",
    "        - args (dict): The model/training configurations\n",
    "    Returns:\n",
    "        - generated_data (np.ndarray): The synthetic data generated by the model\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize TimeGAN dataset and dataloader\n",
    "    dataset = TimeGANDataset(data, time)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False    \n",
    "    )\n",
    "\n",
    "    model.to(args.device)\n",
    "\n",
    "    # Initialize Optimizers\n",
    "    e_opt = torch.optim.Adam(model.embedder.parameters(), lr=args.learning_rate)\n",
    "    r_opt = torch.optim.Adam(model.recovery.parameters(), lr=args.learning_rate)\n",
    "    s_opt = torch.optim.Adam(model.supervisor.parameters(), lr=args.learning_rate)\n",
    "    g_opt = torch.optim.Adam(model.generator.parameters(), lr=args.learning_rate)\n",
    "    d_opt = torch.optim.Adam(model.discriminator.parameters(), lr=args.learning_rate)\n",
    "    \n",
    "    # TensorBoard writer\n",
    "    writer = SummaryWriter(os.path.join(f\"tensorboard/{args.exp}\"))\n",
    "\n",
    "    print(\"\\nStart Embedding Network Training\")\n",
    "    embedding_trainer(\n",
    "        model=model, \n",
    "        dataloader=dataloader, \n",
    "        e_opt=e_opt, \n",
    "        r_opt=r_opt, \n",
    "        args=args, \n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "    print(\"\\nStart Training with Supervised Loss Only\")\n",
    "    supervisor_trainer(\n",
    "        model=model,\n",
    "        dataloader=dataloader,\n",
    "        s_opt=s_opt,\n",
    "        g_opt=g_opt,\n",
    "        args=args,\n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "    print(\"\\nStart Joint Training\")\n",
    "    joint_trainer(\n",
    "        model=model,\n",
    "        dataloader=dataloader,\n",
    "        e_opt=e_opt,\n",
    "        r_opt=r_opt,\n",
    "        s_opt=s_opt,\n",
    "        g_opt=g_opt,\n",
    "        d_opt=d_opt,\n",
    "        args=args,\n",
    "        writer=writer,\n",
    "    )\n",
    "\n",
    "    # Save model, args, and hyperparameters\n",
    "    torch.save(args, f\"{args.model_path}/args.pickle\")\n",
    "    torch.save(model.state_dict(), f\"{args.model_path}/model.pt\")\n",
    "    print(f\"\\nSaved at path: {args.model_path}\")\n",
    "\n",
    "def timegan_generator(model, T, args):\n",
    "    \"\"\"The inference procedure for TimeGAN\n",
    "    Args:\n",
    "        - model (torch.nn.module): The model model that generates synthetic data\n",
    "        - T (List[int]): The time to be generated on\n",
    "        - args (dict): The model/training configurations\n",
    "    Returns:\n",
    "        - generated_data (np.ndarray): The synthetic data generated by the model\n",
    "    \"\"\"\n",
    "    # Load model for inference\n",
    "    if not os.path.exists(args.model_path):\n",
    "        raise ValueError(f\"Model directory not found...\")\n",
    "\n",
    "    # Load arguments and model\n",
    "    with open(f\"{args.model_path}/args.pickle\", \"rb\") as fb:\n",
    "        args = torch.load(fb)\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"{args.model_path}/model.pt\"))\n",
    "    \n",
    "    print(\"\\nGenerating Data...\")\n",
    "    # Initialize model to evaluation mode and run without gradients\n",
    "    model.to(args.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Generate fake data\n",
    "        Z = torch.rand((len(T), args.max_seq_len, args.Z_dim))\n",
    "        \n",
    "        generated_data = model(X=None, T=T, Z=Z, obj=\"inference\")\n",
    "\n",
    "    return generated_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data preprocessing\n",
    "\"\"\"Hide-and-Seek Privacy Challenge Codebase.\n",
    "Reference: James Jordon, Daniel Jarrett, Jinsung Yoon, Ari Ercole, Cheng Zhang, Danielle Belgrave, Mihaela van der Schaar, \n",
    "\"Hide-and-Seek Privacy Challenge: Synthetic Data Generation vs. Patient Re-identification with Clinical Time-series Data,\" \n",
    "Neural Information Processing Systems (NeurIPS) Competition, 2020.\n",
    "Link: https://www.vanderschaar-lab.com/announcing-the-neurips-2020-hide-and-seek-privacy-challenge/\n",
    "Last updated Date: Oct 17th 2020\n",
    "Code author: Jinsung Yoon, Evgeny Saveliev\n",
    "Contact: jsyoon0823@gmail.com, e.s.saveliev@gmail.com\n",
    "-----------------------------\n",
    "(1) data_preprocess: Load the data and preprocess into a 3d numpy array\n",
    "(2) imputater: Impute missing data \n",
    "\"\"\"\n",
    "# Local packages\n",
    "import os\n",
    "from typing import Union, Tuple, List\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 3rd party modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def data_preprocess(\n",
    "    file_name: str, \n",
    "    max_seq_len: int, \n",
    "    padding_value: float=-1.0,\n",
    "    impute_method: str=\"mode\", \n",
    "    scaling_method: str=\"minmax\", \n",
    ") -> Tuple[np.ndarray, np.ndarray, List]:\n",
    "    \"\"\"Load the data and preprocess into 3d numpy array.\n",
    "    Preprocessing includes:\n",
    "    1. Remove outliers\n",
    "    2. Extract sequence length for each patient id\n",
    "    3. Impute missing data \n",
    "    4. Normalize data\n",
    "    6. Sort dataset according to sequence length\n",
    "    Args:\n",
    "    - file_name (str): CSV file name\n",
    "    - max_seq_len (int): maximum sequence length\n",
    "    - impute_method (str): The imputation method (\"median\" or \"mode\") \n",
    "    - scaling_method (str): The scaler method (\"standard\" or \"minmax\")\n",
    "    Returns:\n",
    "    - processed_data: preprocessed data\n",
    "    - time: ndarray of ints indicating the length for each data\n",
    "    - params: the parameters to rescale the data \n",
    "    \"\"\"\n",
    "\n",
    "    #########################\n",
    "    # Load data\n",
    "    #########################\n",
    "\n",
    "    index = 'Idx'\n",
    "\n",
    "    # Load csv\n",
    "    print(\"Loading data...\\n\")\n",
    "    ori_data = pd.read_csv(file_name)\n",
    "\n",
    "    # Remove spurious column, so that column 0 is now 'admissionid'.\n",
    "    if ori_data.columns[0] == \"Unnamed: 0\":  \n",
    "        ori_data = ori_data.drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "    #########################\n",
    "    # Remove outliers from dataset\n",
    "    #########################\n",
    "    \n",
    "    no = ori_data.shape[0]\n",
    "    z_scores = stats.zscore(ori_data, axis=0, nan_policy='omit')\n",
    "    z_filter = np.nanmax(np.abs(z_scores), axis=1) < 3\n",
    "    ori_data = ori_data[z_filter]\n",
    "    print(f\"Dropped {no - ori_data.shape[0]} rows (outliers)\\n\")\n",
    "\n",
    "    # Parameters\n",
    "    uniq_id = np.unique(ori_data[index])\n",
    "    no = len(uniq_id)\n",
    "    dim = len(ori_data.columns) - 1\n",
    "\n",
    "    #########################\n",
    "    # Impute, scale and pad data\n",
    "    #########################\n",
    "    \n",
    "    # Initialize scaler\n",
    "    if scaling_method == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(ori_data)\n",
    "        params = [scaler.data_min_, scaler.data_max_]\n",
    "    \n",
    "    elif scaling_method == \"standard\":\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(ori_data)\n",
    "        params = [scaler.mean_, scaler.var_]\n",
    "\n",
    "    # Imputation values\n",
    "    if impute_method == \"median\":\n",
    "        impute_vals = ori_data.median()\n",
    "    elif impute_method == \"mode\":\n",
    "        impute_vals = stats.mode(ori_data).mode[0]\n",
    "    else:\n",
    "        raise ValueError(\"Imputation method should be `median` or `mode`\")    \n",
    "\n",
    "    # TODO: Sanity check for padding value\n",
    "    # if np.any(ori_data == padding_value):\n",
    "    #     print(f\"Padding value `{padding_value}` found in data\")\n",
    "    #     padding_value = np.nanmin(ori_data.to_numpy()) - 1\n",
    "    #     print(f\"Changed padding value to: {padding_value}\\n\")\n",
    "    \n",
    "    # Output initialization\n",
    "    output = np.empty([no, max_seq_len, dim])  # Shape:[no, max_seq_len, dim]\n",
    "    output.fill(padding_value)\n",
    "    time = []\n",
    "\n",
    "    # For each uniq id\n",
    "    for i in tqdm(range(no)):\n",
    "        # Extract the time-series data with a certain admissionid\n",
    "\n",
    "        curr_data = ori_data[ori_data[index] == uniq_id[i]].to_numpy()\n",
    "\n",
    "        # Impute missing data\n",
    "        curr_data = imputer(curr_data, impute_vals)\n",
    "\n",
    "        # Normalize data\n",
    "        curr_data = scaler.transform(curr_data)\n",
    "        \n",
    "        # Extract time and assign to the preprocessed data (Excluding ID)\n",
    "        curr_no = len(curr_data)\n",
    "\n",
    "        # Pad data to `max_seq_len`\n",
    "        if curr_no >= max_seq_len:\n",
    "            output[i, :, :] = curr_data[:max_seq_len, 1:]  # Shape: [1, max_seq_len, dim]\n",
    "            time.append(max_seq_len)\n",
    "        else:\n",
    "            output[i, :curr_no, :] = curr_data[:, 1:]  # Shape: [1, max_seq_len, dim]\n",
    "            time.append(curr_no)\n",
    "\n",
    "    return output, time, params, max_seq_len, padding_value\n",
    "\n",
    "def imputer(\n",
    "    curr_data: np.ndarray, \n",
    "    impute_vals: List, \n",
    "    zero_fill: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Impute missing data given values for each columns.\n",
    "    Args:\n",
    "        curr_data (np.ndarray): Data before imputation.\n",
    "        impute_vals (list): Values to be filled for each column.\n",
    "        zero_fill (bool, optional): Whather to Fill with zeros the cases where \n",
    "            impute_val is nan. Defaults to True.\n",
    "    Returns:\n",
    "        np.ndarray: Imputed data.\n",
    "    \"\"\"\n",
    "\n",
    "    curr_data = pd.DataFrame(data=curr_data)\n",
    "    impute_vals = pd.Series(impute_vals)\n",
    "    \n",
    "    # Impute data\n",
    "    imputed_data = curr_data.fillna(impute_vals)\n",
    "\n",
    "    # Zero-fill, in case the `impute_vals` for a particular feature is `nan`.\n",
    "    imputed_data = imputed_data.fillna(0.0)\n",
    "\n",
    "    # Check for any N/A values\n",
    "    if imputed_data.isnull().any().any():\n",
    "        raise ValueError(\"NaN values remain after imputation\")\n",
    "\n",
    "    return imputed_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local modules\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# 3rd-Party Modules\n",
    "import numpy as np\n",
    "import torch\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Self-Written Modules\n",
    "from data.data_preprocess import data_preprocess\n",
    "from metrics.metric_utils import (\n",
    "    feature_prediction, one_step_ahead_prediction, reidentify_score\n",
    ")\n",
    "\n",
    "from models.timegan import TimeGAN\n",
    "from models.utils import timegan_trainer, timegan_generator\n",
    "\n",
    "def main(args):\n",
    "    ##############################################\n",
    "    # Initialize output directories\n",
    "    ##############################################\n",
    "\n",
    "    ## Runtime directory\n",
    "    code_dir = os.path.abspath(\".\")\n",
    "    if not os.path.exists(code_dir):\n",
    "        raise ValueError(f\"Code directory not found at {code_dir}.\")\n",
    "\n",
    "    ## Data directory\n",
    "    data_path = os.path.abspath(\"./data\")\n",
    "    if not os.path.exists(data_path):\n",
    "        raise ValueError(f\"Data file not found at {data_path}.\")\n",
    "    data_dir = os.path.dirname(data_path)\n",
    "    data_file_name = os.path.basename(data_path)\n",
    "\n",
    "    ## Output directories\n",
    "    args.model_path = os.path.abspath(f\"./output/{args.exp}/\")\n",
    "    out_dir = os.path.abspath(args.model_path)\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # TensorBoard directory\n",
    "    tensorboard_path = os.path.abspath(\"./tensorboard\")\n",
    "    if not os.path.exists(tensorboard_path):\n",
    "        os.makedirs(tensorboard_path, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nCode directory:\\t\\t\\t{code_dir}\")\n",
    "    print(f\"Data directory:\\t\\t\\t{data_path}\")\n",
    "    print(f\"Output directory:\\t\\t{out_dir}\")\n",
    "    print(f\"TensorBoard directory:\\t\\t{tensorboard_path}\\n\")\n",
    "\n",
    "    ##############################################\n",
    "    # Initialize random seed and CUDA\n",
    "    ##############################################\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    if args.device == \"cuda\" and torch.cuda.is_available():\n",
    "        print(\"Using CUDA\\n\")\n",
    "        args.device = torch.device(\"cuda:0\")\n",
    "        # torch.cuda.manual_seed_all(args.seed)\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    else:\n",
    "        print(\"Using CPU\\n\")\n",
    "        args.device = torch.device(\"cpu\")\n",
    "\n",
    "    #########################\n",
    "    # Load and preprocess data for model\n",
    "    #########################\n",
    "\n",
    "    data_path = \"data/stock.csv\"\n",
    "    X, T, _, args.max_seq_len, args.padding_value = data_preprocess(\n",
    "        data_path, args.max_seq_len\n",
    "    )\n",
    "\n",
    "    print(f\"Processed data: {X.shape} (Idx x MaxSeqLen x Features)\\n\")\n",
    "    print(f\"Original data preview:\\n{X[:2, :10, :2]}\\n\")\n",
    "\n",
    "    args.feature_dim = X.shape[-1]\n",
    "    args.Z_dim = X.shape[-1]\n",
    "\n",
    "    # Train-Test Split data and time\n",
    "    train_data, test_data, train_time, test_time = train_test_split(\n",
    "        X, T, test_size=args.train_rate, random_state=args.seed\n",
    "    )\n",
    "\n",
    "    #########################\n",
    "    # Initialize and Run model\n",
    "    #########################\n",
    "\n",
    "    # Log start time\n",
    "    start = time.time()\n",
    "\n",
    "    model = TimeGAN(args)\n",
    "    if args.is_train == True:\n",
    "        timegan_trainer(model, train_data, train_time, args)\n",
    "    generated_data = timegan_generator(model, train_time, args)\n",
    "    generated_time = train_time\n",
    "\n",
    "    # Log end time\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Generated data preview:\\n{generated_data[:2, -10:, :2]}\\n\")\n",
    "    print(f\"Model Runtime: {(end - start)/60} mins\\n\")\n",
    "\n",
    "    #########################\n",
    "    # Save train and generated data for visualization\n",
    "    #########################\n",
    "    \n",
    "    # Save splitted data and generated data\n",
    "    with open(f\"{args.model_path}/train_data.pickle\", \"wb\") as fb:\n",
    "        pickle.dump(train_data, fb)\n",
    "    with open(f\"{args.model_path}/train_time.pickle\", \"wb\") as fb:\n",
    "        pickle.dump(train_time, fb)\n",
    "    with open(f\"{args.model_path}/test_data.pickle\", \"wb\") as fb:\n",
    "        pickle.dump(test_data, fb)\n",
    "    with open(f\"{args.model_path}/test_time.pickle\", \"wb\") as fb:\n",
    "        pickle.dump(test_time, fb)\n",
    "    with open(f\"{args.model_path}/fake_data.pickle\", \"wb\") as fb:\n",
    "        pickle.dump(generated_data, fb)\n",
    "    with open(f\"{args.model_path}/fake_time.pickle\", \"wb\") as fb:\n",
    "        pickle.dump(generated_time, fb)\n",
    "\n",
    "    #########################\n",
    "    # Preprocess data for seeker\n",
    "    #########################\n",
    "\n",
    "    # Define enlarge data and its labels\n",
    "    enlarge_data = np.concatenate((train_data, test_data), axis=0)\n",
    "    enlarge_time = np.concatenate((train_time, test_time), axis=0)\n",
    "    enlarge_data_label = np.concatenate((np.ones([train_data.shape[0], 1]), np.zeros([test_data.shape[0], 1])), axis=0)\n",
    "\n",
    "    # Mix the order\n",
    "    idx = np.random.permutation(enlarge_data.shape[0])\n",
    "    enlarge_data = enlarge_data[idx]\n",
    "    enlarge_data_label = enlarge_data_label[idx]\n",
    "\n",
    "    #########################\n",
    "    # Evaluate the performance\n",
    "    #########################\n",
    "\n",
    "    # 1. Feature prediction\n",
    "    feat_idx = np.random.permutation(train_data.shape[2])[:args.feat_pred_no]\n",
    "    print(\"Running feature prediction using original data...\")\n",
    "    ori_feat_pred_perf = feature_prediction(\n",
    "        (train_data, train_time), \n",
    "        (test_data, test_time),\n",
    "        feat_idx\n",
    "    )\n",
    "    print(\"Running feature prediction using generated data...\")\n",
    "    new_feat_pred_perf = feature_prediction(\n",
    "        (generated_data, generated_time),\n",
    "        (test_data, test_time),\n",
    "        feat_idx\n",
    "    )\n",
    "\n",
    "    feat_pred = [ori_feat_pred_perf, new_feat_pred_perf]\n",
    "\n",
    "    print('Feature prediction results:\\n' +\n",
    "          f'(1) Ori: {str(np.round(ori_feat_pred_perf, 4))}\\n' +\n",
    "          f'(2) New: {str(np.round(new_feat_pred_perf, 4))}\\n')\n",
    "\n",
    "    # 2. One step ahead prediction\n",
    "    print(\"Running one step ahead prediction using original data...\")\n",
    "    ori_step_ahead_pred_perf = one_step_ahead_prediction(\n",
    "        (train_data, train_time), \n",
    "        (test_data, test_time)\n",
    "    )\n",
    "    print(\"Running one step ahead prediction using generated data...\")\n",
    "    new_step_ahead_pred_perf = one_step_ahead_prediction(\n",
    "        (generated_data, generated_time),\n",
    "        (test_data, test_time)\n",
    "    )\n",
    "\n",
    "    step_ahead_pred = [ori_step_ahead_pred_perf, new_step_ahead_pred_perf]\n",
    "\n",
    "    print('One step ahead prediction results:\\n' +\n",
    "          f'(1) Ori: {str(np.round(ori_step_ahead_pred_perf, 4))}\\n' +\n",
    "          f'(2) New: {str(np.round(new_step_ahead_pred_perf, 4))}\\n')\n",
    "\n",
    "    print(f\"Total Runtime: {(time.time() - start)/60} mins\\n\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "       return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Inputs for the main function\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Experiment Arguments\n",
    "    parser.add_argument(\n",
    "        '--device',\n",
    "        choices=['cuda', 'cpu'],\n",
    "        default='cuda',\n",
    "        type=str)\n",
    "    parser.add_argument(\n",
    "        '--exp',\n",
    "        default='test',\n",
    "        type=str)\n",
    "    parser.add_argument(\n",
    "        \"--is_train\",\n",
    "        type=str2bool,\n",
    "        default=True)\n",
    "    parser.add_argument(\n",
    "        '--seed',\n",
    "        default=0,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--feat_pred_no',\n",
    "        default=2,\n",
    "        type=int)\n",
    "\n",
    "    # Data Arguments\n",
    "    parser.add_argument(\n",
    "        '--max_seq_len',\n",
    "        default=100,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--train_rate',\n",
    "        default=0.5,\n",
    "        type=float)\n",
    "\n",
    "    # Model Arguments\n",
    "    parser.add_argument(\n",
    "        '--emb_epochs',\n",
    "        default=600,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--sup_epochs',\n",
    "        default=600,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--gan_epochs',\n",
    "        default=600,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        default=128,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--hidden_dim',\n",
    "        default=20,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--num_layers',\n",
    "        default=3,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--dis_thresh',\n",
    "        default=0.15,\n",
    "        type=float)\n",
    "    parser.add_argument(\n",
    "        '--optimizer',\n",
    "        choices=['adam'],\n",
    "        default='adam',\n",
    "        type=str)\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        default=1e-3,\n",
    "        type=float)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Call main function\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84466edc4e1d5c241a20da639ce3a41433db7b9d4de6bf8f0f6eb53778c45d81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
