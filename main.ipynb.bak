{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/NTLAB/HypnoGAN/e/HYPNOG-46\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "# Local packages:\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "# 3rd party packages:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm,trange\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilenames\n",
    "import ast \n",
	"import matplotlib.pyplotas plt\n"
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import neptune\n",
    "from neptune.types import File\n",
    "run = neptune.init_run(\n",
    "    project=\"NTLAB/HypnoGAN\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhNGRjNDgzOC04OTk5LTQ0YTktYjQ4Ny1hMTE4NzRjNjBiM2EifQ==\",\n",
    ")\n",
    "\n",
    "\n",
    "# personal packages:\n",
    "#from Data.preprocess import preprocess_data\n",
    "#from model.timegan import TimeGAN\n",
    "#from model.utils import timegan_trainer, timegan_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeGAN_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"A time series dataset for TimeGAN.\n",
    "    Args:\n",
    "        data(numpy.ndarray): the padded dataset to be fitted. Has to transform to ndarray from DataFrame during initialize\n",
    "        time(numpy.ndarray): the length of each data\n",
    "    Parameters:\n",
    "        - x (torch.FloatTensor): the real value features of the data\n",
    "        - t (torch.LongTensor): the temporal feature of the data\n",
    "    \"\"\"\n",
    "    def __init__(self,data, time=None):\n",
    "        #sanity check data and time\n",
    "        \n",
    "        \n",
    "        \n",
    "        if isinstance(time,type(None)):\n",
    "            time = [len(x) for x in data]\n",
    "            \n",
    "        if len(data) != len(time):\n",
    "            run.stop()\n",
    "            raise ValueError( f\"len(data) `{len(data)}` != len(time) {len(time)}\")\n",
    "            \n",
    "        \n",
    "        self.X = torch.FloatTensor(data)\n",
    "        self.T = torch.LongTensor(time)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx],self.T[idx]\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Minibatch sampling\n",
    "        \"\"\"\n",
    "        # Pad sequences to max length\n",
    "        X_mb = [X for X in batch[0]]\n",
    "        \n",
    "        # The actual length of each data\n",
    "        T_mb = [T for T in batch[1]]\n",
    "        \n",
    "        return X_mb, T_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    The embedding network (encoder) that maps the input data to a latent space.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, hidden_dim, num_layers, padding_value=0, max_seq_len=1000):\n",
    "        super(EmbeddingNetwork, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.padding_value = padding_value\n",
    "        self. max_seq_len = max_seq_len\n",
    "\n",
    "        #Embedder Architecture\n",
    "        self.emb_rnn = nn.GRU(\n",
    "            input_size=self.feature_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.emb_linear = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.emb_sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "        # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L61\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.emb_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(1)\n",
    "\n",
    "            for name, param in self.emb_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "    def forward(self,X,T):\n",
    "        \"\"\"Forward pass of the embedding features from original space to latent space.\n",
    "        Args:\n",
    "            X: Input time series feature (B x S x F)\n",
    "            T: INput temporal information (B)\n",
    "        Returns:\n",
    "            H: latent space embeddings (B x S x H)\n",
    "        \"\"\"\n",
    "        # Dynamic RNN input for ignoring paddings\n",
    "\n",
    "        X_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input =X,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # 128*100*71\n",
    "        H_o,H_t = self.emb_rnn(X_pack)\n",
    "\n",
    "        #pad RNN output back to sequence length\n",
    "\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "\n",
    "        #128*100*10\n",
    "        logits = self.emb_linear(H_o)\n",
    "        H = self.emb_sigmoid(logits)\n",
    "\n",
    "        return H\n",
    "    \n",
    "class RecoveryNetwork(nn.Module):\n",
    "    \"\"\"The recovery network (decoder) for TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,hidden_dim,feature_dim,num_layers,padding_value=0,max_seq_len=1000):\n",
    "        super(RecoveryNetwork, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.padding_value = padding_value\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        #Recovery Architecture\n",
    "        self.rec_rnn = nn.GRU(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.rec_linear = nn.Linear(self.hidden_dim, self.feature_dim)\n",
    "\n",
    "        # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name,param in self.rec_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(0)\n",
    "            for name,param in self.rec_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        \n",
    "    def forward(self,H,T):\n",
    "        \"\"\" Forward pass of the recovery features from latent space to original space.\n",
    "        Args:\n",
    "            H: latent representation (B x S x E)\n",
    "            T: input temporal information (B)\n",
    "        Returns:\n",
    "            X_tilde: recovered features (B x S x F)\n",
    "        \"\"\"\n",
    "        #Dynamic RNN input for ignoring paddings\n",
    "        H_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input = H,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        #128 x 100 x 10\n",
    "        H_o,H_t = self.rec_rnn(H_pack)\n",
    "        #pad RNN output back to sequence length\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "        #128 x 100 x 71\n",
    "        X_tilde = self.rec_linear(H_o)\n",
    "        return X_tilde\n",
    "\n",
    "class SupervisorNetwork(nn.Module):\n",
    "        \"\"\"The supervisor network for TimeGAN\n",
    "        \"\"\"\n",
    "        def __init__(self,hidden_dim,num_layers,padding_value=0,max_seq_len=1000):\n",
    "            super(SupervisorNetwork,self).__init__()\n",
    "            self.hidden_dim =hidden_dim\n",
    "            self.num_layers = num_layers\n",
    "            self.padding_value = padding_value\n",
    "            self.max_seq_len = max_seq_len\n",
    "\n",
    "            #supervisor architecture\n",
    "            self.sup_rnn = nn.GRU(\n",
    "                input_size=self.hidden_dim,\n",
    "                hidden_size=self.hidden_dim,\n",
    "                num_layers=self.num_layers-1,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            self.sup_linear = nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "            self.sup_sigmoid = nn.Sigmoid()\n",
    "             # Init weights\n",
    "            # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "            # Reference: \n",
    "            # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "            # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "            with torch.no_grad():\n",
    "                for name, param in self.sup_rnn.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'bias_ih' in name:\n",
    "                        param.data.fill_(1)\n",
    "                    elif 'bias_hh' in name:\n",
    "                        param.data.fill_(0)\n",
    "                for name, param in self.sup_linear.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        param.data.fill_(0)\n",
    "        def forward(self,H,T):\n",
    "            \"\"\"Forward pass for the supervisor for predicting next step\n",
    "            Args:\n",
    "                H: latent representation (B x S x E)\n",
    "                T: input temporal information (B)\n",
    "            Returns:\n",
    "                H_hat: predicted next step data (B x S x E)\n",
    "            \"\"\"\n",
    "\n",
    "            #Dynamic RNN input for ignoring paddings\n",
    "            H_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "                input = H,\n",
    "                lengths=T,\n",
    "                batch_first=True,\n",
    "                enforce_sorted=False,\n",
    "            )\n",
    "\n",
    "            H_o,H_t = self.sup_rnn(H_pack)\n",
    "            #pad RNN output back to sequence length\n",
    "            H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "                sequence=H_o,\n",
    "                batch_first=True,\n",
    "                padding_value=self.padding_value,\n",
    "                total_length=self.max_seq_len,\n",
    "            )\n",
    "            logits = self.sup_linear(H_o)\n",
    "            H_hat = self.sup_sigmoid(logits)\n",
    "            return H_hat\n",
    "\n",
    "class GeneratorNetwork(nn.Module):\n",
    "    \"\"\"The generator network for TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,Z_dim,hidden_dim,num_layers,padding_value=0,max_seq_len=1000):\n",
    "        super(GeneratorNetwork,self).__init__()\n",
    "        self.Z_dim = Z_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.padding_value = padding_value\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        #Generator Architecture\n",
    "        self.gen_rnn = nn.GRU(\n",
    "            input_size=self.Z_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.gen_linear = nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "        self.gen_sigmoid = nn.Sigmoid()\n",
    "                # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.gen_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(0)\n",
    "            for name, param in self.gen_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        \n",
    "    def forward(self,Z,T):\n",
    "        \"\"\" Takes in random noise (features) and generates synthetic features within the last latent space\n",
    "        Args:\n",
    "            Z: input random noise (B x S x Z)\n",
    "            T: input temporal information (B)\n",
    "        Returns:\n",
    "            H: embeddings (B x S x E)\n",
    "        \"\"\"\n",
    "        #Dynamic RNN input for ignoring paddings\n",
    "        Z_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input = Z,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # 128*100*71\n",
    "        H_o,H_t = self.gen_rnn(Z_pack)\n",
    "\n",
    "        #pad RNN output back to sequence length\n",
    "\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "\n",
    "        #128*100*10\n",
    "        logits = self.gen_linear(H_o)\n",
    "        H = self.gen_sigmoid(logits)\n",
    "\n",
    "        return H\n",
    "\n",
    "class DiscriminatorNetwork(nn.Module):\n",
    "    \"\"\"The discriminator network for TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,hidden_dim,num_layers,padding_value=0,max_seq_len=1000   ):\n",
    "        super(DiscriminatorNetwork,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.padding_value = padding_value\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        #Discriminator Architecture\n",
    "        self.dis_rnn = nn.GRU(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dis_linear = nn.Linear(self.hidden_dim,1)\n",
    "\n",
    "        # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.dis_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(0)\n",
    "            for name, param in self.dis_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "    \n",
    "    def forward(self, H, T):\n",
    "        \"\"\" Forward pass for predicting if the data is real or synthetic\n",
    "        \n",
    "        Args:\n",
    "            H: latent representation (B x S x E)\n",
    "            T: input temporal information (B)\n",
    "        Returns:\n",
    "        logits: prediction logits(B x S x 1)\n",
    "        \"\"\"\n",
    "        # dynamic RNN input for ignoring paddings\n",
    "        H_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input = H,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # 128*100*10\n",
    "        H_o,H_t = self.dis_rnn(H_pack)\n",
    "\n",
    "        # pad RNN output back to sequence length\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "\n",
    "        logits = self.dis_linear(H_o).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "class TimeGAN(nn.Module):\n",
    "    \"\"\" Implementation of TimeGan (Yoon et al., 2019) using PyTorch\n",
    "    \n",
    "    Reference:\n",
    "        - Yoon, J., Jarret, D., van der Schaar, M. (2019). Time-series Generative Adversarial Networks. (https://papers.nips.cc/paper/2019/hash/c9efe5f26cd17ba6216bbe2a7d26d490-Abstract.html)\n",
    "        - https://github.com/jsyoon0823/TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,device,feature_dim,Z_dim,hidden_dim,max_seq_len,batch_size,num_layers,padding_value):\n",
    "        super(TimeGAN,self).__init__()\n",
    "        self.device =device\n",
    "        self.feature_dim = feature_dim\n",
    "        self.Z_dim = Z_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layer = num_layers\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "        self.embedder = EmbeddingNetwork(feature_dim=feature_dim,hidden_dim=hidden_dim,num_layers=num_layers,padding_value=padding_value,max_seq_len=max_seq_len)\n",
    "        self.recovery = RecoveryNetwork(feature_dim=feature_dim,hidden_dim=hidden_dim,num_layers=num_layers,padding_value=padding_value,max_seq_len=max_seq_len)\n",
    "        self.generator = GeneratorNetwork(Z_dim=Z_dim,hidden_dim=hidden_dim,num_layers=num_layers,padding_value=padding_value,max_seq_len=max_seq_len)\n",
    "        self.discriminator = DiscriminatorNetwork(hidden_dim=hidden_dim,num_layers=num_layers,padding_value=padding_value,max_seq_len=max_seq_len)\n",
    "\n",
    "        self.supervisor = SupervisorNetwork(hidden_dim=hidden_dim,num_layers=num_layers,padding_value=padding_value,max_seq_len=max_seq_len)\n",
    "\n",
    "    def _recovery_forward(self, X, T):\n",
    "        \"\"\" The embedding network forward pass and the embedder network loss\n",
    "        Args:\n",
    "            X: input features\n",
    "            T: input temporal information\n",
    "        Returns:\n",
    "            E_loss: the reconstruction loss\n",
    "            X_tilde: the reconstructed features\n",
    "        \"\"\"\n",
    "\n",
    "        # FOrward pass\n",
    "        H = self.embedder(X,T)\n",
    "        X_tilde = self.recovery(H,T)\n",
    "\n",
    "        #for Joint training\n",
    "        H_hat_supervise = self.supervisor(H,T)\n",
    "        G_loss_S = F.mse_loss(\n",
    "            H_hat_supervise[:,:-1,:],\n",
    "            H[:,1:,:],\n",
    "        ) #Teacher forcing next output\n",
    "\n",
    "        #Reconstruction loss\n",
    "        E_loss_T0 = F.mse_loss(X_tilde,X)\n",
    "        E_loss0 = 10*torch.sqrt(E_loss_T0)\n",
    "        E_loss = E_loss0 + 0.1*G_loss_S\n",
    "        return E_loss, E_loss0,E_loss_T0\n",
    "    def _supervisor_forward(self, X, T):\n",
    "        \"\"\" The supervisor training forward pass\n",
    "        Args:\n",
    "            X: original input features\n",
    "            T: input temporal information\n",
    "        Returns:\n",
    "            S_loss: the supervisor's loss\n",
    "        \"\"\"\n",
    "        #supervisor forward pass\n",
    "        H = self.embedder(X,T)\n",
    "        H_hat_supervise = self.supervisor(H,T)\n",
    "\n",
    "        #supervised loss\n",
    "        S_loss = F.mse_loss(\n",
    "            H_hat_supervise[:,:-1,:],\n",
    "            H[:,1:,:],\n",
    "        ) #Teacher forcing next output\n",
    "        return S_loss\n",
    "    def _discriminator_forward(self, X, T, Z, gamma=1):\n",
    "        \"\"\" The discriminator forward pass and adversarial loss\n",
    "        Args:\n",
    "            X: input features\n",
    "            T: input temporal information\n",
    "            Z: input noise\n",
    "            gamma: the weight for the adversarial loss\n",
    "        Returns:\n",
    "            D_loss: adversarial loss\n",
    "        \"\"\"\n",
    "        #Real\n",
    "        H = self.embedder(X, T).detach()\n",
    "\n",
    "        #generator\n",
    "        E_hat = self.generator(Z,T).detach()\n",
    "        H_hat = self.supervisor(E_hat,T).detach()\n",
    "        \n",
    "        #forward pass\n",
    "        Y_real = self.discriminator(H,T)        #Encode original data\n",
    "        Y_fake = self.discriminator(H_hat,T)    #Output of generator + supervisor\n",
    "        Y_fake_e = self.discriminator(E_hat,T)  #Output of generator\n",
    "\n",
    "        D_loss_real = F.binary_cross_entropy_with_logits(Y_real, torch.ones_like(Y_real))\n",
    "        D_loss_fake = F.binary_cross_entropy_with_logits(Y_fake, torch.zeros_like(Y_fake))\n",
    "        D_loss_fake_e = F.binary_cross_entropy_with_logits(Y_fake_e, torch.zeros_like(Y_fake_e))\n",
    "\n",
    "        D_loss = D_loss_real + D_loss_fake + gamma * D_loss_fake_e\n",
    "\n",
    "        return D_loss\n",
    "    \n",
    "    def _generator_forward(self, X, T, Z, gamma=1):\n",
    "        \"\"\" The generator forward pass\n",
    "        Args:\n",
    "            X: original input features\n",
    "            T: input temporal information\n",
    "            Z: input noise for the generator\n",
    "            gamma: the weight for the adversarial loss\n",
    "        Returns:\n",
    "            G_loss: the generator loss\n",
    "        \"\"\"\n",
    "        #supervisor forward pass\n",
    "        H = self.embedder(X,T)\n",
    "        H_hat_supervise = self.supervisor(H,T)\n",
    "\n",
    "        #generator forward pass\n",
    "        E_hat = self.generator(Z,T)\n",
    "        H_hat = self.supervisor(E_hat,T)\n",
    "\n",
    "        #synthetic data generated\n",
    "        X_hat = self.recovery(H_hat,T)\n",
    "\n",
    "        #generator loss\n",
    "        #Adversarial loss\n",
    "        Y_fake = self.discriminator(H_hat,T)        #Output of supervisor\n",
    "        Y_fake_e = self.discriminator(E_hat,T)      #Output of generator\n",
    "\n",
    "        G_loss_U = F.binary_cross_entropy_with_logits(Y_fake, torch.ones_like(Y_fake))\n",
    "        G_loss_U_e = F.binary_cross_entropy_with_logits(Y_fake_e, torch.ones_like(Y_fake_e))\n",
    "\n",
    "        #Supervised loss\n",
    "        G_loss_S = F.mse_loss(\n",
    "            H_hat_supervise[:,:-1,:],\n",
    "            H[:,1:,:],\n",
    "        ) #Teacher forcing next output\n",
    "\n",
    "        #Two moments losses\n",
    "        G_loss_V1 = torch.mean(\n",
    "            torch.abs(torch.sqrt(X_hat.var(dim=0,unbiased=False)+1e-6) - torch.sqrt(X.var(dim=0,unbiased=False)+1e-6))\n",
    "        )\n",
    "        G_loss_V2 = torch.mean(torch.abs((X_hat.mean(dim=0)) - (X.mean(dim=0))))\n",
    "        G_loss_V = G_loss_V1 + G_loss_V2\n",
    "        \n",
    "        #sum of losses\n",
    "        G_loss = G_loss_U + gamma * G_loss_U_e + 100 * torch.sqrt(G_loss_S) + 100 * G_loss_V\n",
    "    \n",
    "        return G_loss\n",
    "    \n",
    "    def _inference(self, Z,T):\n",
    "        \"\"\" Inference for generating synthetic data\n",
    "        Args:\n",
    "            Z: input noise\n",
    "            T: temporal information\n",
    "        Returns:\n",
    "            X_hat: the generated data\n",
    "        \"\"\"\n",
    "\n",
    "        #generator forward pass\n",
    "        E_hat = self.generator(Z,T)\n",
    "        H_hat = self.supervisor(E_hat,T)\n",
    "\n",
    "        #synthetic data generated\n",
    "        X_hat = self.recovery(H_hat,T)\n",
    "        return X_hat\n",
    "\n",
    "    def forward(self,X,T,Z, obj, gamma=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: input features (B,H,F)\n",
    "            T: The temporal information (B)\n",
    "            Z: the sampled noise (B,H,Z)\n",
    "            obj: the network to be trained ('autoencoder','supervisor','generator','discriminator')\n",
    "            gamma: loss hyperparameter\n",
    "        Returns:\n",
    "            loss: loss for the forward pass\n",
    "            X_hat: the generated data\n",
    "        \"\"\"\n",
    "\n",
    "        #Move variables to device\n",
    "        if obj !='inference':\n",
    "            if X is None:\n",
    "                run.stop()\n",
    "                raise ValueError('X cannot be empty')\n",
    "                \n",
    "            \n",
    "            X = torch.FloatTensor(X)\n",
    "            X = X.to(self.device)\n",
    "\n",
    "        if Z is not None:\n",
    "            Z = torch.FloatTensor(Z)\n",
    "            Z = Z.to(self.device)\n",
    "        \n",
    "        if obj == 'autoencoder':\n",
    "            #embedder and recovery forward\n",
    "            loss = self._recovery_forward(X,T)\n",
    "        elif obj == 'supervisor':\n",
    "            loss = self._supervisor_forward(X,T)\n",
    "        elif obj == 'generator':\n",
    "            if Z is None:\n",
    "                run.stop()\n",
    "                raise ValueError('Z cannot be empty')\n",
    "                \n",
    "            loss = self._generator_forward(X,T,Z,gamma)\n",
    "        elif obj == 'discriminator':\n",
    "            if Z is None:\n",
    "                run.stop()\n",
    "                raise ValueError('Z cannot be empty')\n",
    "            loss = self._discriminator_forward(X,T,Z,gamma)\n",
    "            return loss\n",
    "        elif obj == 'inference':\n",
    "            X_hat = self._inference(Z,T)\n",
    "            #X_hat = X_hat.cpu.detach()\n",
    "			 X_hat = X_hat.detach().cpu()\n,
    "\n",
    "            return X_hat\n",
    "        else:\n",
    "            run.stop()\n",
    "            raise ValueError('obj must be autoencoder, supervisor, generator or discriminator')\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_trainer(\n",
    "        model: torch.nn.Module,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        e_opt: torch.optim.Optimizer,\n",
    "        r_opt: torch.optim.Optimizer,\n",
    "        emb_epochs: int,\n",
    "        writer \n",
    "):\n",
    "    \"\"\"\n",
    "    Training loop for embedding and recovery functions.\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader to use\n",
    "        e_opt (torch.optim.Optimizer): The optimizer for the embedding function\n",
    "        r_opt (torch.optim.Optimizer): The optimizer for the recovery function\n",
    "        args (Dict): The model/training configuration\n",
    "        writer (SummaryWriter): Neptune logger\n",
    "    \"\"\"\n",
    "    logger = trange(emb_epochs, desc =f\"Epoch:0, Loss:0\")\n",
    "    for epoch in logger:\n",
    "        for X_mb,T_mb in dataloader:\n",
    "\n",
    "            #reset gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            #forward pass\n",
    "            _,E_loss0,E_loss_T0 = model(X=X_mb,T=T_mb,Z=None,obj=\"autoencoder\")\n",
    "            loss = np.sqrt(E_loss_T0.item())\n",
    "\n",
    "            #backward pass\n",
    "            E_loss0.backward()\n",
    "\n",
    "            #update weights\n",
    "            e_opt.step()\n",
    "            r_opt.step()\n",
    "\n",
    "        # Log loss for final batch of each epochs\n",
    "        logger.set_description(f\"Epoch:{epoch}, Loss:{loss:.4f}\")\n",
    "        writer['Embedding/Training_loss'].append(loss)\n",
    "        \"\"\"if writer:\n",
    "            writer.add_scalar(\"Embedding/Loss:\",loss,epoch)\n",
    "            writer.flush()\"\"\"\n",
    "\n",
    "def supervisor_trainer(\n",
    "    model: torch.nn.Module, \n",
    "    dataloader: torch.utils.data.DataLoader, \n",
    "    s_opt: torch.optim.Optimizer, \n",
    "    g_opt: torch.optim.Optimizer,\n",
    "    sup_epochs: int,\n",
    "    writer\n",
    "):\n",
    "    \"\"\"\n",
    "    The training loop for the supervisor function\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader to use\n",
    "        s_opt (torch.optim.Optimizer): The optimizer for the supervisor function\n",
    "        g_opt (torch.optim.Optimizer): The optimizer for the generator function\n",
    "        args (Dict): The model/training configuration\n",
    "        writer (Union[torch.utils.tensorboard.SummaryWriter, type(None)], optional): The tensorboard writer to use. Defaults to None.\n",
    "    \"\"\"\n",
    "    logger = trange(sup_epochs, desc=f\"Epoch: 0, Loss: 0\")\n",
    "    for epoch in logger:\n",
    "        for X_mb, T_mb in dataloader:\n",
    "            # Reset gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Forward Pass\n",
    "            S_loss = model(X=X_mb, T=T_mb, Z=None, obj=\"supervisor\")\n",
    "\n",
    "            # Backward Pass\n",
    "            S_loss.backward()\n",
    "            loss = np.sqrt(S_loss.item())\n",
    "\n",
    "            # Update model parameters\n",
    "            s_opt.step()\n",
    "\n",
    "        # Log loss for final batch of each epoch (29 iters)\n",
    "        logger.set_description(f\"Epoch: {epoch}, Loss: {loss:.4f}\")\n",
    "        writer['Supervisor/Training_loss'].append(loss)\n",
    "        \"\"\"if writer:\n",
    "            writer.add_scalar(\n",
    "                \"Supervisor/Loss:\",loss,epoch)\n",
    "            writer.flush()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_trainer(\n",
    "    model: torch.nn.Module, \n",
    "    dataloader: torch.utils.data.DataLoader, \n",
    "    e_opt: torch.optim.Optimizer, \n",
    "    r_opt: torch.optim.Optimizer, \n",
    "    s_opt: torch.optim.Optimizer, \n",
    "    g_opt: torch.optim.Optimizer, \n",
    "    d_opt: torch.optim.Optimizer,\n",
    "    sup_epochs: int,\n",
    "    batch_size: int,\n",
    "    max_seq_len: int,\n",
    "    Z_dim: int,\n",
    "    dis_thresh: float,\n",
    "    writer\n",
    "    ):\n",
    "    \"\"\"\n",
    "    The training loop for training the model altogether\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader to use\n",
    "        e_opt (torch.optim.Optimizer): The optimizer for the embedding function\n",
    "        r_opt (torch.optim.Optimizer): The optimizer for the recovery function\n",
    "        s_opt (torch.optim.Optimizer): The optimizer for the supervisor function\n",
    "        g_opt (torch.optim.Optimizer): The optimizer for the generator function\n",
    "        d_opt (torch.optim.Optimizer): The optimizer for the discriminator function\n",
    "        args (Dict): The model/training configuration\n",
    "        writer (Union[torch.utils.tensorboard.SummaryWriter, type(None)], optional): The tensorboard writer to use. Defaults to None.\n",
    "    \"\"\"\n",
    "    logger = trange(\n",
    "        sup_epochs, \n",
    "        desc=f\"Epoch: 0, E_loss: 0, G_loss: 0, D_loss: 0\"\n",
    "    )\n",
    "    for epoch in logger:\n",
    "        for X_mb, T_mb in dataloader:\n",
    "            ## Generator Training\n",
    "            for _ in range(2):\n",
    "                # Random Generator\n",
    "                Z_mb = torch.rand((batch_size, max_seq_len, Z_dim))\n",
    "\n",
    "                # Forward Pass (Generator)\n",
    "                model.zero_grad()\n",
    "                G_loss = model(X=X_mb, T=T_mb, Z=Z_mb, obj=\"generator\")\n",
    "                G_loss.backward()\n",
    "                G_loss = np.sqrt(G_loss.item())\n",
    "\n",
    "                # Update model parameters\n",
    "                g_opt.step()\n",
    "                s_opt.step()\n",
    "\n",
    "                # Forward Pass (Embedding)\n",
    "                model.zero_grad()\n",
    "                E_loss, _, E_loss_T0 = model(X=X_mb, T=T_mb, Z=Z_mb, obj=\"autoencoder\")\n",
    "                E_loss.backward()\n",
    "                E_loss = np.sqrt(E_loss.item())\n",
    "                \n",
    "                # Update model parameters\n",
    "                e_opt.step()\n",
    "                r_opt.step()\n",
    "\n",
    "            # Random Generator\n",
    "            Z_mb = torch.rand((batch_size, max_seq_len, Z_dim))\n",
    "\n",
    "            ## Discriminator Training\n",
    "            model.zero_grad()\n",
    "            # Forward Pass\n",
    "            D_loss = model(X=X_mb, T=T_mb, Z=Z_mb, obj=\"discriminator\")\n",
    "\n",
    "            # Check Discriminator loss\n",
    "            if D_loss > dis_thresh:\n",
    "                # Backward Pass\n",
    "                D_loss.backward()\n",
    "\n",
    "                # Update model parameters\n",
    "                d_opt.step()\n",
    "            D_loss = D_loss.item()\n",
    "\n",
    "        logger.set_description(\n",
    "            f\"Epoch: {epoch}, E: {E_loss:.4f}, G: {G_loss:.4f}, D: {D_loss:.4f}\"\n",
    "        )\n",
    "        #writer['Joint/Training_loss'].append([E_loss,G_loss,D_loss])\n",
	"        writer['Joint/E_loss'].append(E_loss)\n",
    "        writer['Joint/G_loss'].append(G_loss)\n",
    "        writer['Joint/D_loss'].append(D_loss)\n",
	"		 \n",
    "        \"\"\"if writer:\n",
    "            writer.add_scalar(\n",
    "                'Joint/Embedding_Loss:',E_loss,epoch)\n",
    "            writer.add_scalar(\n",
    "                'Joint/Generator_Loss:',G_loss,epoch)\n",
    "            writer.add_scalar('Joint/Discriminator_Loss:',D_loss,epoch)\n",
    "            writer.flush()\"\"\"\n",
    "\n",
    "def timegan_trainer(model,train_data,train_time,batch_size,lr,emb_epochs,sup_epochs,max_seq_length,Z_dim,dis_thresh,device,model_path,writer):\n",
    "    \"\"\"\n",
    "    The trainign procedure for TimeGAN.\n",
    "    Args:\n",
    "        model (torch.nn.module): The model that generates synthetic data\n",
    "        loaded_data(pandas.DataFrame): The data to train on, including data and time\n",
    "        args (Dict): The model/training configuration\n",
    "    Returns:\n",
    "        generated_data (np.array): The synthetic data generated by the model\n",
    "    \"\"\"\n",
    "    dataset = TimeGAN_Dataset(data=train_data,time=train_time)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False)\n",
    "    model.to(device)\n",
    "\n",
    "    #initialize optimizers\n",
    "    e_opt = torch.optim.Adam(model.embedder.parameters(), lr=lr)\n",
    "    r_opt = torch.optim.Adam(model.recovery.parameters(), lr=lr)\n",
    "    s_opt = torch.optim.Adam(model.supervisor.parameters(), lr=lr)\n",
    "    g_opt = torch.optim.Adam(model.generator.parameters(), lr=lr)\n",
    "    d_opt = torch.optim.Adam(model.discriminator.parameters(), lr=lr)\n",
    "\n",
    "    #initialize tensorboard writer\n",
    "    #writer = SummaryWriter(os.path.join(f\"tensorboard/{args.exp}\"))\n",
    "\n",
    "    print(\"\\nStart Embedding Network Training\")\n",
    "    embedding_trainer(model=model, dataloader=dataloader, e_opt=e_opt, r_opt=r_opt, emb_epochs=emb_epochs,writer=writer)\n",
    "\n",
    "    print(\"\\nStart Training with Supervised Loss Only\")\n",
    "    supervisor_trainer(model=model, dataloader=dataloader, s_opt=s_opt,g_opt=g_opt, sup_epochs=sup_epochs,writer=writer)\n",
    "\n",
    "    print(\"\\nStart Joint Training\")\n",
    "    joint_trainer(model=model, dataloader=dataloader, e_opt=e_opt, r_opt=r_opt, s_opt=s_opt, g_opt=g_opt, d_opt=d_opt, sup_epochs=sup_epochs, batch_size=batch_size, max_seq_len=max_seq_length, Z_dim=Z_dim, dis_thresh=dis_thresh,writer=writer)\n",
    "\n",
    "\n",
    "    #save model,args, and hyperparameters\n",
    "    #torch.save(args,f\"{args.model_path}/args.pickle\")\n",
    "    torch.save(model.state_dict(),f\"{model_path}/model.pt\")\n",
    "    \n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def timegan_generator(model,T,model_path,batch_size, max_seq_len, Z_dim ,device):\n",
    "    \"\"\"\n",
    "    The interference procedure for TimeGAN.\n",
    "    Args:\n",
    "        model (torch.nn.module): The model that generates synthetic data\n",
    "        T (List[int]): The time to generate data for\n",
    "        args (Dict): The model/training configuration\n",
    "    returns:\n",
    "        generated_data (np.array): The synthetic data generated by the model\n",
    "    \"\"\"\n",
    "    #load model\n",
    "    if not os.path.exists(model_path):\n",
    "        run.stop()\n",
    "        raise ValueError(f\"Model not found at {model_path}\")\n",
    "    model.load_state_dict(torch.load(f\"{model_path}/model.pt\"))\n",
    "    print(\"\\nStart Generating Synthetic Data\")\n",
    "    #Initialize model to evaluation mode and run without gradients\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Random Generator\n",
    "        Z = torch.rand((batch_size, max_seq_len, Z_dim))\n",
    "        # Forward Pass (Generator)\n",
    "        generated_data = model(X=None, T=T, Z=Z, obj=\"inference\")\n",
    "    return generated_data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_limit=None,save_dataset=None):\n",
    "    \"\"\"\n",
    "    data_limit=None,save_dataset=None\n",
    "    Load and preprocess real life datasets.\n",
    "    \n",
    "    Args:\n",
    "        data_limit (int): The number of data points to load. If None, all data points are loaded. Default: None. Used for testing.\n",
    "        save_dataset (bool): If 'Full', the dataset is saved to a csv file. If it's 'limited', than save the limited dataset if data_limit is not None. Default: None.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        dataset (pandas.DataFrame): The dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    main_dataset = pd.DataFrame()\n",
    "    cluster = True\n",
    "    if cluster == False:\n",
    "        Tk().withdraw() # we don't want a full GUI, so keep the root window from appearing\n",
    "        filenames = askopenfilenames() # show an \"Open\" dialog box and return the path to the selected file\n",
    "        print(filenames)\n",
    "        for filename in filenames:\n",
    "            \n",
    "            if filename.endswith('.mat'):\n",
    "                #output df format: [id,value_array]\n",
    "                df = load_mat_as_df(filename)\n",
    "                print(df)\n",
    "            \n",
    "            elif filename.endswith('.csv'):\n",
    "\n",
    "                #use create_dataset_csv.py to create a csv file\n",
    "                if filename.find('dataset') != -1:\n",
    "                    df = pd.read_csv(filename,sep=';',index_col=0)\n",
    "\n",
    "                \"\"\" CSV format:\n",
    "                ID|time|Sleeping stage|length|additional_info\n",
    "\n",
    "                \"\"\"\n",
    "                pass\n",
    "\n",
    "            elif filename.endswith('.xml'):\n",
    "                ## TODO: add xml support\n",
    "                #df =\n",
    "                pass\n",
    "            else:\n",
    "                print(\"Unsupported file format, skipping file:\",filename,\".\")\n",
    "                pass\n",
    "        \n",
    "            main_dataset.append(df)\n",
    "    else:\n",
    "        #load datasets from cluster\n",
    "        filename = \"Data/generated_dataset.csv\"\n",
    "        main_dataset = pd.read_csv(filename,sep=';',index_col=0)\n",
    "\n",
    "    #Cut df to data_limit size for testing purposes\n",
    "    if data_limit is not None:\n",
    "        if data_limit < len(main_dataset):\n",
    "            print(f'The length of the dataset is {len(main_dataset)}, the new length is {data_limit}')\n",
    "            main_dataset = main_dataset[:data_limit]\n",
    "        elif data_limit >= len(main_dataset):\n",
    "            print(\"data_limit is bigger than the dataset size, using the whole dataset\")\n",
    "    elif data_limit <= 0 or data_limit == '':\n",
    "        print(\"data_limit is 0 or less, using the whole dataset\")\n",
    "    #print the headers of the dataset\n",
    "    #print(main_dataset.head())\n",
    "    # transform dataset.time and dataset.Sleeping stage to float and int\n",
    "    #print(main_dataset)\n",
    "    main_dataset.time = main_dataset.time.apply(string_to_float_array)\n",
    "    \n",
    "    main_dataset.Sleeping_stage = main_dataset.Sleeping_stage.apply(string_to_int_array)\n",
    "    \n",
    "    #print(main_dataset.head())\n",
    "    return main_dataset #dataset as df\n",
    "\n",
    "def load_mat_as_df(mat_file_path, var_name):\n",
    "    mat = sio.loadmat(mat_file_path,simplify_cells=True)\n",
    "\n",
    "    if var_name not in list(mat.keys()):\n",
    "        var_name = get_variable_name(mat)   \n",
    "        \n",
    "\n",
    "    return pd.DataFrame(mat[var_name])\n",
    "\n",
    "def get_variable_name(loaded_mat):\n",
    "\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    root.title('.mat variable selector')\n",
    "    tk.Label(root, text=\"Choose a variable:\").pack()\n",
    "    choices = list(loaded_mat.keys())\n",
    "\n",
    "    variable = tk.StringVar(root)\n",
    "    variable.set(choices[0]) # default value\n",
    "    w = tk.Combobox(root, textvariable=variable,values=choices)\n",
    "\n",
    "    w.pack()\n",
    "    def ok():\n",
    "        print (\"value is:\" + variable.get())\n",
    "        root.destroy()\n",
    "    def cancel():\n",
    "        root.destroy()\n",
    "        raise InterruptedError('User cancelled, invalid variable name')\n",
    "\n",
    "    button1 = tk.Button(root, text=\"OK\", command=ok)\n",
    "    button2 = tk.Button(root, text=\"Cancel\", command=cancel)\n",
    "    button1.pack()\n",
    "    button2.pack()\n",
    "    root.mainloop()\n",
    "    \n",
    "    return variable.get()\n",
    "def string_to_float_list(s):\n",
    "    return [float(x) for x in s.strip('[]').split(',')]\n",
    "def string_to_int_list(s):\n",
    "    return [int(x) for x in s.strip('[]').split(',')]\n",
    "def string_to_float_array(s):\n",
    "    return [float(x) for x in ast.literal_eval(s)]\n",
    "def string_to_int_array(s):\n",
    "    return [int(x) for x in ast.literal_eval(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(writer,padding_value,data_limit=None,save_dataset=None,norm_enable=False):\n",
    "    \"\"\"\n",
    "    padding_value: int = -1.0,\n",
    "    data_limit: int = None\"\"\"\n",
    "    # Load and preprocess data\n",
    "    # \n",
    "    # Steps:\n",
    "    # 0. Sanity checks\n",
    "    # 1. Load data from files (csv,mat,xml)\n",
    "    # 2. Preprocess data:\n",
    "    # 2.1. Remove outliers\n",
    "    # 2.2. Extract sequence length and time\n",
    "    # 2.3. Resample data\n",
    "    # 2.4. Normalize data\n",
    "    # 2.5. Padding\n",
    "    # 3 Save data to csv file\n",
    "    #  \n",
    "    # Args:\n",
    "    #     data_limit (int): The number of lines to load from the data file. If None, all data points are loaded. Default: None. Used for testing.\n",
    "    #     padding_value (int): The value used for padding\n",
    "    #     save_dataset (bool): If 'Full', the dataset is saved to a csv file. If it's 'limited', than save the limited dataset if data_limit is not None. Default: None.\n",
    "    #     norm_enable (bool): If True, normalize the data. Default: False.\n",
    "    #     \n",
    "    # \n",
    "    # Returns:\n",
    "    #     prep_data (pandas.DataFrame): The processed data\n",
    "\n",
    "    #######################################\n",
    "    # 0. Sanity checks\n",
    "    #######################################\n",
    "    \n",
    "    # Check if save_dataset is valid\n",
    "    if save_dataset is not None:\n",
    "        if save_dataset.lower =='none':\n",
    "            save_dataset = None\n",
    "        elif save_dataset.lower =='full':\n",
    "            save_dataset = 'Full'\n",
    "        elif save_dataset.lower =='limited':\n",
    "            save_dataset = 'Limited'\n",
    "        else:\n",
    "            raise ValueError('save_dataset must be None, Full or Limited')\n",
    "    # Check if data_limit is valid\n",
    "    if data_limit is not None:\n",
    "        if isinstance(data_limit, str) == True:\n",
    "            data_limit= None\n",
    "        elif isinstance(data_limit, float) == True and data_limit.is_integer() == True:\n",
    "            data_limit = int(data_limit)\n",
    "        elif isinstance(data_limit, int) == True:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('data_limit must be None or int')\n",
    "    # Check if padding_value is valid\n",
    "    if isinstance(padding_value, str) == True:\n",
    "        raise ValueError('padding_value must be int')\n",
    "    elif isinstance(padding_value, float) == True and padding_value.is_integer() == True:\n",
    "        padding_value = int(padding_value)\n",
    "    elif isinstance(padding_value, int) == True:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError('padding_value must be int')\n",
    "    # Check if norm_enable is valid\n",
    "    if isinstance(norm_enable, bool) == True:\n",
    "        pass\n",
    "    elif isinstance(norm_enable, str) == True:\n",
    "        if norm_enable.lower() == 'true':\n",
    "            norm_enable = True\n",
    "        elif norm_enable.lower() == 'false':\n",
    "            norm_enable = False\n",
    "        else:\n",
    "            raise ValueError('norm_enable must be bool')\n",
    "    else:\n",
    "        raise ValueError('norm_enable must be bool')\n",
    "\n",
    "\n",
    "    #######################################\n",
    "    # 1. Load data from files (csv,mat,xml)\n",
    "    #######################################\n",
    "\n",
    "    loaded_data = load_data(data_limit=data_limit,save_dataset=save_dataset)\n",
    "    \"\"\"\n",
    "    loaded data =       time_data   , data  , length\n",
    "    (pandas.DataFrame), (np.array)  ,(list) ,(int)\n",
    "    \n",
    "    ()\n",
    "    \"\"\"\n",
    "    #convert data to np.array\n",
    "    #loaded_data['data'] = loaded_data['data'].apply(lambda x: np.array(x))\n",
    "    \n",
    "    #######################################\n",
    "    # 2. Preprocess data:\n",
    "    #######################################\n",
    "    # 2.1. Remove outliers\n",
    "    #######################################\n",
    "    \"\"\"\n",
    "    Remove row's with unacceptable sleep stages values\n",
    "    \"\"\"\n",
    "    \n",
    "    sleep_stages = np.array([1,2,3,4,5])\n",
    "    loaded_data[loaded_data['Sleeping_stage'].apply(lambda x: all(elem in sleep_stages for elem in x))]\n",
    "\n",
    "    #######################################\n",
    "    # 2.2. Extract sequence length and time\n",
    "    #######################################\n",
    "    \"\"\"\n",
    "    Extract sequence length of all lines and time of each line\n",
    "    \"\"\"\n",
    "    loaded_data['length'] = loaded_data['Sleeping_stage'].apply(lambda x: len(x))\n",
    "	 \n",
    "    #plot length distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2,figsize=(10,5))\n",
    "    ax1.hist(loaded_data['length'], )\n",
    "    ax1.set_title('Original length distribution')\n",
    "    _,bins,_=ax1.hist(loaded_data['length'])\n",
    "    # drop rows with +- 10% of mean length\n",
    "    #caluclate mean length \n",
    "    mean_len = loaded_data.length.mean()\n",
    "    #df = df[df['Age'] <= 1.1 * mean_age]\n",
    "    loaded_data=loaded_data[loaded_data['length'] <= 1.1 *mean_len]\n",
    "    loaded_data=loaded_data[loaded_data['length'] >=0.9*mean_len]\n",
    "    ax2.hist(loaded_data['length'],bins=bins,)\n",
    "    writer['Length_comp'].upload(fig)\n",
    "    \n",
    "    #######################################\n",
    "    # 2.4. Normalize data\n",
    "    #######################################\n",
    "    \"\"\"\n",
    "    Normalize data to [0,1] using MinMaxScaler algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    if norm_enable == True:\n",
    "        loaded_data['Sleeping_stage']=MinMaxNormalizer(loaded_data['Sleeping_stage'])\n",
    "    \n",
    "\n",
    "    #######################################\n",
    "    # 2.5. Padding\n",
    "    #######################################\n",
    "    \"\"\"\n",
    "    Padding data to given length\n",
    "    \"\"\"\n",
    "    #print(f'length of all row: {loaded_data.length}')\n",
    "    max_len = max(loaded_data['length'])\n",
    "    length = len(loaded_data)\n",
    "    time_pad = -1.0\n",
    "    #get the number of columns in the data\n",
    "    dim =len(loaded_data.columns)-1\n",
    "    print(f'dim is {dim}')\n",
    "    # Question: Current padding value is 0, is it ok? Do we need it or just resample?\n",
    "    data_info = {\n",
    "        'length' : length,\n",
    "        'max_length' : max_len,\n",
    "        'paddding_value_stage' : padding_value,\n",
    "        'paddding_value_time' : time_pad,\n",
    "        'dim' : dim\n",
    "    }\n",
    "\n",
    "    loaded_data['Sleeping_stage'] = loaded_data['Sleeping_stage'].apply(lambda x: np.transpose(x))\n",
    "    \n",
    "    prep_data = pd.DataFrame(columns=['Sleeping_stage','time'])\n",
    "    #prep_time = pd.DataFrame(columns=['time'])\n",
    "    print(f'Padding data to {max_len} length')\n",
    "\n",
    "    #shape: [length, max_len,features] = [length, max_len,1]\n",
    "    # init empty array\n",
    "    padded_data = np.empty((length,max_len,1))\n",
    "    #fill array with padding value\n",
    "    padded_data.fill(padding_value)\n",
    "\n",
    "    time = []\n",
    "\n",
    "    for i in tqdm(range(length)):\n",
    "        #get a row of Sleep stage data\n",
    "        tmp_stage = loaded_data.iloc[i]['Sleeping_stage']\n",
    "        #print tmp_stage type\n",
    "        #print(tmp_stage.shape)\n",
    "\n",
    "        #impute missing values\n",
    "        #tmp_stage = impute_missing_values(tmp_stage)\n",
    "        \n",
    "        #get time data\n",
    "        tmp_time = len(tmp_stage)\n",
    "        #reshape tmp_stage to [tmp_time,1]\n",
    "        tmp_stage = tmp_stage.reshape(tmp_time,1)\n",
    "        # pad data to 'max_seq_len'\n",
    "        if len(tmp_stage) >= max_len:\n",
    "            padded_data[i,:,:] = tmp_stage[:max_len,0:]\n",
    "            time.append(max_len)\n",
    "        else:\n",
    "            padded_data[i,:tmp_time,:] = tmp_stage[:,0:]\n",
    "            time.append(len(tmp_stage))\n",
    "\n",
    "    return padded_data, time, data_info    \n",
    "    \n",
    "def impute_missing_values(\n",
    "        curr_data: np.ndarray,\n",
    "        )-> np.ndarray:\n",
    "    \"\"\"ArithmeticError\n",
    "    Impute missing values in data.\n",
    "    Args:\n",
    "        curr_data: 1-D array of data.\n",
    "    \n",
    "    Returns:\n",
    "        data: 1-D array of data with missing values filled.\n",
    "    \"\"\"\n",
    "    index = range(1,len(curr_data)+1)\n",
    "    curr_data = pd.DataFrame(data=curr_data,index=index,columns=['data'])\n",
    "    #impute data\n",
    "    imputed_data=curr_data.fillna(1)\n",
    "    \n",
    "    # check for any N/A values\n",
    "    if imputed_data.isnull().any().any():\n",
    "        raise ArithmeticError('Missing values were not imputed correctly')\n",
    "    print(f'imputed data is {imputed_data.data[0].dtype}')\n",
    "    return imputed_data.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(length)):\n",
    "        #create empty array with padding value\n",
    "        tmp_stage = np.empty(max_len)\n",
    "        #fill array with padding value\n",
    "        tmp_stage.fill(padding_value)\n",
    "        #fill array with data\n",
    "        print(f'padding shape {tmp_stage.shape}, while data shape is {loaded_data.iloc[i][\"Sleeping_stage\"].shape}')\n",
    "        #np.copyto(tmp_stage,loaded_data.iloc[i]['Sleeping_stage'])\n",
    "        tmp_stage[0:len(loaded_data['Sleeping_stage'][i])]=loaded_data['Sleeping_stage'][i]\n",
    "        #create empty array for padded time\n",
    "        #tmp_time = np.empty(max_len)\n",
    "        #fill array with padding value\n",
    "        #tmp_time.fill(time_pad)\n",
    "        #fill array with data\n",
    "        #np.copyto(tmp_time,loaded_data.iloc[i]['time'])\n",
    "        #tmp_time[0:len(loaded_data['time'][i])]=loaded_data['time'][i]\n",
    "\n",
    "        #append to prep_data\n",
    "        #prep_data = prep_data.append({'time':tmp_time,'Sleeping_stage':tmp_stage},ignore_index=True)\n",
    "        tmp_time = len(tmp_stage)\n",
    "        prep_data=prep_data.append({'Sleeping_stage':tmp_stage,'time':tmp_time},ignore_index=True)\n",
    "        \n",
    "\n",
    "    # add index column to prep_data\n",
    "    #prep_data['index'] = prep_data.index\n",
    "    \n",
    "\n",
    "\n",
    "    # 3.1 Save dataset to file\n",
    "    if save_dataset == 'Full':\n",
    "        #save dataset to a csv file\n",
    "        prep_data.to_csv('full_dataset.csv',sep=';')\n",
    "    elif save_dataset == 'Limited':\n",
    "        if data_limit is not None or data_limit != '' or data_limit > 0:\n",
    "            #save dataset as limited dataset\n",
    "            prep_data.to_csv('limited_dataset.csv',sep=';')\n",
    "        else:\n",
    "            print(f\"Warning: data_limit is {data_limit} which is not supported value, dataset is not limited, saving full dataset.\")\n",
    "            prep_data.to_csv('full_dataset.csv',sep=';')\n",
    "    elif save_dataset is None or save_dataset == '':\n",
    "        pass\n",
    "    else:\n",
    "        run.stop()\n",
    "        raise ValueError(\"Invalid save_dataset value, valid values are \\'Full\\',\\'Limited\\',None.\")\n",
    "    \"\"\"\n",
    "    \"\"\"for i in tqdm(range(length)):\n",
    "        #create empty array with padding value\n",
    "        tmp_array = np.empty([max_len,1])\n",
    "        tmp_array.fill(padding_value)\n",
    "        #fill array with data\n",
    "        tmp_array[:loaded_data['Sleeping_stage'][i].shape[0],:loaded_data['Sleeping_stage'][i].shape[1]] = loaded_data['Sleeping_stage'][i]\n",
    "        #append to prep_data\n",
    "        prep_data.append(tmp_array)\n",
    "    \"\"\"\n",
    "    #print data type of prepared data.time\n",
    "    #print(f'prep_data time type is {type(prep_data.time[0])}')\n",
    "    #print(f'prep_data shape is {prep_data}')\n",
    "    #print(f'prep_data time shape is {prep_data.time.shape}')\n",
    "    #print(f'prep_data Sleeping_stage shape is {prep_data.Sleeping_stage.shape}')\n",
    "    \n",
    "    #save dataset to a csv file\n",
    "    #prep_data.to_csv('test_dataset.csv',sep=';')\n",
    "\n",
    "\n",
    "    #\n",
    "\n",
    "    #return prep_data.Sleeping_stage ,prep_data.time,data_info\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def MinMaxNormalizer(data,min_value=1,max_value=5):\n",
    "    numerator = data-min_value\n",
    "    denominator = max_value-min_value\n",
    "    norm_data = numerator/denominator\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code directory:\t\t\tc:\\Users\\tomo9\\Documents\\00_School\\00_Thesis\\HypnoGAN\n",
      "Data directory:\t\t\tc:\\Users\\tomo9\\Documents\\00_School\\00_Thesis\\HypnoGAN\\Data\n",
      "Output directory:\t\tc:\\Users\\tomo9\\Documents\\00_School\\00_Thesis\\HypnoGAN\\Output\\test\n",
      "Using CUDA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# Experiment Arguments\n",
    "#######################################\n",
    "\n",
    "# select device:\n",
    "device = \"cuda\"\n",
    "\n",
    "#set random seed\n",
    "seed = 0\n",
    "\n",
    "#experiment name\n",
    "exp_name =\"test\"\n",
    "\n",
    "#normalization enable\n",
    "norm_enable = False\n",
    "\n",
    "# padding value\n",
    "padding_value = 0.0\n",
    "\n",
    "# Train\n",
    "is_train = True\n",
    "\n",
    "# dataset save:\n",
    "# Full: save full dataset\n",
    "# Limited: save limited dataset\n",
    "# None: don't save dataset\n",
    "save_dataset = None\n",
    "\n",
    "# save arguments to neptune\n",
    "run[\"Initialization/Arguments/ExperimentArgs/device\"] = device\n",
    "run[\"Initialization/Arguments/ExperimentArgs/seed\"] = seed\n",
    "run[\"Initialization/Arguments/ExperimentArgs/exp_name\"] = exp_name\n",
    "run[\"Initialization/Arguments/ExperimentArgs/norm_enable\"] = norm_enable\n",
    "run[\"Initialization/Arguments/ExperimentArgs/padding_value\"] = padding_value\n",
    "run[\"Initialization/Arguments/ExperimentArgs/is_train\"] = is_train\n",
    "#run[\"Initialization/Arguments/ExperimentArgs/save_dataset\"] = save_dataset\n",
    "\n",
    "\n",
    "#######################################\n",
    "# Data Arguments\n",
    "#######################################\n",
    "\n",
    "#data limit for testing\n",
    "# None: use full dataset\n",
    "# int: use limited dataset\n",
    "data_limit = 130\n",
    "\n",
    "#train test split rate\n",
    "train_rate = 0.6\n",
    "# save arguments to neptune\n",
    "run[\"Initialization/Arguments/DataArgs/data_limit\"] = data_limit\n",
    "run[\"Initialization/Arguments/DataArgs/train_rate\"] = train_rate\n",
    "\n",
    "#######################################\n",
    "# Model Arguments\n",
    "#######################################\n",
    "\n",
    "# embedding model epochs\n",
    "emb_epochs = 300\n",
    "\n",
    "# GAN model epochs\n",
    "gan_epochs = 300\n",
    "\n",
    "# supervised model epochs\n",
    "sup_epochs = 300\n",
    "\n",
    "# batch size\n",
    "batch_size = 64\n",
    "\n",
    "# hidden dimension of RNN\n",
    "hidden_dim = 20\n",
    "\n",
    "# number of layers in RNN\n",
    "num_layers = 3\n",
    "\n",
    "# discriminator threshold\n",
    "dis_thresh = 0.15\n",
    "\n",
    "#learning rate\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "# save arguments to neptune\n",
    "run[\"Initialization/Arguments/ModelArgs/embedding_epochs\"] = emb_epochs\n",
    "run[\"Initialization/Arguments/ModelArgs/gan_epochs\"] = gan_epochs\n",
    "run[\"Initialization/Arguments/ModelArgs/supervised_epochs\"] = sup_epochs\n",
    "run[\"Initialization/Arguments/ModelArgs/batch_size\"] = batch_size\n",
    "run[\"Initialization/Arguments/ModelArgs/hidden_dim\"] = hidden_dim\n",
    "run[\"Initialization/Arguments/ModelArgs/num_layers\"] = num_layers\n",
    "run[\"Initialization/Arguments/ModelArgs/discriminator_threshold\"] = dis_thresh\n",
    "run[\"Initialization/Arguments/ModelArgs/learning_rate\"] = lr\n",
    "\n",
    "\n",
    "\n",
    "######################################\n",
    "# Initialize output directories\n",
    "######################################\n",
    "## runtime directory\n",
    "code_dir = os.path.abspath(\".\")\n",
    "if not os.path.exists(code_dir):\n",
    "    run.stop()\n",
    "    raise ValueError(f\"Code directory not found at {code_dir}\")\n",
    "\n",
    "## Data directory\n",
    "data_path = os.path.abspath(\"./Data\")\n",
    "if not os.path.exists(data_path):\n",
    "    run.stop()\n",
    "    raise ValueError(f\"Data directory not found at {data_path}\")\n",
    "data_dir = os.path.dirname(data_path)\n",
    "data__file_name = os.path.basename(data_path)\n",
    "## Output directory\n",
    "model_path = os.path.abspath(f\"./Output/{exp_name}/\")\n",
    "out_dir = os.path.abspath(model_path)\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir,exist_ok=True)\n",
    "\n",
    "print(f\"Code directory:\\t\\t\\t{code_dir}\")\n",
    "print(f\"Data directory:\\t\\t\\t{data_path}\")\n",
    "print(f\"Output directory:\\t\\t{out_dir}\")\n",
    "\n",
    " ######################################\n",
    "# Initialize random seed and CUDA\n",
    "######################################\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if device == \"cuda\" and torch.cuda.is_available():\n",
    "    print(\"Using CUDA\\n\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    print(\"Using CPU\\n\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_limit is bigger than the dataset size, using the whole dataset\n",
      "dim is 3\n",
      "Padding data to 1186 length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 129/129 [00:00<00:00, 12764.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1027,)\n",
      "(985,)\n",
      "(1030,)\n",
      "(1101,)\n",
      "(1016,)\n",
      "(983,)\n",
      "(1000,)\n",
      "(965,)\n",
      "(1027,)\n",
      "(1125,)\n",
      "(974,)\n",
      "(1052,)\n",
      "(1036,)\n",
      "(982,)\n",
      "(974,)\n",
      "(1055,)\n",
      "(987,)\n",
      "(980,)\n",
      "(1078,)\n",
      "(984,)\n",
      "(998,)\n",
      "(1035,)\n",
      "(997,)\n",
      "(990,)\n",
      "(1013,)\n",
      "(1060,)\n",
      "(1186,)\n",
      "(985,)\n",
      "(1046,)\n",
      "(1037,)\n",
      "(1007,)\n",
      "(997,)\n",
      "(991,)\n",
      "(1005,)\n",
      "(1000,)\n",
      "(993,)\n",
      "(1033,)\n",
      "(990,)\n",
      "(979,)\n",
      "(1043,)\n",
      "(1000,)\n",
      "(1024,)\n",
      "(993,)\n",
      "(1049,)\n",
      "(1013,)\n",
      "(991,)\n",
      "(995,)\n",
      "(984,)\n",
      "(1039,)\n",
      "(949,)\n",
      "(1026,)\n",
      "(992,)\n",
      "(992,)\n",
      "(1034,)\n",
      "(995,)\n",
      "(974,)\n",
      "(978,)\n",
      "(982,)\n",
      "(982,)\n",
      "(1019,)\n",
      "(922,)\n",
      "(1002,)\n",
      "(989,)\n",
      "(972,)\n",
      "(1014,)\n",
      "(1019,)\n",
      "(989,)\n",
      "(1020,)\n",
      "(1000,)\n",
      "(984,)\n",
      "(1018,)\n",
      "(991,)\n",
      "(994,)\n",
      "(1010,)\n",
      "(1028,)\n",
      "(1041,)\n",
      "(1095,)\n",
      "(998,)\n",
      "(1027,)\n",
      "(1002,)\n",
      "(1042,)\n",
      "(988,)\n",
      "(986,)\n",
      "(998,)\n",
      "(1013,)\n",
      "(1013,)\n",
      "(990,)\n",
      "(1011,)\n",
      "(1022,)\n",
      "(980,)\n",
      "(1000,)\n",
      "(1037,)\n",
      "(977,)\n",
      "(870,)\n",
      "(1098,)\n",
      "(982,)\n",
      "(1023,)\n",
      "(984,)\n",
      "(997,)\n",
      "(990,)\n",
      "(982,)\n",
      "(1022,)\n",
      "(1000,)\n",
      "(984,)\n",
      "(1009,)\n",
      "(1048,)\n",
      "(985,)\n",
      "(1007,)\n",
      "(981,)\n",
      "(982,)\n",
      "(1006,)\n",
      "(1037,)\n",
      "(983,)\n",
      "(1012,)\n",
      "(976,)\n",
      "(982,)\n",
      "(1143,)\n",
      "(1023,)\n",
      "(996,)\n",
      "(996,)\n",
      "(1005,)\n",
      "(1030,)\n",
      "(994,)\n",
      "(976,)\n",
      "(1000,)\n",
      "(1001,)\n",
      "(968,)\n",
      "(990,)\n",
      "(979,)\n",
      "[[[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]]\n",
      "Processed Data: (129, 1186, 1) (Idx x Max_Sequence_Length x Features(=1))\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Original data preview:\n",
      "[[[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]]\n",
      "\n",
      "feature_dim: 1, Z_dim: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Embedding Network Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:299, Loss:1.1879: 100%|| 300/300 [02:41<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Training with Supervised Loss Only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 299, Loss: 0.0428: 100%|| 300/300 [02:05<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Joint Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0, E: 3.4454, G: 11.2630, D: 2.4527:   0%|          | 0/300 [00:05<?, ?it/s]C:\\Users\\tomo9\\AppData\\Local\\Temp\\ipykernel_19848\\3758180778.py:80: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'list'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "  writer['Joint/Training_loss'].append([E_loss,G_loss,D_loss])\n",
      "Epoch: 299, E: 0.4105, G: 6.3041, D: 1.4814: 100%|| 300/300 [26:25<00:00,  5.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to c:\\Users\\tomo9\\Documents\\00_School\\00_Thesis\\HypnoGAN\\Output\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "timegan_generator() got an unexpected keyword argument 'max_seq_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mif\u001b[39;00m is_train \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     timegan_trainer(model,train_data,train_time,batch_size\u001b[39m=\u001b[39mbatch_size,lr\u001b[39m=\u001b[39mlr,emb_epochs\u001b[39m=\u001b[39memb_epochs,sup_epochs\u001b[39m=\u001b[39msup_epochs,max_seq_length\u001b[39m=\u001b[39mmax_seq_len,Z_dim\u001b[39m=\u001b[39mZ_dim,dis_thresh\u001b[39m=\u001b[39mdis_thresh,device\u001b[39m=\u001b[39mdevice,model_path\u001b[39m=\u001b[39mmodel_path,writer\u001b[39m=\u001b[39mrun)\n\u001b[1;32m---> 31\u001b[0m generated_data \u001b[39m=\u001b[39m timegan_generator(model, train_time, model_path\u001b[39m=\u001b[39;49mmodel_path,batch_size\u001b[39m=\u001b[39;49mbatch_size,max_seq_length\u001b[39m=\u001b[39;49mmax_seq_len,Z_dim\u001b[39m=\u001b[39;49mZ_dim,device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m     32\u001b[0m generated_time \u001b[39m=\u001b[39m train_time\n\u001b[0;32m     33\u001b[0m \u001b[39m# Log end time\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: timegan_generator() got an unexpected keyword argument 'max_seq_length'"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################\n",
    "# Load and preprocess data for model\n",
    "######################################\n",
    "X,T,loaded_data_info = preprocess_data(writer=run,padding_value=padding_value,data_limit=data_limit,save_dataset=save_dataset,norm_enable=norm_enable)\n",
    "#print(X)\n",
    "#print(f\"Processed Data: {X.shape} (Idx x Max_Sequence_Length x Features(=1))\")\n",
    "#print(X[0])\n",
    "print(f\"Original data preview:\\n{X[:2, :10, :]}\\n\")\n",
    "feature_dim = X.shape[-1]\n",
    "Z_dim = X.shape[-1]\n",
    "#print(f'feature_dim: {feature_dim}, Z_dim: {Z_dim}')\n",
    "\n",
    "max_seq_len = loaded_data_info['max_length']\n",
    " # Train-Test Split data and time\n",
    " # TODO: Same people should be in the same pool at train test split\n",
    " # Make the split on the subject ID's, so also have to att ID to the loaded data\n",
    "train_data, test_data, train_time, test_time = train_test_split(\n",
    "    X, T, test_size=train_rate, random_state=seed\n",
    ")\n",
    "#########################\n",
    "# Initialize and Run model\n",
    "#########################\n",
    " # Log start time\n",
    "start = time.time()\n",
    "#init TimeGAN model\n",
    "model = TimeGAN(device=device,feature_dim=feature_dim,Z_dim=Z_dim,  hidden_dim=hidden_dim, num_layers=num_layers, max_seq_len=max_seq_len,batch_size=batch_size,padding_value=padding_value)\n",
    "\n",
    "#model = TimeGAN(feature_dim=feature_dim,  hidden_dim=hidden_dim, num_layers=num_layers, max_seq_len=max_seq_len,padding_value=padding_value)\n",
    "if is_train == True:\n",
    "    timegan_trainer(model,train_data,train_time,batch_size=batch_size,lr=lr,emb_epochs=emb_epochs,sup_epochs=sup_epochs,max_seq_length=max_seq_len,Z_dim=Z_dim,dis_thresh=dis_thresh,device=device,model_path=model_path,writer=run)\n",
    "generated_data = timegan_generator(model, train_time, model_path=model_path,batch_size=batch_size,max_seq_len=max_seq_len,Z_dim=Z_dim,device=device)\n",
    "generated_time = train_time\n",
    "# Log end time\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Generated data preview:\\n{generated_data[:2, -10:, :2]}\\n\")\n",
    "print(f\"Model Runtime: {(end - start)/60} mins\\n\")\n",
    "# Save splitted data and generated data\n",
    "with open(f\"{model_path}/train_data.pickle\", \"wb\") as fb:\n",
    "    run[\"pickled_train_data\"].upload(File.as_pickle(train_data))\n",
    "    pickle.dump(train_data, fb)\n",
    "with open(f\"{model_path}/train_time.pickle\", \"wb\") as fb:\n",
    "    run[\"pickled_train_time\"].upload(File.as_pickle(train_time))\n",
    "    pickle.dump(train_time, fb)\n",
    "with open(f\"{model_path}/test_data.pickle\", \"wb\") as fb:\n",
    "    run[\"pickled_test_data\"].upload(File.as_pickle(test_data))\n",
    "    pickle.dump(test_data, fb)\n",
    "with open(f\"{model_path}/test_time.pickle\", \"wb\") as fb:\n",
    "    run[\"pickled_test_time\"].upload(File.as_pickle(test_time))\n",
    "    pickle.dump(test_time, fb)\n",
    "with open(f\"{model_path}/fake_data.pickle\", \"wb\") as fb:\n",
    "    run[\"pickled_fake_data\"].upload(File.as_pickle(generated_data))\n",
    "    pickle.dump(generated_data, fb)\n",
    "with open(f\"{model_path}/fake_time.pickle\", \"wb\") as fb:\n",
    "    run[\"pickled_fake_time\"].upload(File.as_pickle(generated_time))\n",
    "    pickle.dump(generated_time, fb)\n",
    "\n",
    "# Stop Neptune experiment\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Generating Synthetic Data\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generated_data \u001b[39m=\u001b[39m timegan_generator(model, train_time, model_path\u001b[39m=\u001b[39;49mmodel_path,batch_size\u001b[39m=\u001b[39;49mbatch_size,max_seq_len\u001b[39m=\u001b[39;49mmax_seq_len,Z_dim\u001b[39m=\u001b[39;49mZ_dim,device\u001b[39m=\u001b[39;49mdevice)\n",
      "Cell \u001b[1;32mIn[77], line 152\u001b[0m, in \u001b[0;36mtimegan_generator\u001b[1;34m(model, T, model_path, batch_size, max_seq_len, Z_dim, device)\u001b[0m\n\u001b[0;32m    150\u001b[0m     Z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand((batch_size, max_seq_len, Z_dim))\n\u001b[0;32m    151\u001b[0m     \u001b[39m# Forward Pass (Generator)\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m     generated_data \u001b[39m=\u001b[39m model(X\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, T\u001b[39m=\u001b[39;49mT, Z\u001b[39m=\u001b[39;49mZ, obj\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minference\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    153\u001b[0m \u001b[39mreturn\u001b[39;00m generated_data\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[53], line 575\u001b[0m, in \u001b[0;36mTimeGAN.forward\u001b[1;34m(self, X, T, Z, obj, gamma)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[39melif\u001b[39;00m obj \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39minference\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    574\u001b[0m     X_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inference(Z,T)\n\u001b[1;32m--> 575\u001b[0m     X_hat \u001b[39m=\u001b[39m X_hat\u001b[39m.\u001b[39;49mcpu\u001b[39m.\u001b[39;49mdetach()\n\u001b[0;32m    577\u001b[0m     \u001b[39mreturn\u001b[39;00m X_hat\n\u001b[0;32m    578\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'detach'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run HYPNOG-45 received stop signal. Exiting\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Run HYPNOG-44 received stop signal. Exiting\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "All 0 operations synced, thanks for waiting!\n",
      "Done!\n",
      "All 0 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/NTLAB/HypnoGAN/e/HYPNOG-45/metadata\n",
      "Explore the metadata in the Neptune app:\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "generated_data = timegan_generator(model, train_time, model_path=model_path,batch_size=batch_size,max_seq_len=max_seq_len,Z_dim=Z_dim,device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "\n",
    "# Evaluation variables:\n",
    "feat_pred_no = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Preprocess data for seeker\n",
    "#########################\n",
    "# Define enlarge data and its labels\n",
    "enlarge_data = np.concatenate((train_data, test_data), axis=0)\n",
    "enlarge_time = np.concatenate((train_time, test_time), axis=0)\n",
    "enlarge_data_label = np.concatenate((np.ones([train_data.shape[0], 1]), np.zeros([test_data.shape[0], 1])), axis=0)\n",
    "# Mix the order\n",
    "idx = np.random.permutation(enlarge_data.shape[0])\n",
    "enlarge_data = enlarge_data[idx]\n",
    "enlarge_data_label = enlarge_data_label[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_prediction(train_data, test_data, index):\n",
    "    \"\"\"Use the other features to predict a certain feature.\n",
    "\n",
    "    Args:\n",
    "    - train_data (train_data, train_time): training time-series\n",
    "    - test_data (test_data, test_data): testing time-series\n",
    "    - index: feature index to be predicted\n",
    "\n",
    "    Returns:\n",
    "    - perf: average performance of feature predictions (in terms of AUC or MSE)\n",
    "    \"\"\"\n",
    "    train_data, train_time = train_data\n",
    "    test_data, test_time = test_data\n",
    "\n",
    "    # Parameters\n",
    "    no, seq_len, dim = train_data.shape\n",
    "\n",
    "    # Set model parameters\n",
    "\n",
    "    args = {}\n",
    "    args[\"device\"] = \"cuda\"\n",
    "    args[\"task\"] = \"regression\"\n",
    "    args[\"model_type\"] = \"gru\"\n",
    "    args[\"bidirectional\"] = False\n",
    "    args[\"epochs\"] = 20\n",
    "    args[\"batch_size\"] = 128\n",
    "    args[\"in_dim\"] = dim-1\n",
    "    args[\"h_dim\"] = dim-1\n",
    "    args[\"out_dim\"] = 1\n",
    "    args[\"n_layers\"] = 3\n",
    "    args[\"dropout\"] = 0.5\n",
    "    args[\"padding_value\"] = -1.0\n",
    "    args[\"max_seq_len\"] = 100\n",
    "    args[\"learning_rate\"] = 1e-3\n",
    "    args[\"grad_clip_norm\"] = 5.0\n",
    "\n",
    "    # Output initialization\n",
    "    perf = list()\n",
    "  \n",
    "    # For each index\n",
    "    for idx in index:\n",
    "        # Set training features and labels\n",
    "        train_dataset = FeaturePredictionDataset(\n",
    "            train_data, \n",
    "            train_time, \n",
    "            idx\n",
    "        )\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=args[\"batch_size\"],\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # Set testing features and labels\n",
    "        test_dataset = FeaturePredictionDataset(\n",
    "            test_data, \n",
    "            test_time,\n",
    "            idx\n",
    "        )\n",
    "        test_dataloader = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=no,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        model = GeneralRNN(args)\n",
    "        model.to(args[\"device\"])\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=args[\"learning_rate\"]\n",
    "        )\n",
    "\n",
    "        logger = trange(args[\"epochs\"], desc=f\"Epoch: 0, Loss: 0\")\n",
    "        for epoch in logger:\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for train_x, train_t, train_y in train_dataloader:\n",
    "                train_x = train_x.to(args[\"device\"])\n",
    "                train_y = train_y.to(args[\"device\"])\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                train_p = model(train_x, train_t)\n",
    "                loss = criterion(train_p, train_y)\n",
    "                # backward\n",
    "                loss.backward()\n",
    "                # optimize\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            logger.set_description(f\"Epoch: {epoch}, Loss: {running_loss:.4f}\")\n",
    "\n",
    "        \n",
    "        # Evaluate the trained model\n",
    "        with torch.no_grad():\n",
    "            temp_perf = 0\n",
    "            for test_x, test_t, test_y in test_dataloader:\n",
    "                test_x = test_x.to(args[\"device\"])\n",
    "                test_p = model(test_x, test_t).cpu().numpy()\n",
    "\n",
    "                test_p = np.reshape(test_p, [-1])\n",
    "                test_y = np.reshape(test_y.numpy(), [-1])\n",
    "        \n",
    "                temp_perf = rmse_error(test_y, test_p)\n",
    "      \n",
    "        perf.append(temp_perf)\n",
    "    \n",
    "    return perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feat_idx = np.random.permutation(train_data.shape[2])[:feat_pred_no]\n",
    "print(\"Running feature prediction using original data...\")\n",
    "ori_feat_pred_perf = feature_prediction(\n",
    "    (train_data, train_time), \n",
    "    (test_data, test_time),\n",
    "    feat_idx\n",
    ")\n",
    "print(\"Running feature prediction using generated data...\")\n",
    "new_feat_pred_perf = feature_prediction(\n",
    "    (generated_data, generated_time),\n",
    "    (test_data, test_time),\n",
    "    feat_idx\n",
    ")\n",
    "feat_pred = [ori_feat_pred_perf, new_feat_pred_perf]\n",
    "print('Feature prediction results:\\n' +\n",
    "      f'(1) Ori: {str(np.round(ori_feat_pred_perf, 4))}\\n' +\n",
    "      f'(2) New: {str(np.round(new_feat_pred_perf, 4))}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
