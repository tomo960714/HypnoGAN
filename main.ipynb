{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomo9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "# Local packages:\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from typing import Dict, Union\n",
    "\n",
    "# 3rd party packages:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm,trange\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# TODO: Implement Neptune logger\n",
    "\n",
    "# personal packages:\n",
    "#from Data.preprocess import preprocess_data\n",
    "#from model.timegan import TimeGAN\n",
    "#from model.utils import timegan_trainer, timegan_generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "module() takes at most 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\98_GitRepo\\00_HypnoGAN\\HypnoGAN\\main.ipynb Cell 5\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/98_GitRepo/00_HypnoGAN/HypnoGAN/main.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mTimeGAN_Dataset\u001b[39;00m(torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mdataset):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/98_GitRepo/00_HypnoGAN/HypnoGAN/main.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\"\"A time series dataset for TimeGAN.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/98_GitRepo/00_HypnoGAN/HypnoGAN/main.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/98_GitRepo/00_HypnoGAN/HypnoGAN/main.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m        data(numpy.ndarray): the padded dataset to be fitted. Has to transform to ndarray from DataFrame during initializ\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/98_GitRepo/00_HypnoGAN/HypnoGAN/main.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m        - t (torch.LongTensor): the temporal feature of the data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/98_GitRepo/00_HypnoGAN/HypnoGAN/main.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/98_GitRepo/00_HypnoGAN/HypnoGAN/main.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,args,data):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/98_GitRepo/00_HypnoGAN/HypnoGAN/main.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m#sanity check data and time\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: module() takes at most 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "class TimeGAN_Dataset(torch.utils.data.dataset):\n",
    "    \"\"\"A time series dataset for TimeGAN.\n",
    "    Args:\n",
    "        data(numpy.ndarray): the padded dataset to be fitted. Has to transform to ndarray from DataFrame during initializ\n",
    "        time(numpy.ndarray): the length of each data\n",
    "    Parameters:\n",
    "        - x (torch.FloatTensor): the real value features of the data\n",
    "        - t (torch.LongTensor): the temporal feature of the data\n",
    "    \"\"\"\n",
    "    def __init__(self,args):\n",
    "        #sanity check data and time\n",
    "        value = args.data['Sleeping stage'].values\n",
    "        time = args.data['time'].values\n",
    "        if len(value) != len(time):\n",
    "            raise ValueError( f\"len(value) `{len(value)}` != len(time) {len(time)}\")\n",
    "        if isinstance(time,type(None)):\n",
    "            time = [len(x) for x in data]\n",
    "        self.X = torch.FloatTensor(value)\n",
    "        self.T = torch.LongTensor(time)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx],self.T[idx]\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Minibatch sampling\n",
    "        \"\"\"\n",
    "        # Pad sequences to max length\n",
    "        X_mb = [X for X in batch[0]]\n",
    "        \n",
    "        # The actual length of each data\n",
    "        T_mb = [T for T in batch[1]]\n",
    "        \n",
    "        return X_mb, T_mb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    The embedding network (encoder) that maps the input data to a latent space.\n",
    "    \"\"\"\n",
    "    def __init__(self,args):\n",
    "        super(EmbeddingNetwork, self).__init__()\n",
    "        self.feature_dim = args.feature_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.num_layers = args.num_layers\n",
    "        self.padding_value = args.padding_value\n",
    "        self. max_seq_len = args.max_seq_len\n",
    "\n",
    "        #Embedder Architecture\n",
    "        self.emb_rnn = nn.GRU(\n",
    "            input_size=self.feature_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.emb_linear = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.emb_sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "        # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L61\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.emb_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(1)\n",
    "\n",
    "            for name, param in self.emb_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "    def forward(self,X,T):\n",
    "        \"\"\"Forward pass of the embedding features from original space to latent space.\n",
    "        Args:\n",
    "            X: Input time series feature (B x S x F)\n",
    "            T: INput temporal information (B)\n",
    "        Returns:\n",
    "            H: latent space embeddings (B x S x H)\n",
    "        \"\"\"\n",
    "        # Dynamic RNN input for ignoring paddings\n",
    "\n",
    "        X_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input =X,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # 128*100*71\n",
    "        H_o,H_t = self.emb_rnn(X_pack)\n",
    "\n",
    "        #pad RNN output back to sequence length\n",
    "\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "\n",
    "        #128*100*10\n",
    "        logits = self.emb_linear(H_o)\n",
    "        H = self.emb_sigmoid(logits)\n",
    "\n",
    "        return H\n",
    "    \n",
    "class RecoveryNetwork(nn.Module):\n",
    "    \"\"\"The recovery network (decoder) for TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,arg):\n",
    "        super(RecoveryNetwork, self).__init__()\n",
    "        self.hidden_dim = arg.hidden_dim\n",
    "        self.feature_dim = arg.feature_dim\n",
    "        self.num_layers = arg.num_layers\n",
    "        self.padding_value = arg.padding_value\n",
    "        self.max_seq_len = arg.max_seq_len\n",
    "\n",
    "        #Recovery Architecture\n",
    "        self.rec_rnn = nn.GRU(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.rec_linear = nn.Linear(self.hidden_dim, self.feature_dim)\n",
    "\n",
    "        # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name,param in self.rec_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(0)\n",
    "            for name,param in self.rec_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        \n",
    "    def forward(self,H,T):\n",
    "        \"\"\" Forward pass of the recovery features from latent space to original space.\n",
    "        Args:\n",
    "            H: latent representation (B x S x E)\n",
    "            T: input temporal information (B)\n",
    "        Returns:\n",
    "            X_tilde: recovered features (B x S x F)\n",
    "        \"\"\"\n",
    "        #Dynamic RNN input for ignoring paddings\n",
    "        H_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input = H,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        #128 x 100 x 10\n",
    "        H_o,H_t = self.rec_rnn(H_pack)\n",
    "        #pad RNN output back to sequence length\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "        #128 x 100 x 71\n",
    "        X_tilde = self.rec_linear(H_o)\n",
    "        return X_tilde\n",
    "\n",
    "class SupervisorNetwork(nn.Module):\n",
    "        \"\"\"The supervisor network for TimeGAN\n",
    "        \"\"\"\n",
    "        def __init__(self,args):\n",
    "            super(SupervisorNetwork,self).__init__()\n",
    "            self.hidden_dim = args.hidden_dim\n",
    "            self.num_layers = args.num_layers\n",
    "            self.padding_value = args.padding_value\n",
    "            self.max_seq_len = args.max_seq_len\n",
    "\n",
    "            #supervisor architecture\n",
    "            self.sup_rnn = nn.GRU(\n",
    "                input_size=self.hidden_dim,\n",
    "                hidden_size=self.hidden_dim,\n",
    "                num_layers=self.num_layers-1,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            self.sup_linear = nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "            self.sup_sigmoid = nn.Sigmoid()\n",
    "             # Init weights\n",
    "            # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "            # Reference: \n",
    "            # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "            # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "            with torch.no_grad():\n",
    "                for name, param in self.sup_rnn.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'bias_ih' in name:\n",
    "                        param.data.fill_(1)\n",
    "                    elif 'bias_hh' in name:\n",
    "                        param.data.fill_(0)\n",
    "                for name, param in self.sup_linear.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        param.data.fill_(0)\n",
    "        def forward(self,H,T):\n",
    "            \"\"\"Forward pass for the supervisor for predicting next step\n",
    "            Args:\n",
    "                H: latent representation (B x S x E)\n",
    "                T: input temporal information (B)\n",
    "            Returns:\n",
    "                H_hat: predicted next step data (B x S x E)\n",
    "            \"\"\"\n",
    "\n",
    "            #Dynamic RNN input for ignoring paddings\n",
    "            H_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "                input = H,\n",
    "                lengths=T,\n",
    "                batch_first=True,\n",
    "                enforce_sorted=False,\n",
    "            )\n",
    "\n",
    "            H_o,H_t = self.sup_rnn(H_pack)\n",
    "            #pad RNN output back to sequence length\n",
    "            H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "                sequence=H_o,\n",
    "                batch_first=True,\n",
    "                padding_value=self.padding_value,\n",
    "                total_length=self.max_seq_len,\n",
    "            )\n",
    "            logits = self.sup_linear(H_o)\n",
    "            H_hat = self.sup_sigmoid(logits)\n",
    "            return H_hat\n",
    "\n",
    "class GeneratorNetwork(nn.Module):\n",
    "    \"\"\"The generator network for TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,args):\n",
    "        super(GeneratorNetwork,self).__init__()\n",
    "        self.Z_dim = args.Z_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.num_layers = args.num_layers\n",
    "        self.padding_value = args.padding_value\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "\n",
    "        #Generator Architecture\n",
    "        self.gen_rnn = nn.GRU(\n",
    "            input_size=self.Z_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.gen_linear = nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "        self.gen_sigmoid = nn.Sigmoid()\n",
    "                # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.gen_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(0)\n",
    "            for name, param in self.gen_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        \n",
    "    def forward(self,Z,T):\n",
    "        \"\"\" Takes in random noise (features) and generates synthetic features within the last latent space\n",
    "        Args:\n",
    "            Z: input random noise (B x S x Z)\n",
    "            T: input temporal information (B)\n",
    "        Returns:\n",
    "            H: embeddings (B x S x E)\n",
    "        \"\"\"\n",
    "        #Dynamic RNN input for ignoring paddings\n",
    "        Z_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input = Z,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # 128*100*71\n",
    "        H_o,H_t = self.gen_rnn(Z_pack)\n",
    "\n",
    "        #pad RNN output back to sequence length\n",
    "\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "\n",
    "        #128*100*10\n",
    "        logits = self.gen_linear(H_o)\n",
    "        H = self.gen_sigmoid(logits)\n",
    "\n",
    "        return H\n",
    "\n",
    "class DiscriminatorNetwork(nn.Module):\n",
    "    \"\"\"The discriminator network for TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,args):\n",
    "        super(DiscriminatorNetwork,self).__init__()\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.num_layers = args.num_layers\n",
    "        self.padding_value = args.padding_value\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "\n",
    "        #Discriminator Architecture\n",
    "        self.dis_rnn = nn.GRU(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dis_linear = nn.Linear(self.hidden_dim,1)\n",
    "\n",
    "        # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.dis_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(0)\n",
    "            for name, param in self.dis_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "    \n",
    "    def forward(self, H, T):\n",
    "        \"\"\" Forward pass for predicting if the data is real or synthetic\n",
    "        \n",
    "        Args:\n",
    "            H: latent representation (B x S x E)\n",
    "            T: input temporal information (B)\n",
    "        Returns:\n",
    "        logits: prediction logits(B x S x 1)\n",
    "        \"\"\"\n",
    "        # dynamic RNN input for ignoring paddings\n",
    "        H_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input = H,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # 128*100*10\n",
    "        H_o,H_t = self.dis_rnn(H_pack)\n",
    "\n",
    "        # pad RNN output back to sequence length\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "\n",
    "        logits = self.dis_linear(H_o).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "class TimeGAN(nn.Module):\n",
    "    \"\"\" Implementation of TimeGan (Yoon et al., 2019) using PyTorch\n",
    "    \n",
    "    Reference:\n",
    "        - Yoon, J., Jarret, D., van der Schaar, M. (2019). Time-series Generative Adversarial Networks. (https://papers.nips.cc/paper/2019/hash/c9efe5f26cd17ba6216bbe2a7d26d490-Abstract.html)\n",
    "        - https://github.com/jsyoon0823/TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,args):\n",
    "        super(TimeGAN,self).__init__()\n",
    "        self.device =args.device\n",
    "        self.feature_dim = args.feature_dim\n",
    "        self.Z_dim = args.Z_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        self.embedder = EmbeddingNetwork(args)\n",
    "        self.recovery = RecoveryNetwork(args)\n",
    "        self.generator = GeneratorNetwork(args)\n",
    "        self.discriminator = DiscriminatorNetwork(args)\n",
    "        self.supervisor = SupervisorNetwork(args)\n",
    "\n",
    "    def _recovery_forward(self, X, T):\n",
    "        \"\"\" The embedding network forward pass and the embedder network loss\n",
    "        Args:\n",
    "            X: input features\n",
    "            T: input temporal information\n",
    "        Returns:\n",
    "            E_loss: the reconstruction loss\n",
    "            X_tilde: the reconstructed features\n",
    "        \"\"\"\n",
    "\n",
    "        # FOrward pass\n",
    "        H = self.embedder(X,T)\n",
    "        X_tilde = self.recovery(H,T)\n",
    "\n",
    "        #for Joint training\n",
    "        H_hat_supervise = self.supervisor(H,T)\n",
    "        G_loss_S = F.mse_loss(\n",
    "            H_hat_supervise[:,:-1,:],\n",
    "            H[:,1:,:],\n",
    "        ) #Teacher forcing next output\n",
    "\n",
    "        #Reconstruction loss\n",
    "        E_loss_T0 = F.mse_loss(X_tilde,X)\n",
    "        E_loss0 = 10*torch.sqrt(E_loss_T0)\n",
    "        E_loss = E_loss0 + 0.1*G_loss_S\n",
    "        return E_loss, E_loss0,E_loss_T0\n",
    "    def _supervisor_forward(self, X, T):\n",
    "        \"\"\" The supervisor training forward pass\n",
    "        Args:\n",
    "            X: original input features\n",
    "            T: input temporal information\n",
    "        Returns:\n",
    "            S_loss: the supervisor's loss\n",
    "        \"\"\"\n",
    "        #supervisor forward pass\n",
    "        H = self.embedder(X,T)\n",
    "        H_hat_supervise = self.supervisor(H,T)\n",
    "\n",
    "        #supervised loss\n",
    "        S_loss = F.mse_loss(\n",
    "            H_hat_supervise[:,:-1,:],\n",
    "            H[:,1:,:],\n",
    "        ) #Teacher forcing next output\n",
    "        return S_loss\n",
    "    def _discriminator_forward(self, X, T, Z, gamma=1):\n",
    "        \"\"\" The discriminator forward pass and adversarial loss\n",
    "        Args:\n",
    "            X: input features\n",
    "            T: input temporal information\n",
    "            Z: input noise\n",
    "            gamma: the weight for the adversarial loss\n",
    "        Returns:\n",
    "            D_loss: adversarial loss\n",
    "        \"\"\"\n",
    "        #Real\n",
    "        H = self.embedder(X, T).detach()\n",
    "\n",
    "        #generator\n",
    "        E_hat = self.generator(Z,T).detach()\n",
    "        H_hat = self.supervisor(E_hat,T).detach()\n",
    "        \n",
    "        #forward pass\n",
    "        Y_real = self.discriminator(H,T)        #Encode original data\n",
    "        Y_fake = self.discriminator(H_hat,T)    #Output of generator + supervisor\n",
    "        Y_fake_e = self.discriminator(E_hat,T)  #Output of generator\n",
    "\n",
    "        D_loss_real = F.binary_cross_entropy_with_logits(Y_real, torch.ones_like(Y_real))\n",
    "        D_loss_fake = F.binary_cross_entropy_with_logits(Y_fake, torch.zeros_like(Y_fake))\n",
    "        D_loss_fake_e = F.binary_cross_entropy_with_logits(Y_fake_e, torch.zeros_like(Y_fake_e))\n",
    "\n",
    "        D_loss = D_loss_real + D_loss_fake + gamma * D_loss_fake_e\n",
    "\n",
    "        return D_loss\n",
    "    \n",
    "    def _generator_forward(self, X, T, Z, gamma=1):\n",
    "        \"\"\" The generator forward pass\n",
    "        Args:\n",
    "            X: original input features\n",
    "            T: input temporal information\n",
    "            Z: input noise for the generator\n",
    "            gamma: the weight for the adversarial loss\n",
    "        Returns:\n",
    "            G_loss: the generator loss\n",
    "        \"\"\"\n",
    "        #supervisor forward pass\n",
    "        H = self.embedder(X,T)\n",
    "        H_hat_supervise = self.supervisor(H,T)\n",
    "\n",
    "        #generator forward pass\n",
    "        E_hat = self.generator(Z,T)\n",
    "        H_hat = self.supervisor(E_hat,T)\n",
    "\n",
    "        #synthetic data generated\n",
    "        X_hat = self.recovery(H_hat,T)\n",
    "\n",
    "        #generator loss\n",
    "        #Adversarial loss\n",
    "        Y_fake = self.discriminator(H_hat,T)        #Output of supervisor\n",
    "        Y_fake_e = self.discriminator(E_hat,T)      #Output of generator\n",
    "\n",
    "        G_loss_U = F.binary_cross_entropy_with_logits(Y_fake, torch.ones_like(Y_fake))\n",
    "        G_loss_U_e = F.binary_cross_entropy_with_logits(Y_fake_e, torch.ones_like(Y_fake_e))\n",
    "\n",
    "        #Supervised loss\n",
    "        G_loss_S = F.mse_loss(\n",
    "            H_hat_supervise[:,:-1,:],\n",
    "            H[:,1:,:],\n",
    "        ) #Teacher forcing next output\n",
    "\n",
    "        #Two moments losses\n",
    "        G_loss_V1 = torch.mean(\n",
    "            torch.abs(torch.sqrt(X_hat.var(dim=0,unbiased=False)+1e-6) - torch.sqrt(X.var(dim=0,unbiased=False)+1e-6))\n",
    "        )\n",
    "        G_loss_V2 = torch.mean(torch.abs((X_hat.mean(dim=0)) - (X.mean(dim=0))))\n",
    "        G_loss_V = G_loss_V1 + G_loss_V2\n",
    "        \n",
    "        #sum of losses\n",
    "        G_loss = G_loss_U + gamma * G_loss_U_e + 100 * torch.sqrt(G_loss_S) + 100 * G_loss_V\n",
    "    \n",
    "        return G_loss\n",
    "    \n",
    "    def _inference(self, Z,T):\n",
    "        \"\"\" Inference for generating synthetic data\n",
    "        Args:\n",
    "            Z: input noise\n",
    "            T: temporal information\n",
    "        Returns:\n",
    "            X_hat: the generated data\n",
    "        \"\"\"\n",
    "\n",
    "        #generator forward pass\n",
    "        E_hat = self.generator(Z,T)\n",
    "        H_hat = self.supervisor(E_hat,T)\n",
    "\n",
    "        #synthetic data generated\n",
    "        X_hat = self.recovery(H_hat,T)\n",
    "        return X_hat\n",
    "\n",
    "    def forward(self,X,T,Z, obj, gamma=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: input features (B,H,F)\n",
    "            T: The temporal information (B)\n",
    "            Z: the sampled noise (B,H,Z)\n",
    "            obj: the network to be trained ('autoencoder','supervisor','generator','discriminator')\n",
    "            gamma: loss hyperparameter\n",
    "        Returns:\n",
    "            loss: loss for the forward pass\n",
    "            X_hat: the generated data\n",
    "        \"\"\"\n",
    "\n",
    "        #Move variables to device\n",
    "        if obj !='inference':\n",
    "            if X is None:\n",
    "                raise ValueError('X cannot be empty')\n",
    "            \n",
    "            X = torch.FloatTensor(X)\n",
    "            X = X.to(self.device)\n",
    "\n",
    "        if Z is not None:\n",
    "            Z = torch.FloatTensor(Z)\n",
    "            Z = Z.to(self.device)\n",
    "        \n",
    "        if obj == 'autoencoder':\n",
    "            #embedder and recovery forward\n",
    "            loss = self._recovery_forward(X,T)\n",
    "        elif obj == 'supervisor':\n",
    "            loss = self._supervisor_forward(X,T)\n",
    "        elif obj == 'generator':\n",
    "            if Z is None:\n",
    "                raise ValueError('Z cannot be empty')\n",
    "            loss = self._generator_forward(X,T,Z,gamma)\n",
    "        elif obj == 'discriminator':\n",
    "            if Z is None:\n",
    "                raise ValueError('Z cannot be empty')\n",
    "            loss = self._discriminator_forward(X,T,Z,gamma)\n",
    "            return loss\n",
    "        elif obj == 'inference':\n",
    "            X_hat = self._inference(Z,T)\n",
    "            X_hat = X_hat.cpu.detach()\n",
    "\n",
    "            return X_hat\n",
    "        else:\n",
    "            raise ValueError('obj must be autoencoder, supervisor, generator or discriminator')\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_trainer(\n",
    "        model: torch.nn.Module,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        e_opt: torch.optim.Optimizer,\n",
    "        r_opt: torch.optim.Optimizer,\n",
    "        args: Dict,\n",
    "        writer: Union[torch.utils.tensorboard.SummaryWriter, type(None)]=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Training loop for embedding and recovery functions.\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader to use\n",
    "        e_opt (torch.optim.Optimizer): The optimizer for the embedding function\n",
    "        r_opt (torch.optim.Optimizer): The optimizer for the recovery function\n",
    "        args (Dict): The model/training configuration\n",
    "        writer (Union[torch.utils.tensorboard.SummaryWriter, type(None)], optional): The tensorboard writer to use. Defaults to None.\n",
    "    \"\"\"\n",
    "    logger = trange(args.emb_epochs, desc =f\"Epoch:0, Loss:0\")\n",
    "    for epoch in logger:\n",
    "        for X_mb,T_mb in dataloader:\n",
    "\n",
    "            #reset gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            #forward pass\n",
    "            _,E_loss0,E_loss_T0 = model(X=X_mb,T=T_mb,Z=None,obj=\"autoencoder\")\n",
    "            loss = np.sqrt(E_loss_T0.item())\n",
    "\n",
    "            #backward pass\n",
    "            E_loss0.backward()\n",
    "\n",
    "            #update weights\n",
    "            e_opt.step()\n",
    "            r_opt.step()\n",
    "\n",
    "        # Log loss for final batch of each epochs\n",
    "        logger.set_description(f\"Epoch:{epoch}, Loss:{loss:.4f}\")\n",
    "        if writer:\n",
    "            writer.add_scalar(\"Embedding/Loss:\",loss,epoch)\n",
    "            writer.flush()\n",
    "\n",
    "def supervisor_trainer(\n",
    "    model: torch.nn.Module, \n",
    "    dataloader: torch.utils.data.DataLoader, \n",
    "    s_opt: torch.optim.Optimizer, \n",
    "    g_opt: torch.optim.Optimizer, \n",
    "    args: Dict, \n",
    "    writer: Union[torch.utils.tensorboard.SummaryWriter, type(None)]=None\n",
    "):\n",
    "    \"\"\"\n",
    "    The training loop for the supervisor function\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader to use\n",
    "        s_opt (torch.optim.Optimizer): The optimizer for the supervisor function\n",
    "        g_opt (torch.optim.Optimizer): The optimizer for the generator function\n",
    "        args (Dict): The model/training configuration\n",
    "        writer (Union[torch.utils.tensorboard.SummaryWriter, type(None)], optional): The tensorboard writer to use. Defaults to None.\n",
    "    \"\"\"\n",
    "    logger = trange(args.sup_epochs, desc=f\"Epoch: 0, Loss: 0\")\n",
    "    for epoch in logger:\n",
    "        for X_mb, T_mb in dataloader:\n",
    "            # Reset gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Forward Pass\n",
    "            S_loss = model(X=X_mb, T=T_mb, Z=None, obj=\"supervisor\")\n",
    "\n",
    "            # Backward Pass\n",
    "            S_loss.backward()\n",
    "            loss = np.sqrt(S_loss.item())\n",
    "\n",
    "            # Update model parameters\n",
    "            s_opt.step()\n",
    "\n",
    "        # Log loss for final batch of each epoch (29 iters)\n",
    "        logger.set_description(f\"Epoch: {epoch}, Loss: {loss:.4f}\")\n",
    "        if writer:\n",
    "            writer.add_scalar(\n",
    "                \"Supervisor/Loss:\",loss,epoch)\n",
    "            writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_trainer(\n",
    "    model: torch.nn.Module, \n",
    "    dataloader: torch.utils.data.DataLoader, \n",
    "    e_opt: torch.optim.Optimizer, \n",
    "    r_opt: torch.optim.Optimizer, \n",
    "    s_opt: torch.optim.Optimizer, \n",
    "    g_opt: torch.optim.Optimizer, \n",
    "    d_opt: torch.optim.Optimizer, \n",
    "    args: Dict, \n",
    "    writer: Union[torch.utils.tensorboard.SummaryWriter, type(None)]=None, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    The training loop for training the model altogether\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader to use\n",
    "        e_opt (torch.optim.Optimizer): The optimizer for the embedding function\n",
    "        r_opt (torch.optim.Optimizer): The optimizer for the recovery function\n",
    "        s_opt (torch.optim.Optimizer): The optimizer for the supervisor function\n",
    "        g_opt (torch.optim.Optimizer): The optimizer for the generator function\n",
    "        d_opt (torch.optim.Optimizer): The optimizer for the discriminator function\n",
    "        args (Dict): The model/training configuration\n",
    "        writer (Union[torch.utils.tensorboard.SummaryWriter, type(None)], optional): The tensorboard writer to use. Defaults to None.\n",
    "    \"\"\"\n",
    "    logger = trange(\n",
    "        args.sup_epochs, \n",
    "        desc=f\"Epoch: 0, E_loss: 0, G_loss: 0, D_loss: 0\"\n",
    "    )\n",
    "    for epoch in logger:\n",
    "        for X_mb, T_mb in dataloader:\n",
    "            ## Generator Training\n",
    "            for _ in range(2):\n",
    "                # Random Generator\n",
    "                Z_mb = torch.rand((args.batch_size, args.max_seq_len, args.Z_dim))\n",
    "\n",
    "                # Forward Pass (Generator)\n",
    "                model.zero_grad()\n",
    "                G_loss = model(X=X_mb, T=T_mb, Z=Z_mb, obj=\"generator\")\n",
    "                G_loss.backward()\n",
    "                G_loss = np.sqrt(G_loss.item())\n",
    "\n",
    "                # Update model parameters\n",
    "                g_opt.step()\n",
    "                s_opt.step()\n",
    "\n",
    "                # Forward Pass (Embedding)\n",
    "                model.zero_grad()\n",
    "                E_loss, _, E_loss_T0 = model(X=X_mb, T=T_mb, Z=Z_mb, obj=\"autoencoder\")\n",
    "                E_loss.backward()\n",
    "                E_loss = np.sqrt(E_loss.item())\n",
    "                \n",
    "                # Update model parameters\n",
    "                e_opt.step()\n",
    "                r_opt.step()\n",
    "\n",
    "            # Random Generator\n",
    "            Z_mb = torch.rand((args.batch_size, args.max_seq_len, args.Z_dim))\n",
    "\n",
    "            ## Discriminator Training\n",
    "            model.zero_grad()\n",
    "            # Forward Pass\n",
    "            D_loss = model(X=X_mb, T=T_mb, Z=Z_mb, obj=\"discriminator\")\n",
    "\n",
    "            # Check Discriminator loss\n",
    "            if D_loss > args.dis_thresh:\n",
    "                # Backward Pass\n",
    "                D_loss.backward()\n",
    "\n",
    "                # Update model parameters\n",
    "                d_opt.step()\n",
    "            D_loss = D_loss.item()\n",
    "\n",
    "        logger.set_description(\n",
    "            f\"Epoch: {epoch}, E: {E_loss:.4f}, G: {G_loss:.4f}, D: {D_loss:.4f}\"\n",
    "        )\n",
    "        if writer:\n",
    "            writer.add_scalar(\n",
    "                'Joint/Embedding_Loss:',E_loss,epoch)\n",
    "            writer.add_scalar(\n",
    "                'Joint/Generator_Loss:',G_loss,epoch)\n",
    "            writer.add_scalar('Joint/Discriminator_Loss:',D_loss,epoch)\n",
    "            writer.flush()\n",
    "\n",
    "def timegan_trainer(model,loaded_data,args):\n",
    "    \"\"\"\n",
    "    The trainign procedure for TimeGAN.\n",
    "    Args:\n",
    "        model (torch.nn.module): The model that generates synthetic data\n",
    "        loaded_data(pandas.DataFrame): The data to train on, including data and time\n",
    "        args (Dict): The model/training configuration\n",
    "    Returns:\n",
    "        generated_data (np.array): The synthetic data generated by the model\n",
    "    \"\"\"\n",
    "    dataset = TimeGAN_Dataset(args=args,data=loaded_data[\"data\"],time=loaded_data[\"time\"])\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    model.to(args.device)\n",
    "\n",
    "    #initialize optimizers\n",
    "    e_opt = torch.optim.Adam(model.embedder.parameters(), lr=args.lr)\n",
    "    r_opt = torch.optim.Adam(model.recovery.parameters(), lr=args.lr)\n",
    "    s_opt = torch.optim.Adam(model.supervisor.parameters(), lr=args.lr)\n",
    "    g_opt = torch.optim.Adam(model.generator.parameters(), lr=args.lr)\n",
    "    d_opt = torch.optim.Adam(model.discriminator.parameters(), lr=args.lr)\n",
    "\n",
    "    #initialize tensorboard writer\n",
    "    writer = SummaryWriter(os.path.join(f\"tensorboard/{args.exp}\"))\n",
    "\n",
    "    print(\"\\nStart Embedding Network Training\")\n",
    "    embedding_trainer(model=model, dataloader=dataloader, e_opt=e_opt, r_opt=r_opt, args=args, writer=writer)\n",
    "\n",
    "    print(\"\\nStart Training with Supervised Loss Only\")\n",
    "    supervisor_trainer(model=model, dataloader=dataloader, s_opt=s_opt,g_opt=g_opt, args=args, writer=writer)\n",
    "\n",
    "    print(\"\\nStart Joint Training\")\n",
    "    joint_trainer(model=model, dataloader=dataloader, e_opt=e_opt, r_opt=r_opt, s_opt=s_opt, g_opt=g_opt, d_opt=d_opt, args=args, writer=writer)\n",
    "\n",
    "\n",
    "    #save model,args, and hyperparameters\n",
    "    torch.save(args,f\"{args.model_path}/args.pickle\")\n",
    "    torch.save(model.state_dict(),f\"{args.model_path}/model.pt\")\n",
    "    print(f\"Model saved to {args.model_path}\")\n",
    "\n",
    "def timegan_generator(model,T,args):\n",
    "    \"\"\"\n",
    "    The interference procedure for TimeGAN.\n",
    "    Args:\n",
    "        model (torch.nn.module): The model that generates synthetic data\n",
    "        T (List[int]): The time to generate data for\n",
    "        args (Dict): The model/training configuration\n",
    "    returns:\n",
    "        generated_data (np.array): The synthetic data generated by the model\n",
    "    \"\"\"\n",
    "    #load model\n",
    "    if not os.path.exists(args.model_path):\n",
    "        raise ValueError(f\"Model not found at {args.model_path}\")\n",
    "    model.load_state_dict(torch.load(f\"{args.model_path}/model.pt\"))\n",
    "    print(\"\\nStart Generating Synthetic Data\")\n",
    "    #Initialize model to evaluation mode and run without gradients\n",
    "    model.to(args.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Random Generator\n",
    "        Z = torch.rand((args.batch_size, args.max_seq_len, args.Z_dim))\n",
    "        # Forward Pass (Generator)\n",
    "        generated_data = model(X=None, T=T, Z=Z, obj=\"inference\")\n",
    "    return generated_data.numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args):\n",
    "    \"\"\"\n",
    "    data_limit=None,save_dataset=None\n",
    "    Load and preprocess real life datasets.\n",
    "    \n",
    "    Args:\n",
    "        data_limit (int): The number of data points to load. If None, all data points are loaded. Default: None. Used for testing.\n",
    "        save_dataset (bool): If 'Full', the dataset is saved to a csv file. If it's 'limited', than save the limited dataset if data_limit is not None. Default: None.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        dataset (pandas.DataFrame): The dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    main_dataset = pd.DataFrame()\n",
    "\n",
    "    \n",
    "    Tk().withdraw() # we don't want a full GUI, so keep the root window from appearing\n",
    "    filenames = askopenfilenames() # show an \"Open\" dialog box and return the path to the selected file\n",
    "    print(filenames)\n",
    "    for filename in filenames:\n",
    "        \n",
    "        if filename.endswith('.mat'):\n",
    "            #output df format: [id,value_array]\n",
    "            df = load_mat_as_df(filename)\n",
    "            print(df)\n",
    "        \n",
    "        elif filename.endswith('.csv'):\n",
    "\n",
    "            #use create_dataset_csv.py to create a csv file\n",
    "            if filename.find('dataset') != -1:\n",
    "                df = pd.read_csv(filename,sep=';',index_col=0)\n",
    "\n",
    "            \"\"\" CSV format:\n",
    "            ID|time|Sleeping stage|length|additional_info\n",
    "\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        elif filename.endswith('.xml'):\n",
    "            ## TODO: add xml support\n",
    "            #df =\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Unsupported file format, skipping file:\",filename,\".\")\n",
    "            pass\n",
    "    \n",
    "        main_dataset.append(df)\n",
    "\n",
    "    #Cut df to data_limit size for testing purposes\n",
    "    if args.data_limit is not None:\n",
    "        if args.save_dataset == 'Full':\n",
    "            #save dataset to a csv file\n",
    "            main_dataset.to_csv('Full_dataset.csv',sep=';')\n",
    "        \n",
    "        elif args.save_dataset == 'Limited':\n",
    "            main_dataset = main_dataset[:args.data_limit]\n",
    "            #save dataset to a csv file\n",
    "            main_dataset.to_csv('limited_dataset.csv',sep=';')\n",
    "        elif args.save_dataset == 'None':\n",
    "            main_dataset = main_dataset[:args.data_limit]\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Invalid save_dataset value, valid values are 'Full','Limited','None'.\")\n",
    "\n",
    "    elif args.data_limit is None:\n",
    "        if args.save_dataset == 'Full':\n",
    "            #save dataset to a csv file\n",
    "            main_dataset.to_csv('full_dataset.csv',sep=';')\n",
    "        elif args.save_dataset == 'Limited':\n",
    "            print(\"Warning: data_limit is None, dataset is not limited, saving full dataset.\")\n",
    "            main_dataset.to_csv('full_dataset.csv',sep=';')\n",
    "        elif args.save_dataset == 'None':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Invalid save_dataset value, valid values are 'Full','Limited','None'.\")\n",
    "\n",
    "    \n",
    "    return main_dataset #dataset as df\n",
    "\n",
    "def load_mat_as_df(mat_file_path, var_name):\n",
    "    mat = sio.loadmat(mat_file_path,simplify_cells=True)\n",
    "\n",
    "    if var_name not in list(mat.keys()):\n",
    "        var_name = get_variable_name(mat)   \n",
    "        \n",
    "\n",
    "    return pd.DataFrame(mat[var_name])\n",
    "\n",
    "def get_variable_name(loaded_mat):\n",
    "\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    root.title('.mat variable selector')\n",
    "    tk.Label(root, text=\"Choose a variable:\").pack()\n",
    "    choices = list(loaded_mat.keys())\n",
    "\n",
    "    variable = tk.StringVar(root)\n",
    "    variable.set(choices[0]) # default value\n",
    "    w = tk.Combobox(root, textvariable=variable,values=choices)\n",
    "\n",
    "    w.pack()\n",
    "    def ok():\n",
    "        print (\"value is:\" + variable.get())\n",
    "        root.destroy()\n",
    "    def cancel():\n",
    "        root.destroy()\n",
    "        raise InterruptedError('User cancelled, invalid variable name')\n",
    "\n",
    "    button1 = tk.Button(root, text=\"OK\", command=ok)\n",
    "    button2 = tk.Button(root, text=\"Cancel\", command=cancel)\n",
    "    button1.pack()\n",
    "    button2.pack()\n",
    "    root.mainloop()\n",
    "    \n",
    "    return variable.get()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(args):\n",
    "    \"\"\"\n",
    "    padding_value: int = -1.0,\n",
    "    data_limit: int = None\"\"\"\n",
    "    # Load and preprocess data\n",
    "    # \n",
    "    # 1. Load data from files (csv,mat,xml)\n",
    "    # 2. Preprocess data:\n",
    "    # 2.1. Remove outliers\n",
    "    # 2.2. Extract sequence length and time\n",
    "    # 2.3. Resample data\n",
    "    # 2.4. Normalize data\n",
    "    # 2.5. Padding \n",
    "    #  \n",
    "    # Args:\n",
    "    #     data_limit (int): The number of lines to load from the data file\n",
    "    #     padding_value (int): The value used for padding\n",
    "    #     \n",
    "    # \n",
    "    # Returns:\n",
    "    #     prep_data (pandas.DataFrame): The processed data\n",
    "\n",
    "    #######################################\n",
    "    # 1. Load data from files (csv,mat,xml)\n",
    "    #######################################\n",
    "\n",
    "    loaded_data = load_data(data_limit=args.data_limit)\n",
    "    \"\"\"\n",
    "    loaded data =       time_data   , data  , length\n",
    "    (pandas.DataFrame), (np.array)  ,(list) ,(int)\n",
    "    \n",
    "    ()\n",
    "    \"\"\"\n",
    "    #######################################\n",
    "    # 2. Preprocess data:\n",
    "    #######################################\n",
    "    # 2.1. Remove outliers\n",
    "    #######################################\n",
    "    \"\"\"\n",
    "    Remove row's with unacceptable sleep stages values\n",
    "    \"\"\"\n",
    "    sleep_stages = np.array([1,2,3,4,5])\n",
    "    loaded_data[loaded_data['data'].apply(lambda x: all(elem in sleep_stages for elem in x))]\n",
    "\n",
    "    #######################################\n",
    "    # 2.2. Extract sequence length and time\n",
    "    #######################################\n",
    "    \"\"\"\n",
    "    Extract sequence length of all lines and time of each line\n",
    "    \"\"\"\n",
    "    loaded_data['length'] = loaded_data['data'].apply(lambda x: len(x))\n",
    "\n",
    "    \n",
    "    #######################################\n",
    "    # 2.4. Normalize data\n",
    "    #######################################\n",
    "    \"\"\"\n",
    "    Normalize data to [0,1] using MinMaxScaler algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    if args.norm_enable == True:\n",
    "        loaded_data['data']=MinMaxNormalizer(loaded_data['data'])\n",
    "    \n",
    "\n",
    "    #######################################\n",
    "    # 2.5. Padding\n",
    "    #######################################\n",
    "    \"\"\"\n",
    "    Padding data to given length\n",
    "    \"\"\"\n",
    "   \n",
    "    # Question: Current padding value is 0, is it ok? Do we need it or just resample?\n",
    "    data_info = {\n",
    "        'length' : len(loaded_data),\n",
    "        'max_length' : max(loaded_data['length']),\n",
    "        'paddding_value' : args.padding_value,\n",
    "\n",
    "    }\n",
    "    \n",
    "    loaded_data['data'] = loaded_data['data'].apply(lambda x: np.transpose(x))\n",
    "    prep_data = pd.DataFrame(columns=['time','data'])\n",
    "    for i in tqdm(range(data_info.length)):\n",
    "        #create empty array with padding value\n",
    "        tmp_array = np.empty([data_info.max_length,1])\n",
    "        tmp_array.fill(args.padding_value)\n",
    "        #fill array with data\n",
    "        tmp_array[:loaded_data['data'][i].shape[0],:loaded_data['data'][i].shape[1]] = loaded_data['data'][i]\n",
    "        #append to prep_data\n",
    "        prep_data.append(tmp_array)\n",
    "\n",
    "    return prep_data.to_numpy(),loaded_data['time_data'].to_numpy(),data_info\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def MinMaxNormalizer(data,min_value=1,max_value=5):\n",
    "    numerator = data-min_value\n",
    "    denominator = max_value-min_value\n",
    "    norm_data = numerator/denominator\n",
    "    return norm_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# Experiment Arguments\n",
    "parser.add_argument(\n",
    "    \"--device\",\n",
    "    choices=[\"cuda\", \"cpu\"],\n",
    "    default=\"cuda\",\n",
    "    type=str,\n",
    "    help=\"Device to use for training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",\n",
    "    default=0,\n",
    "    type=int,\n",
    "    help=\"Random seed for reproducibility\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--exp\",\n",
    "    default=\"test\",\n",
    "    type=str,\n",
    "    help=\"Experiment name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--norm_enable\",\n",
    "    default=False,\n",
    "    type=bool,\n",
    "    help=\"Enable normalization\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_dataset\",\n",
    "    choices=[\"Full\",\"Limited\",\"None\"],\n",
    "    default=\"Full\",\n",
    "    type=str,\n",
    "    help=\"Save loaded datasets as csv\",\n",
    ")\n",
    "# Data Arguments\n",
    "parser.add_argument(\n",
    "    \"--data_limit\",\n",
    "    default=None,\n",
    "    type=int,\n",
    "    help=\"Number of data points to use (None or int)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train_rate\",\n",
    "    default=0.6,\n",
    "    type=float,\n",
    "    help=\"Train test split rate\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_seq_len\",\n",
    "    default=1000,\n",
    "    type=int,\n",
    "    help=\"Maximum sequence length\",\n",
    ")\n",
    "\n",
    "# Model Arguments\n",
    "parser.add_argument(\n",
    "    \"--emb_epochs\",\n",
    "    default=600,\n",
    "    type=int,\n",
    "    help=\"Number of epochs to train embedding model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gan_epochs\",\n",
    "    default=600,\n",
    "    type=int,\n",
    "    help=\"Number of epochs to train GAN model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--sup_epochs\",\n",
    "    default=600,\n",
    "    type=int,\n",
    "    help=\"Number of epochs to train supervised model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\",\n",
    "    default=64,\n",
    "    type=int,\n",
    "    help=\"Batch size for training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",\n",
    "    default=20,\n",
    "    type=int,\n",
    "    help=\"Hidden dimension of RNN\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_layers\",\n",
    "    default=3,\n",
    "    type=int,\n",
    "    help=\"Number of layers in RNN\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dis_thresh\",\n",
    "    default=0.15,\n",
    "    type=float,\n",
    "    help=\"Discriminator threshold\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--padding_value\",\n",
    "    default=0,\n",
    "    type=int,\n",
    "    help=\"Data padding value\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--learning_rate',\n",
    "    default=1e-3,\n",
    "    type=float)\n",
    "args = parser.parse_args()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypnogan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
