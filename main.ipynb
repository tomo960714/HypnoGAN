{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_219644/221579484.py:26: NeptuneWarning: To avoid unintended consumption of logging hours during interactive sessions, the following monitoring options are disabled unless set to 'True' when initializing the run: 'capture_stdout', 'capture_stderr', and 'capture_hardware_metrics'.\n",
      "  run = neptune.init_run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/NTLAB/HypnoGAN/e/HYPNOG-150\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "# Local packages:\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "# 3rd party packages:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm,trange\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilenames\n",
    "import ast \n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import neptune\n",
    "from neptune.types import File\n",
    "run = neptune.init_run(\n",
    "    project=\"NTLAB/HypnoGAN\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhNGRjNDgzOC04OTk5LTQ0YTktYjQ4Ny1hMTE4NzRjNjBiM2EifQ==\",\n",
    "    source_files=[\"main.ipynb\", \"Data/create_dataset_csv.ipynb\"],\n",
    ")\n",
    "\n",
    "\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "#torch.cuda.empty_cache()\n",
    "#gc.collect()\n",
    "\n",
    "# personal packages:\n",
    "#from Data.preprocess import preprocess_data\n",
    "#from model.timegan import TimeGAN\n",
    "#from model.utils import timegan_trainer, timegan_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeGAN_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"A time series dataset for TimeGAN.\n",
    "    Args:\n",
    "        data(numpy.ndarray): the padded dataset to be fitted. Has to transform to ndarray from DataFrame during initialize\n",
    "        time(numpy.ndarray): the length of each data\n",
    "    Parameters:\n",
    "        - x (torch.FloatTensor): the real value features of the data\n",
    "        - t (torch.LongTensor): the temporal feature of the data\n",
    "    \"\"\"\n",
    "    def __init__(self,data, time=None):\n",
    "        #sanity check data and time\n",
    "        \n",
    "        \n",
    "        \n",
    "        if isinstance(time,type(None)):\n",
    "            time = [len(x) for x in data]\n",
    "            \n",
    "        if len(data) != len(time):\n",
    "            run.stop()\n",
    "            raise ValueError( f\"len(data) `{len(data)}` != len(time) {len(time)}\")\n",
    "            \n",
    "        \n",
    "        self.X = torch.FloatTensor(data)\n",
    "        self.T = torch.LongTensor(time)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx],self.T[idx]\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Minibatch sampling\n",
    "        \"\"\"\n",
    "        # Pad sequences to max length\n",
    "        X_mb = [X for X in batch[0]]\n",
    "        \n",
    "        # The actual length of each data\n",
    "        T_mb = [T for T in batch[1]]\n",
    "        \n",
    "        return X_mb, T_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    The embedding network (encoder) that maps the input data to a latent space.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, hidden_dim, num_layers, padding_value=0, max_seq_len=1000):\n",
    "        super(EmbeddingNetwork, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.padding_value = padding_value\n",
    "        self. max_seq_len = max_seq_len\n",
    "\n",
    "        #Embedder Architecture\n",
    "        self.emb_rnn = nn.GRU(\n",
    "            input_size=self.feature_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.emb_linear = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.emb_sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "        # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L61\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.emb_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(1)\n",
    "\n",
    "            for name, param in self.emb_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "    def forward(self,X,T):\n",
    "        \"\"\"Forward pass of the embedding features from original space to latent space.\n",
    "        Args:\n",
    "            X: Input time series feature (B x S x F)\n",
    "            T: INput temporal information (B)\n",
    "        Returns:\n",
    "            H: latent space embeddings (B x S x H)\n",
    "        \"\"\"\n",
    "        # Dynamic RNN input for ignoring paddings\n",
    "\n",
    "        X_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input =X,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # 128*100*71\n",
    "        H_o,H_t = self.emb_rnn(X_pack)\n",
    "\n",
    "        #pad RNN output back to sequence length\n",
    "\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "\n",
    "        #128*100*10\n",
    "        logits = self.emb_linear(H_o)\n",
    "        H = self.emb_sigmoid(logits)\n",
    "\n",
    "        return H\n",
    "    \n",
    "class RecoveryNetwork(nn.Module):\n",
    "    \"\"\"The recovery network (decoder) for TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,hidden_dim,feature_dim,num_layers,padding_value=0,max_seq_len=1000):\n",
    "        super(RecoveryNetwork, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.padding_value = padding_value\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        #Recovery Architecture\n",
    "        self.rec_rnn = nn.GRU(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.rec_linear = nn.Linear(self.hidden_dim, self.feature_dim)\n",
    "\n",
    "        # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name,param in self.rec_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(0)\n",
    "            for name,param in self.rec_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        \n",
    "    def forward(self,H,T):\n",
    "        \"\"\" Forward pass of the recovery features from latent space to original space.\n",
    "        Args:\n",
    "            H: latent representation (B x S x E)\n",
    "            T: input temporal information (B)\n",
    "        Returns:\n",
    "            X_tilde: recovered features (B x S x F)\n",
    "        \"\"\"\n",
    "        #Dynamic RNN input for ignoring paddings\n",
    "        H_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input = H,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        #128 x 100 x 10\n",
    "        H_o,H_t = self.rec_rnn(H_pack)\n",
    "        #pad RNN output back to sequence length\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "        #128 x 100 x 71\n",
    "        X_tilde = self.rec_linear(H_o)\n",
    "        return X_tilde\n",
    "\n",
    "class SupervisorNetwork(nn.Module):\n",
    "        \"\"\"The supervisor network for TimeGAN\n",
    "        \"\"\"\n",
    "        def __init__(self,hidden_dim,num_layers,padding_value=0,max_seq_len=1000):\n",
    "            super(SupervisorNetwork,self).__init__()\n",
    "            self.hidden_dim =hidden_dim\n",
    "            self.num_layers = num_layers\n",
    "            self.padding_value = padding_value\n",
    "            self.max_seq_len = max_seq_len\n",
    "\n",
    "            #supervisor architecture\n",
    "            self.sup_rnn = nn.GRU(\n",
    "                input_size=self.hidden_dim,\n",
    "                hidden_size=self.hidden_dim,\n",
    "                num_layers=self.num_layers-1,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            self.sup_linear = nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "            self.sup_sigmoid = nn.Sigmoid()\n",
    "             # Init weights\n",
    "            # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "            # Reference: \n",
    "            # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "            # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "            with torch.no_grad():\n",
    "                for name, param in self.sup_rnn.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'bias_ih' in name:\n",
    "                        param.data.fill_(1)\n",
    "                    elif 'bias_hh' in name:\n",
    "                        param.data.fill_(0)\n",
    "                for name, param in self.sup_linear.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        param.data.fill_(0)\n",
    "        def forward(self,H,T):\n",
    "            \"\"\"Forward pass for the supervisor for predicting next step\n",
    "            Args:\n",
    "                H: latent representation (B x S x E)\n",
    "                T: input temporal information (B)\n",
    "            Returns:\n",
    "                H_hat: predicted next step data (B x S x E)\n",
    "            \"\"\"\n",
    "\n",
    "            #Dynamic RNN input for ignoring paddings\n",
    "            H_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "                input = H,\n",
    "                lengths=T,\n",
    "                batch_first=True,\n",
    "                enforce_sorted=False,\n",
    "            )\n",
    "\n",
    "            H_o,H_t = self.sup_rnn(H_pack)\n",
    "            #pad RNN output back to sequence length\n",
    "            H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "                sequence=H_o,\n",
    "                batch_first=True,\n",
    "                padding_value=self.padding_value,\n",
    "                total_length=self.max_seq_len,\n",
    "            )\n",
    "            logits = self.sup_linear(H_o)\n",
    "            H_hat = self.sup_sigmoid(logits)\n",
    "            return H_hat\n",
    "\n",
    "class GeneratorNetwork(nn.Module):\n",
    "    \"\"\"The generator network for TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,Z_dim,hidden_dim,num_layers,padding_value=0,max_seq_len=1000):\n",
    "        super(GeneratorNetwork,self).__init__()\n",
    "        self.Z_dim = Z_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.padding_value = padding_value\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        #Generator Architecture\n",
    "        self.gen_rnn = nn.GRU(\n",
    "            input_size=self.Z_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.gen_linear = nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "        self.gen_sigmoid = nn.Sigmoid()\n",
    "                # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "        #aaa\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.gen_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(0)\n",
    "            for name, param in self.gen_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        \n",
    "    def forward(self,Z,T):\n",
    "        \"\"\" Takes in random noise (features) and generates synthetic features within the last latent space\n",
    "        Args:\n",
    "            Z: input random noise (B x S x Z)\n",
    "            T: input temporal information (B)\n",
    "        Returns:\n",
    "            H: embeddings (B x S x E)\n",
    "        \"\"\"\n",
    "        #Dynamic RNN input for ignoring paddings\n",
    "        Z_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input=Z,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # 128*100*71\n",
    "        H_o,H_t = self.gen_rnn(Z_pack)\n",
    "\n",
    "        #pad RNN output back to sequence length\n",
    "\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "\n",
    "        #128*100*10\n",
    "        logits = self.gen_linear(H_o)\n",
    "        H = self.gen_sigmoid(logits)\n",
    "\n",
    "        return H\n",
    "\n",
    "class DiscriminatorNetwork(nn.Module):\n",
    "    \"\"\"The discriminator network for TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,hidden_dim,num_layers,padding_value=0,max_seq_len=1000   ):\n",
    "        super(DiscriminatorNetwork,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.padding_value = padding_value\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        #Discriminator Architecture\n",
    "        self.dis_rnn = nn.GRU(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dis_linear = nn.Linear(self.hidden_dim,1)\n",
    "\n",
    "        # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.dis_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(0)\n",
    "            for name, param in self.dis_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "    \n",
    "    def forward(self, H, T):\n",
    "        \"\"\" Forward pass for predicting if the data is real or synthetic\n",
    "        \n",
    "        Args:\n",
    "            H: latent representation (B x S x E)\n",
    "            T: input temporal information (B)\n",
    "        Returns:\n",
    "        logits: prediction logits(B x S x 1)\n",
    "        \"\"\"\n",
    "        # dynamic RNN input for ignoring paddings\n",
    "        H_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input = H,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # 128*100*10\n",
    "        H_o,H_t = self.dis_rnn(H_pack)\n",
    "\n",
    "        # pad RNN output back to sequence length\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "        #TODO: check size of H_o\n",
    "        #print(f'size of H_\n",
    "        #TODO get an output with the size of batch_size*1 so it gaves a result if the signal is real or not\n",
    "        logits = self.dis_linear(H_o).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "class TimeGAN(nn.Module):\n",
    "    \"\"\" Implementation of TimeGan (Yoon et al., 2019) using PyTorch\n",
    "    \n",
    "    Reference:\n",
    "        - Yoon, J., Jarret, D., van der Schaar, M. (2019). Time-series Generative Adversarial Networks. (https://papers.nips.cc/paper/2019/hash/c9efe5f26cd17ba6216bbe2a7d26d490-Abstract.html)\n",
    "        - https://github.com/jsyoon0823/TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,device,feature_dim,Z_dim,hidden_dim,max_seq_len,batch_size,num_layers,padding_value):\n",
    "        super(TimeGAN,self).__init__()\n",
    "        self.device =device\n",
    "        self.feature_dim = feature_dim\n",
    "        self.Z_dim = Z_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layer = num_layers\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "        self.embedder = EmbeddingNetwork(feature_dim=feature_dim,hidden_dim=hidden_dim,num_layers=num_layers,padding_value=padding_value,max_seq_len=max_seq_len)\n",
    "        self.recovery = RecoveryNetwork(feature_dim=feature_dim,hidden_dim=hidden_dim,num_layers=num_layers,padding_value=padding_value,max_seq_len=max_seq_len)\n",
    "        self.generator = GeneratorNetwork(Z_dim=Z_dim,hidden_dim=hidden_dim,num_layers=num_layers,padding_value=padding_value,max_seq_len=max_seq_len)\n",
    "        self.discriminator = DiscriminatorNetwork(hidden_dim=hidden_dim,num_layers=num_layers,padding_value=padding_value,max_seq_len=max_seq_len)\n",
    "\n",
    "        self.supervisor = SupervisorNetwork(hidden_dim=hidden_dim,num_layers=num_layers,padding_value=padding_value,max_seq_len=max_seq_len)\n",
    "\n",
    "    def _recovery_forward(self, X, T):\n",
    "        \"\"\" The embedding network forward pass and the embedder network loss\n",
    "        Args:\n",
    "            X: input features\n",
    "            T: input temporal information\n",
    "        Returns:\n",
    "            E_loss: the reconstruction loss\n",
    "            X_tilde: the reconstructed features\n",
    "        \"\"\"\n",
    "\n",
    "        # FOrward pass\n",
    "        H = self.embedder(X,T)\n",
    "        X_tilde = self.recovery(H,T)\n",
    "\n",
    "        #for Joint training\n",
    "        H_hat_supervise = self.supervisor(H,T)\n",
    "        G_loss_S = F.mse_loss(\n",
    "            H_hat_supervise[:,:-1,:],\n",
    "            H[:,1:,:],\n",
    "        ) #Teacher forcing next output\n",
    "\n",
    "        #Reconstruction loss\n",
    "        E_loss_T0 = F.mse_loss(X_tilde,X)\n",
    "        E_loss0 = 10*torch.sqrt(E_loss_T0)\n",
    "        E_loss = E_loss0 + 0.1*G_loss_S\n",
    "        return E_loss, E_loss0,E_loss_T0\n",
    "    def _supervisor_forward(self, X, T):\n",
    "        \"\"\" The supervisor training forward pass\n",
    "        Args:\n",
    "            X: original input features\n",
    "            T: input temporal information\n",
    "        Returns:\n",
    "            S_loss: the supervisor's loss\n",
    "        \"\"\"\n",
    "        #supervisor forward pass\n",
    "        H = self.embedder(X,T)\n",
    "        H_hat_supervise = self.supervisor(H,T)\n",
    "\n",
    "        #supervised loss\n",
    "        S_loss = F.mse_loss(\n",
    "            H_hat_supervise[:,:-1,:],\n",
    "            H[:,1:,:],\n",
    "        ) #Teacher forcing next output\n",
    "        return S_loss\n",
    "    def _discriminator_forward(self, X, T, Z, gamma=1):\n",
    "        \"\"\" The discriminator forward pass and adversarial loss\n",
    "        Args:\n",
    "            X: input features\n",
    "            T: input temporal information\n",
    "            Z: input noise\n",
    "            gamma: the weight for the adversarial loss\n",
    "        Returns:\n",
    "            D_loss: adversarial loss\n",
    "        \"\"\"\n",
    "        #Real\n",
    "        H = self.embedder(X, T).detach()\n",
    "\n",
    "        #generator\n",
    "        E_hat = self.generator(Z,T).detach()\n",
    "        H_hat = self.supervisor(E_hat,T).detach()\n",
    "        \n",
    "        #forward pass\n",
    "        Y_real = self.discriminator(H,T)        #Encode original data\n",
    "        Y_fake = self.discriminator(H_hat,T)    #Output of generator + supervisor\n",
    "        Y_fake_e = self.discriminator(E_hat,T)  #Output of generator\n",
    "\n",
    "        D_loss_real = F.binary_cross_entropy_with_logits(Y_real, torch.ones_like(Y_real))\n",
    "        D_loss_fake = F.binary_cross_entropy_with_logits(Y_fake, torch.zeros_like(Y_fake))\n",
    "        D_loss_fake_e = F.binary_cross_entropy_with_logits(Y_fake_e, torch.zeros_like(Y_fake_e))\n",
    "\n",
    "        D_loss = D_loss_real + D_loss_fake + gamma * D_loss_fake_e\n",
    "\n",
    "        return D_loss\n",
    "    \n",
    "    def _generator_forward(self, X, T, Z, gamma=1):\n",
    "        \"\"\" The generator forward pass\n",
    "        Args:\n",
    "            X: original input features\n",
    "            T: input temporal information\n",
    "            Z: input noise for the generator\n",
    "            gamma: the weight for the adversarial loss\n",
    "        Returns:\n",
    "            G_loss: the generator loss\n",
    "        \"\"\"\n",
    "        #supervisor forward pass\n",
    "        H = self.embedder(X,T)\n",
    "        H_hat_supervise = self.supervisor(H,T)\n",
    "\n",
    "        #generator forward pass\n",
    "        E_hat = self.generator(Z,T)\n",
    "        H_hat = self.supervisor(E_hat,T)\n",
    "\n",
    "        #synthetic data generated\n",
    "        X_hat = self.recovery(H_hat,T)\n",
    "\n",
    "        #generator loss\n",
    "        #Adversarial loss\n",
    "        Y_fake = self.discriminator(H_hat,T)        #Output of supervisor\n",
    "        Y_fake_e = self.discriminator(E_hat,T)      #Output of generator\n",
    "\n",
    "        G_loss_U = F.binary_cross_entropy_with_logits(Y_fake, torch.ones_like(Y_fake))\n",
    "        G_loss_U_e = F.binary_cross_entropy_with_logits(Y_fake_e, torch.ones_like(Y_fake_e))\n",
    "\n",
    "        #Supervised loss\n",
    "        G_loss_S = F.mse_loss(\n",
    "            H_hat_supervise[:,:-1,:],\n",
    "            H[:,1:,:],\n",
    "        ) #Teacher forcing next output\n",
    "\n",
    "        #Two moments losses\n",
    "        G_loss_V1 = torch.mean(\n",
    "            torch.abs(torch.sqrt(X_hat.var(dim=0,unbiased=False)+1e-6) - torch.sqrt(X.var(dim=0,unbiased=False)+1e-6))\n",
    "        )\n",
    "        G_loss_V2 = torch.mean(torch.abs((X_hat.mean(dim=0)) - (X.mean(dim=0))))\n",
    "        G_loss_V = G_loss_V1 + G_loss_V2\n",
    "        \n",
    "        #sum of losses\n",
    "        G_loss = G_loss_U + gamma * G_loss_U_e + 100 * torch.sqrt(G_loss_S) + 100 * G_loss_V\n",
    "    \n",
    "        return G_loss\n",
    "    \n",
    "    def _inference(self, Z,T):\n",
    "        \"\"\" Inference for generating synthetic data\n",
    "        Args:\n",
    "            Z: input noise\n",
    "            T: temporal information\n",
    "        Returns:\n",
    "            X_hat: the generated data\n",
    "        \"\"\"\n",
    "\n",
    "        #generator forward pass\n",
    "        E_hat = self.generator(Z,T)\n",
    "\n",
    "        H_hat = self.supervisor(E_hat,T)\n",
    "\n",
    "        #synthetic data generated\n",
    "        X_hat = self.recovery(H_hat,T)\n",
    "        return X_hat\n",
    "\n",
    "    def forward(self,X,T,Z, obj, gamma=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: input features (B,H,F)\n",
    "            T: The temporal information (B)\n",
    "            Z: the sampled noise (B,H,Z)\n",
    "            obj: the network to be trained ('autoencoder','supervisor','generator','discriminator')\n",
    "            gamma: loss hyperparameter\n",
    "        Returns:\n",
    "            loss: loss for the forward pass\n",
    "            X_hat: the generated data\n",
    "        \"\"\"\n",
    "\n",
    "        #Move variables to device\n",
    "        if obj !='inference':\n",
    "            if X is None:\n",
    "                run.stop()\n",
    "                raise ValueError('X cannot be empty')\n",
    "                \n",
    "            \n",
    "            X = torch.FloatTensor(X)\n",
    "            X = X.to(self.device)\n",
    "\n",
    "        if Z is not None:\n",
    "            Z = torch.FloatTensor(Z)\n",
    "            Z = Z.to(self.device)\n",
    "        \n",
    "        if obj == 'autoencoder':\n",
    "            #embedder and recovery forward\n",
    "            loss = self._recovery_forward(X,T)\n",
    "        elif obj == 'supervisor':\n",
    "            loss = self._supervisor_forward(X,T)\n",
    "        elif obj == 'generator':\n",
    "            if Z is None:\n",
    "                run.stop()\n",
    "                raise ValueError('Z cannot be empty')\n",
    "                \n",
    "            loss = self._generator_forward(X,T,Z,gamma)\n",
    "        elif obj == 'discriminator':\n",
    "            if Z is None:\n",
    "                run.stop()\n",
    "                raise ValueError('Z cannot be empty')\n",
    "            loss = self._discriminator_forward(X,T,Z,gamma)\n",
    "            return loss\n",
    "        elif obj == 'inference':\n",
    "            X_hat = self._inference(Z,T)\n",
    "            #X_hat = X_hat.cpu.detach()\n",
    "            X_hat = X_hat.detach().cpu()\n",
    "\n",
    "            return X_hat\n",
    "        else:\n",
    "            run.stop()\n",
    "            raise ValueError('obj must be autoencoder, supervisor, generator or discriminator')\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_trainer(\n",
    "        model: torch.nn.Module,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        e_opt: torch.optim.Optimizer,\n",
    "        r_opt: torch.optim.Optimizer,\n",
    "        emb_epochs: int,\n",
    "        writer \n",
    "):\n",
    "    \"\"\"\n",
    "    Training loop for embedding and recovery functions.\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader to use\n",
    "        e_opt (torch.optim.Optimizer): The optimizer for the embedding function\n",
    "        r_opt (torch.optim.Optimizer): The optimizer for the recovery function\n",
    "        args (Dict): The model/training configuration\n",
    "        writer (SummaryWriter): Neptune logger\n",
    "    \"\"\"\n",
    "    logger = trange(emb_epochs, desc =f\"Epoch:0, Loss:0\")\n",
    "    for epoch in logger:\n",
    "        for X_mb,T_mb in dataloader:\n",
    "\n",
    "            #reset gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            #forward pass\n",
    "            _,E_loss0,E_loss_T0 = model(X=X_mb,T=T_mb,Z=None,obj=\"autoencoder\")\n",
    "            loss = np.sqrt(E_loss_T0.item())\n",
    "\n",
    "            #backward pass\n",
    "            E_loss0.backward()\n",
    "\n",
    "            #update weights\n",
    "            e_opt.step()\n",
    "            r_opt.step()\n",
    "\n",
    "        # Log loss for final batch of each epochs\n",
    "        logger.set_description(f\"Epoch:{epoch}, Loss:{loss:.4f}\")\n",
    "        writer['Embedding/Training_loss'].append(loss)\n",
    "        \"\"\"if writer:\n",
    "            writer.add_scalar(\"Embedding/Loss:\",loss,epoch)\n",
    "            writer.flush()\"\"\"\n",
    "\n",
    "def supervisor_trainer(\n",
    "    model: torch.nn.Module, \n",
    "    dataloader: torch.utils.data.DataLoader, \n",
    "    s_opt: torch.optim.Optimizer, \n",
    "    g_opt: torch.optim.Optimizer,\n",
    "    sup_epochs: int,\n",
    "    writer\n",
    "):\n",
    "    \"\"\"\n",
    "    The training loop for the supervisor function\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader to use\n",
    "        s_opt (torch.optim.Optimizer): The optimizer for the supervisor function\n",
    "        g_opt (torch.optim.Optimizer): The optimizer for the generator function\n",
    "        args (Dict): The model/training configuration\n",
    "        writer (Union[torch.utils.tensorboard.SummaryWriter, type(None)], optional): The tensorboard writer to use. Defaults to None.\n",
    "    \"\"\"\n",
    "    logger = trange(sup_epochs, desc=f\"Epoch: 0, Loss: 0\")\n",
    "    for epoch in logger:\n",
    "        for X_mb, T_mb in dataloader:\n",
    "            # Reset gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Forward Pass\n",
    "            S_loss = model(X=X_mb, T=T_mb, Z=None, obj=\"supervisor\")\n",
    "\n",
    "            # Backward Pass\n",
    "            S_loss.backward()\n",
    "            loss = np.sqrt(S_loss.item())\n",
    "\n",
    "            # Update model parameters\n",
    "            s_opt.step()\n",
    "\n",
    "        # Log loss for final batch of each epoch (29 iters)\n",
    "        logger.set_description(f\"Epoch: {epoch}, Loss: {loss:.4f}\")\n",
    "        writer['Supervisor/Training_loss'].append(loss)\n",
    "        \"\"\"if writer:\n",
    "            writer.add_scalar(\n",
    "                \"Supervisor/Loss:\",loss,epoch)\n",
    "            writer.flush()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_trainer(\n",
    "    model: torch.nn.Module, \n",
    "    dataloader: torch.utils.data.DataLoader, \n",
    "    e_opt: torch.optim.Optimizer, \n",
    "    r_opt: torch.optim.Optimizer, \n",
    "    s_opt: torch.optim.Optimizer, \n",
    "    g_opt: torch.optim.Optimizer, \n",
    "    d_opt: torch.optim.Optimizer,\n",
    "    sup_epochs: int,\n",
    "    batch_size: int,\n",
    "    max_seq_len: int,\n",
    "    Z_dim: int,\n",
    "    dis_thresh: float,\n",
    "    writer\n",
    "    ):\n",
    "    \"\"\"\n",
    "    The training loop for training the model altogether\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader to use\n",
    "        e_opt (torch.optim.Optimizer): The optimizer for the embedding function\n",
    "        r_opt (torch.optim.Optimizer): The optimizer for the recovery function\n",
    "        s_opt (torch.optim.Optimizer): The optimizer for the supervisor function\n",
    "        g_opt (torch.optim.Optimizer): The optimizer for the generator function\n",
    "        d_opt (torch.optim.Optimizer): The optimizer for the discriminator function\n",
    "        args (Dict): The model/training configuration\n",
    "        writer (Union[torch.utils.tensorboard.SummaryWriter, type(None)], optional): The tensorboard writer to use. Defaults to None.\n",
    "    \"\"\"\n",
    "    logger = trange(\n",
    "        sup_epochs, \n",
    "        desc=f\"Epoch: 0, E_loss: 0, G_loss: 0, D_loss: 0\"\n",
    "    )\n",
    "    for epoch in logger:\n",
    "        for X_mb, T_mb in dataloader:\n",
    "            ## Generator Training\n",
    "            for _ in range(2):\n",
    "                # Random Generator\n",
    "                Z_mb = torch.rand((batch_size, max_seq_len, Z_dim))\n",
    "\n",
    "                # Forward Pass (Generator)\n",
    "                model.zero_grad()\n",
    "                G_loss = model(X=X_mb, T=T_mb, Z=Z_mb, obj=\"generator\")\n",
    "                G_loss.backward()\n",
    "                G_loss = np.sqrt(G_loss.item())\n",
    "\n",
    "                # Update model parameters\n",
    "                g_opt.step()\n",
    "                s_opt.step()\n",
    "\n",
    "                # Forward Pass (Embedding)\n",
    "                model.zero_grad()\n",
    "                E_loss, _, E_loss_T0 = model(X=X_mb, T=T_mb, Z=Z_mb, obj=\"autoencoder\")\n",
    "                E_loss.backward()\n",
    "                E_loss = np.sqrt(E_loss.item())\n",
    "                \n",
    "                # Update model parameters\n",
    "                e_opt.step()\n",
    "                r_opt.step()\n",
    "\n",
    "            # Random Generator\n",
    "            Z_mb = torch.rand((batch_size, max_seq_len, Z_dim))\n",
    "\n",
    "            ## Discriminator Training\n",
    "            model.zero_grad()\n",
    "            # Forward Pass\n",
    "            D_loss = model(X=X_mb, T=T_mb, Z=Z_mb, obj=\"discriminator\")\n",
    "\n",
    "            # Check Discriminator loss\n",
    "            if D_loss > dis_thresh:\n",
    "                # Backward Pass\n",
    "                D_loss.backward()\n",
    "\n",
    "                # Update model parameters\n",
    "                d_opt.step()\n",
    "            D_loss = D_loss.item()\n",
    "\n",
    "        logger.set_description(\n",
    "            f\"Epoch: {epoch}, E: {E_loss:.4f}, G: {G_loss:.4f}, D: {D_loss:.4f}\"\n",
    "        )\n",
    "        \n",
    "        writer['Joint/E_loss'].append(E_loss)\n",
    "        writer['Joint/G_loss'].append(G_loss)\n",
    "        writer['Joint/D_loss'].append(D_loss)\n",
    "\n",
    "def timegan_trainer(model,train_data,train_time,batch_size,lr,emb_epochs,sup_epochs,max_seq_length,Z_dim,dis_thresh,device,model_path,writer):\n",
    "    \"\"\"\n",
    "    The trainign procedure for TimeGAN.\n",
    "    Args:\n",
    "        model (torch.nn.module): The model that generates synthetic data\n",
    "        loaded_data(pandas.DataFrame): The data to train on, including data and time\n",
    "        args (Dict): The model/training configuration\n",
    "    Returns:\n",
    "        generated_data (np.array): The synthetic data generated by the model\n",
    "    \"\"\"\n",
    "    dataset = TimeGAN_Dataset(data=train_data,time=train_time)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False)\n",
    "    model.to(device)\n",
    "\n",
    "    #initialize optimizers\n",
    "    e_opt = torch.optim.Adam(model.embedder.parameters(), lr=lr)\n",
    "    r_opt = torch.optim.Adam(model.recovery.parameters(), lr=lr)\n",
    "    s_opt = torch.optim.Adam(model.supervisor.parameters(), lr=lr)\n",
    "    g_opt = torch.optim.Adam(model.generator.parameters(), lr=lr)\n",
    "    d_opt = torch.optim.Adam(model.discriminator.parameters(), lr=lr)\n",
    "\n",
    "    #initialize tensorboard writer\n",
    "    #writer = SummaryWriter(os.path.join(f\"tensorboard/{args.exp}\"))\n",
    "\n",
    "    print(\"\\nStart Embedding Network Training\")\n",
    "    embedding_trainer(model=model, dataloader=dataloader, e_opt=e_opt, r_opt=r_opt, emb_epochs=emb_epochs,writer=writer)\n",
    "\n",
    "    print(\"\\nStart Training with Supervised Loss Only\")\n",
    "    supervisor_trainer(model=model, dataloader=dataloader, s_opt=s_opt,g_opt=g_opt, sup_epochs=sup_epochs,writer=writer)\n",
    "\n",
    "    print(\"\\nStart Joint Training\")\n",
    "    joint_trainer(model=model, dataloader=dataloader, e_opt=e_opt, r_opt=r_opt, s_opt=s_opt, g_opt=g_opt, d_opt=d_opt, sup_epochs=sup_epochs, batch_size=batch_size, max_seq_len=max_seq_length, Z_dim=Z_dim, dis_thresh=dis_thresh,writer=writer)\n",
    "\n",
    "\n",
    "    #save model,args, and hyperparameters\n",
    "    #torch.save(args,f\"{args.model_path}/args.pickle\")\n",
    "    torch.save(model.state_dict(),f\"{model_path}/model.pt\")\n",
    "    \n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def timegan_generator(model,T,model_path,batch_size, max_seq_len, Z_dim ,device):\n",
    "    \"\"\"\n",
    "    The interference procedure for TimeGAN.\n",
    "    Args:\n",
    "        model (torch.nn.module): The model that generates synthetic data\n",
    "        T (List[int]): The time to generate data for\n",
    "        args (Dict): The model/training configuration\n",
    "    returns:\n",
    "        generated_data (np.array): The synthetic data generated by the model\n",
    "    \"\"\"\n",
    "    #load model\n",
    "    if not os.path.exists(model_path):\n",
    "        run.stop()\n",
    "        raise ValueError(f\"Model not found at {model_path}\")\n",
    "    model.load_state_dict(torch.load(f\"{model_path}/model.pt\"))\n",
    "    print(\"\\nStart Generating Synthetic Data\")\n",
    "    #Initialize model to evaluation mode and run without gradients\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Random Generator\n",
    "        print(f'random generated Z dims: {batch_size,max_seq_len,Z_dim}')\n",
    "        Z = torch.rand((batch_size, max_seq_len, Z_dim))\n",
    "        # Forward Pass (Generator)\n",
    "        generated_data = model(X=None, T=T, Z=Z, obj=\"inference\")\n",
    "    return generated_data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(writer,data_limit=None,save_dataset=None):\n",
    "    \"\"\"\n",
    "    data_limit=None,save_dataset=None\n",
    "    Load and preprocess real life datasets.\n",
    "    \n",
    "    Args:\n",
    "        data_limit (int): The number of data points to load. If None, all data points are loaded. Default: None. Used for testing.\n",
    "        save_dataset (bool): If 'Full', the dataset is saved to a csv file. If it's 'limited', than save the limited dataset if data_limit is not None. Default: None.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        dataset (pandas.DataFrame): The dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    main_dataset = pd.DataFrame()\n",
    "    cluster = True\n",
    "    if cluster == False:\n",
    "        Tk().withdraw() # we don't want a full GUI, so keep the root window from appearing\n",
    "        filenames = askopenfilenames() # show an \"Open\" dialog box and return the path to the selected file\n",
    "        print(filenames)\n",
    "        for filename in filenames:\n",
    "            \n",
    "            if filename.endswith('.mat'):\n",
    "                #output df format: [id,value_array]\n",
    "                df = load_mat_as_df(filename)\n",
    "                print(df)\n",
    "            \n",
    "            elif filename.endswith('.csv'):\n",
    "\n",
    "                #use create_dataset_csv.py to create a csv file\n",
    "                if filename.find('dataset') != -1:\n",
    "                    df = pd.read_csv(filename,sep=';',index_col=0)\n",
    "\n",
    "                \"\"\" CSV format:\n",
    "                ID|time|Sleeping stage|length|additional_info\n",
    "\n",
    "                \"\"\"\n",
    "                pass\n",
    "\n",
    "            elif filename.endswith('.xml'):\n",
    "                ## TODO: add xml support\n",
    "                #df =\n",
    "                pass\n",
    "            else:\n",
    "                print(\"Unsupported file format, skipping file:\",filename,\".\")\n",
    "                pass\n",
    "        \n",
    "            main_dataset.append(df)\n",
    "    else:\n",
    "        #load datasets from cluster\n",
    "        filename = \"Data/generated_dataset.csv\"\n",
    "        writer[\"Dataset\"].track_files(filename)\n",
    "        main_dataset = pd.read_csv(filename,sep=';',index_col=0)\n",
    "\n",
    "    #Cut df to data_limit size for testing purposes\n",
    "    if data_limit is not None:\n",
    "        if data_limit < len(main_dataset):\n",
    "            print(f'The length of the dataset is {len(main_dataset)}, the new length is {data_limit}')\n",
    "            main_dataset = main_dataset[:data_limit]\n",
    "        elif data_limit >= len(main_dataset):\n",
    "            print(\"data_limit is bigger than the dataset size, using the whole dataset\")\n",
    "    elif data_limit <= 0 or data_limit == '':\n",
    "        print(\"data_limit is 0 or less, using the whole dataset\")\n",
    "    #print the headers of the dataset\n",
    "    #print(main_dataset.head())\n",
    "    # transform dataset.time and dataset.Sleeping stage to float and int\n",
    "    #print(main_dataset)\n",
    "    #main_dataset.time = main_dataset.time.apply(string_to_float_array)\n",
    "    mask = main_dataset['Sleeping_stage'].str.contains('nan', na=False)\n",
    "\n",
    "    # drop the rows that contain 'NaN' as part of the string\n",
    "    main_dataset = main_dataset[~mask]\n",
    "    main_dataset['Sleeping_stage'] = main_dataset['Sleeping_stage'].apply(string_to_int_array)\n",
    "    \n",
    "    print(main_dataset.head())\n",
    "    return main_dataset #dataset as df\n",
    "\n",
    "def load_mat_as_df(mat_file_path, var_name):\n",
    "    mat = sio.loadmat(mat_file_path,simplify_cells=True)\n",
    "\n",
    "    if var_name not in list(mat.keys()):\n",
    "        var_name = get_variable_name(mat)   \n",
    "        \n",
    "\n",
    "    return pd.DataFrame(mat[var_name])\n",
    "\n",
    "def get_variable_name(loaded_mat):\n",
    "\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    root.title('.mat variable selector')\n",
    "    tk.Label(root, text=\"Choose a variable:\").pack()\n",
    "    choices = list(loaded_mat.keys())\n",
    "\n",
    "    variable = tk.StringVar(root)\n",
    "    variable.set(choices[0]) # default value\n",
    "    w = tk.Combobox(root, textvariable=variable,values=choices)\n",
    "\n",
    "    w.pack()\n",
    "    def ok():\n",
    "        print (\"value is:\" + variable.get())\n",
    "        root.destroy()\n",
    "    def cancel():\n",
    "        root.destroy()\n",
    "        raise InterruptedError('User cancelled, invalid variable name')\n",
    "\n",
    "    button1 = tk.Button(root, text=\"OK\", command=ok)\n",
    "    button2 = tk.Button(root, text=\"Cancel\", command=cancel)\n",
    "    button1.pack()\n",
    "    button2.pack()\n",
    "    root.mainloop()\n",
    "    \n",
    "    return variable.get()\n",
    "def string_to_float_list(s):\n",
    "    return [float(x) for x in s.strip('[]').split(',')]\n",
    "def string_to_int_list(s):\n",
    "    return [int(x) for x in s.strip('[]').split(',')]\n",
    "def string_to_float_array(s):\n",
    "    \"\"\"Convert array-as-string to float array\"\"\"\n",
    "    return [float(x) for x in ast.literal_eval(s)]\n",
    "def string_to_int_array(s):\n",
    "    \"\"\"Convert array-as-string to int array\"\"\"\n",
    "    return [int(x) for x in ast.literal_eval(s)]\n",
    "\n",
    "def deconstruct_array(array):\n",
    "    \"\"\"Dismantle ID x length x 1 array to ID x length x 5 based on x1 values.\"\"\"    \n",
    "    new_array = np.zeros((array.shape[0],array.shape[1],6))\n",
    "    i_max = array.shape[0]-1\n",
    "    j_max = array.shape[1]-1\n",
    "    for i in range(i_max):\n",
    "        for j in range(j_max):\n",
    "            new_array[i,j,int(array[i,j,0])] = 1\n",
    "    return new_array\n",
    "def reconstruct_array(array):\n",
    "    \"\"\"Remake ID x length x 5 array to ID x length x 1 based on x1 values.\"\"\"\n",
    "    #print(f'array shape: {array.shape}')\n",
    "    \n",
    "    new_array = np.zeros((array.shape[0],array.shape[1],1))\n",
    "    i_max = array.shape[0]-1\n",
    "    j_max = array.shape[1]-1\n",
    "    for i in range(i_max):\n",
    "        for j in range(j_max):\n",
    "            new_array[i,j,0] = np.argmax(array[i,j,:])\n",
    "    return new_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(writer,padding_value,len_limit=0.2,data_limit=None,save_dataset=None,norm_enable=False,deconst_enable=False):\n",
    "    \"\"\"\n",
    "    padding_value: int = -1.0,\n",
    "    data_limit: int = None\"\"\"\n",
    "    # Load and preprocess data\n",
    "    # \n",
    "    # Steps:\n",
    "    # 0. Sanity checks\n",
    "    # 1. Load data from files (csv,mat,xml)\n",
    "    # 2. Preprocess data:\n",
    "    # 2.1. Remove outliers\n",
    "    # 2.2. Extract sequence length and time\n",
    "    # 2.3. Resample data\n",
    "    # 2.4. Normalize data\n",
    "    # 2.5. Padding\n",
    "    # 3 Save data to csv file\n",
    "    #  \n",
    "    # Args:\n",
    "    #     data_limit (int): The number of lines to load from the data file. If None, all data points are loaded. Default: None. Used for testing.\n",
    "    #     padding_value (int): The value used for padding\n",
    "    #     save_dataset (bool): If 'Full', the dataset is saved to a csv file. If it's 'limited', than save the limited dataset if data_limit is not None. Default: None.\n",
    "    #     norm_enable (bool): If True, normalize the data. Default: False.\n",
    "    #     \n",
    "    # \n",
    "    # Returns:\n",
    "    #     prep_data (pandas.DataFrame): The processed data\n",
    "\n",
    "    #######################################\n",
    "    # 0. Sanity checks\n",
    "    #######################################\n",
    "    \n",
    "    # Check if save_dataset is valid\n",
    "    if save_dataset is not None:\n",
    "        if save_dataset.lower =='none':\n",
    "            save_dataset = None\n",
    "        elif save_dataset.lower =='full':\n",
    "            save_dataset = 'Full'\n",
    "        elif save_dataset.lower =='limited':\n",
    "            save_dataset = 'Limited'\n",
    "        else:\n",
    "            raise ValueError('save_dataset must be None, Full or Limited')\n",
    "    # Check if data_limit is valid\n",
    "    if data_limit is not None:\n",
    "        if isinstance(data_limit, str) == True:\n",
    "            data_limit= None\n",
    "        elif isinstance(data_limit, float) == True and data_limit.is_integer() == True:\n",
    "            data_limit = int(data_limit)\n",
    "        elif isinstance(data_limit, int) == True:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('data_limit must be None or int')\n",
    "    # Check if padding_value is valid\n",
    "    if isinstance(padding_value, str) == True:\n",
    "        raise ValueError('padding_value must be int')\n",
    "    elif isinstance(padding_value, float) == True and padding_value.is_integer() == True:\n",
    "        padding_value = int(padding_value)\n",
    "    elif isinstance(padding_value, int) == True:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError('padding_value must be int')\n",
    "    # Check if norm_enable is valid\n",
    "    if isinstance(norm_enable, bool) == True:\n",
    "        pass\n",
    "    elif isinstance(norm_enable, str) == True:\n",
    "        if norm_enable.lower() == 'true':\n",
    "            norm_enable = True\n",
    "        elif norm_enable.lower() == 'false':\n",
    "            norm_enable = False\n",
    "        else:\n",
    "            raise ValueError('norm_enable must be bool')\n",
    "    else:\n",
    "        raise ValueError('norm_enable must be bool')\n",
    "\n",
    "\n",
    "    #######################################\n",
    "    # 1. Load data from files (csv,mat,xml)\n",
    "    #######################################\n",
    "\n",
    "    loaded_data = load_data(writer=writer,data_limit=data_limit,save_dataset=save_dataset)\n",
    "    \"\"\"\n",
    "    loaded data =       time_data   , data  , length\n",
    "    (pandas.DataFrame), (np.array)  ,(list) ,(int)\n",
    "    \n",
    "    ()\n",
    "    \"\"\"\n",
    "    #convert data to np.array\n",
    "    #loaded_data['data'] = loaded_data['data'].apply(lambda x: np.array(x))\n",
    "    \n",
    "    #######################################\n",
    "    # 2. Preprocess data:\n",
    "    #######################################\n",
    "    # 2.1. Remove outliers\n",
    "    #######################################\n",
    "    \"\"\"\n",
    "    Remove row's with unacceptable sleep stages values\n",
    "    \"\"\"\n",
    "    \n",
    "    sleep_stages = np.array([1,2,3,4,5])\n",
    "    loaded_data[loaded_data['Sleeping_stage'].apply(lambda x: all(elem in sleep_stages for elem in x))]\n",
    "\n",
    "    #######################################\n",
    "    # 2.2. Extract sequence length and time\n",
    "    #######################################\n",
    "    \"\"\"\n",
    "    Extract sequence length of all lines and time of each line\n",
    "    \"\"\"\n",
    "    loaded_data['length'] = loaded_data['Sleeping_stage'].apply(lambda x: len(x))\n",
    "    if len_limit != 0:\n",
    "        #plot length distribution\n",
    "        fig, (ax1, ax2) = plt.subplots(ncols=2,figsize=(10,5))\n",
    "        ax1.hist(loaded_data['length'], )\n",
    "        ax1.set_title('Original length distribution')\n",
    "        _,bins,_=ax1.hist(loaded_data['length'])\n",
    "        # drop rows with +- 10% of mean length\n",
    "        #caluclate mean length \n",
    "        mean_len = loaded_data.length.mean()\n",
    "        #df = df[df['Age'] <= 1.1 * mean_age]\n",
    "        loaded_data=loaded_data[loaded_data['length'] <= (1+len_limit) *mean_len]\n",
    "        loaded_data=loaded_data[loaded_data['length'] >=(1-len_limit)*mean_len]\n",
    "        ax2.hist(loaded_data['length'],bins=bins,)\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots(ncols=1,figsize=(10,5))\n",
    "        ax1.hist(loaded_data['length'])\n",
    "    writer['Length_comp'].upload(fig)\n",
    "    plt.close()\n",
    "    #######################################\n",
    "    # 2.4. Normalize data\n",
    "    #######################################\n",
    "    \"\"\"\n",
    "    Normalize data to [0,1] using MinMaxScaler algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    if norm_enable == True:\n",
    "        loaded_data['Sleeping_stage']=MinMaxNormalizer(loaded_data['Sleeping_stage'])\n",
    "    \n",
    "\n",
    "    #######################################\n",
    "    # 2.5. Padding\n",
    "    #######################################\n",
    "    \"\"\"\n",
    "    Padding data to given length\n",
    "    \"\"\"\n",
    "    #print(f'length of all row: {loaded_data.length}')\n",
    "    max_len = max(loaded_data['length'])\n",
    "    length = len(loaded_data)\n",
    "    #get the number of columns in the data\n",
    "    dim =len(loaded_data.columns)-1\n",
    "    # Question: Current padding value is 0, is it ok? Do we need it or just resample?\n",
    "    data_info = {\n",
    "        'length' : length,\n",
    "        'max_length' : max_len,\n",
    "        'paddding_value_stage' : padding_value,\n",
    "        'dim' : dim\n",
    "    }\n",
    "\n",
    "    loaded_data['Sleeping_stage'] = loaded_data['Sleeping_stage'].apply(lambda x: np.transpose(x))\n",
    "    \n",
    "    prep_data = pd.DataFrame(columns=['Sleeping_stage','time'])\n",
    "    print(f'Padding data to {max_len} length')\n",
    "\n",
    "    #shape: [length, max_len,features] = [length, max_len,1]\n",
    "    # init empty array\n",
    "    padded_data = np.empty((length,max_len,1))\n",
    "    #fill array with padding value\n",
    "    padded_data.fill(padding_value)\n",
    "\n",
    "    time = []\n",
    "\n",
    "    for i in tqdm(range(length)):\n",
    "        #get a row of Sleep stage data\n",
    "        tmp_stage = loaded_data.iloc[i]['Sleeping_stage']\n",
    "        #print tmp_stage type\n",
    "        #print(tmp_stage.shape)\n",
    "\n",
    "        #impute missing values\n",
    "        #tmp_stage = impute_missing_values(tmp_stage)\n",
    "        \n",
    "        #get time data\n",
    "        tmp_time = len(tmp_stage)\n",
    "        #reshape tmp_stage to [tmp_time,1]\n",
    "        tmp_stage = tmp_stage.reshape(tmp_time,1)\n",
    "        # pad data to 'max_seq_len'\n",
    "        if len(tmp_stage) >= max_len:\n",
    "            padded_data[i,:,:] = tmp_stage[:max_len,0:]\n",
    "            time.append(max_len)\n",
    "        else:\n",
    "            padded_data[i,:tmp_time,:] = tmp_stage[:,0:]\n",
    "            time.append(len(tmp_stage))\n",
    "    if deconst_enable:\n",
    "        padded_data=deconstruct_array(padded_data)\n",
    "        \n",
    "    return padded_data, time, data_info    \n",
    "    \n",
    "def impute_missing_values(\n",
    "        curr_data: np.ndarray,\n",
    "        )-> np.ndarray:\n",
    "    \"\"\"ArithmeticError\n",
    "    Impute missing values in data.\n",
    "    Args:\n",
    "        curr_data: 1-D array of data.\n",
    "    \n",
    "    Returns:\n",
    "        data: 1-D array of data with missing values filled.\n",
    "    \"\"\"\n",
    "    index = range(1,len(curr_data)+1)\n",
    "    curr_data = pd.DataFrame(data=curr_data,index=index,columns=['data'])\n",
    "    #impute data\n",
    "    imputed_data=curr_data.fillna(1)\n",
    "    \n",
    "    # check for any N/A values\n",
    "    if imputed_data.isnull().any().any():\n",
    "        raise ArithmeticError('Missing values were not imputed correctly')\n",
    "    print(f'imputed data is {imputed_data.data[0].dtype}')\n",
    "    return imputed_data.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(length)):\n",
    "        #create empty array with padding value\n",
    "        tmp_stage = np.empty(max_len)\n",
    "        #fill array with padding value\n",
    "        tmp_stage.fill(padding_value)\n",
    "        #fill array with data\n",
    "        print(f'padding shape {tmp_stage.shape}, while data shape is {loaded_data.iloc[i][\"Sleeping_stage\"].shape}')\n",
    "        #np.copyto(tmp_stage,loaded_data.iloc[i]['Sleeping_stage'])\n",
    "        tmp_stage[0:len(loaded_data['Sleeping_stage'][i])]=loaded_data['Sleeping_stage'][i]\n",
    "        #create empty array for padded time\n",
    "        #tmp_time = np.empty(max_len)\n",
    "        #fill array with padding value\n",
    "        #tmp_time.fill(time_pad)\n",
    "        #fill array with data\n",
    "        #np.copyto(tmp_time,loaded_data.iloc[i]['time'])\n",
    "        #tmp_time[0:len(loaded_data['time'][i])]=loaded_data['time'][i]\n",
    "\n",
    "        #append to prep_data\n",
    "        #prep_data = prep_data.append({'time':tmp_time,'Sleeping_stage':tmp_stage},ignore_index=True)\n",
    "        tmp_time = len(tmp_stage)\n",
    "        prep_data=prep_data.append({'Sleeping_stage':tmp_stage,'time':tmp_time},ignore_index=True)\n",
    "        \n",
    "\n",
    "    # add index column to prep_data\n",
    "    #prep_data['index'] = prep_data.index\n",
    "    \n",
    "\n",
    "\n",
    "    # 3.1 Save dataset to file\n",
    "    if save_dataset == 'Full':\n",
    "        #save dataset to a csv file\n",
    "        prep_data.to_csv('full_dataset.csv',sep=';')\n",
    "    elif save_dataset == 'Limited':\n",
    "        if data_limit is not None or data_limit != '' or data_limit > 0:\n",
    "            #save dataset as limited dataset\n",
    "            prep_data.to_csv('limited_dataset.csv',sep=';')\n",
    "        else:\n",
    "            print(f\"Warning: data_limit is {data_limit} which is not supported value, dataset is not limited, saving full dataset.\")\n",
    "            prep_data.to_csv('full_dataset.csv',sep=';')\n",
    "    elif save_dataset is None or save_dataset == '':\n",
    "        pass\n",
    "    else:\n",
    "        run.stop()\n",
    "        raise ValueError(\"Invalid save_dataset value, valid values are \\'Full\\',\\'Limited\\',None.\")\n",
    "    \"\"\"\n",
    "    \"\"\"for i in tqdm(range(length)):\n",
    "        #create empty array with padding value\n",
    "        tmp_array = np.empty([max_len,1])\n",
    "        tmp_array.fill(padding_value)\n",
    "        #fill array with data\n",
    "        tmp_array[:loaded_data['Sleeping_stage'][i].shape[0],:loaded_data['Sleeping_stage'][i].shape[1]] = loaded_data['Sleeping_stage'][i]\n",
    "        #append to prep_data\n",
    "        prep_data.append(tmp_array)\n",
    "    \"\"\"\n",
    "    #print data type of prepared data.time\n",
    "    #print(f'prep_data time type is {type(prep_data.time[0])}')\n",
    "    #print(f'prep_data shape is {prep_data}')\n",
    "    #print(f'prep_data time shape is {prep_data.time.shape}')\n",
    "    #print(f'prep_data Sleeping_stage shape is {prep_data.Sleeping_stage.shape}')\n",
    "    \n",
    "    #save dataset to a csv file\n",
    "    #prep_data.to_csv('test_dataset.csv',sep=';')\n",
    "\n",
    "\n",
    "    #\n",
    "\n",
    "    #return prep_data.Sleeping_stage ,prep_data.time,data_info\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def MinMaxNormalizer(data,min_value=1,max_value=5):\n",
    "    numerator = data-min_value\n",
    "    denominator = max_value-min_value\n",
    "    norm_data = numerator/denominator\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code directory:\t\t\t/home/torr/HypnoGAN\n",
      "Data directory:\t\t\t/home/torr/HypnoGAN/Data\n",
      "Output directory:\t\t/home/torr/HypnoGAN/Output/test_decosnt\n",
      "Using CUDA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# Experiment Arguments\n",
    "#######################################\n",
    "\n",
    "# select device:\n",
    "device = \"cuda\"\n",
    "\n",
    "#set random seed\n",
    "seed = 0\n",
    "\n",
    "#experiment name\n",
    "exp_name =\"test_decosnt\"\n",
    "\n",
    "#normalization enable\n",
    "norm_enable = False\n",
    "\n",
    "# padding value\n",
    "padding_value = 0.0\n",
    "\n",
    "# Train\n",
    "is_train = True\n",
    "\n",
    "# dataset save:\n",
    "# Full: save full dataset\n",
    "# Limited: save limited dataset\n",
    "# None: don't save dataset\n",
    "save_dataset = None\n",
    "\n",
    "# Enableing x1 to x6 transormation:\n",
    "deconst_enable = True\n",
    "\n",
    "# save arguments to neptune\n",
    "run[\"Initialization/Arguments/ExperimentArgs/device\"] = device\n",
    "run[\"Initialization/Arguments/ExperimentArgs/seed\"] = seed\n",
    "run[\"Initialization/Arguments/ExperimentArgs/exp_name\"] = exp_name\n",
    "run[\"Initialization/Arguments/ExperimentArgs/norm_enable\"] = norm_enable\n",
    "run[\"Initialization/Arguments/ExperimentArgs/padding_value\"] = padding_value\n",
    "run[\"Initialization/Arguments/ExperimentArgs/is_train\"] = is_train\n",
    "#run[\"Initialization/Arguments/ExperimentArgs/save_dataset\"] = save_dataset\n",
    "\n",
    "\n",
    "#######################################\n",
    "# Data Arguments\n",
    "#######################################\n",
    "\n",
    "#data limit for testing\n",
    "# None: use full dataset\n",
    "# int: use limited dataset\n",
    "data_limit = 10000\n",
    "\n",
    "#train test split rate\n",
    "train_rate = 0.4\n",
    "# save arguments to neptune\n",
    "run[\"Initialization/Arguments/DataArgs/data_limit\"] = data_limit\n",
    "run[\"Initialization/Arguments/DataArgs/train_rate\"] = train_rate\n",
    "\n",
    "#######################################\n",
    "# Model Arguments\n",
    "#######################################\n",
    "\n",
    "# embedding model epochs\n",
    "emb_epochs = 300\n",
    "\n",
    "# GAN model epochs\n",
    "gan_epochs = 300\n",
    "\n",
    "# supervised model epochs\n",
    "sup_epochs = 300\n",
    "\n",
    "# batch size\n",
    "batch_size = 64\n",
    "\n",
    "# hidden dimension of RNN\n",
    "hidden_dim = 20\n",
    "\n",
    "# number of layers in RNN\n",
    "num_layers = 3\n",
    "\n",
    "# discriminator threshold\n",
    "dis_thresh = 0.15\n",
    "\n",
    "#learning rate\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "# save arguments to neptune\n",
    "run[\"Initialization/Arguments/ModelArgs/embedding_epochs\"] = emb_epochs\n",
    "run[\"Initialization/Arguments/ModelArgs/gan_epochs\"] = gan_epochs\n",
    "run[\"Initialization/Arguments/ModelArgs/supervised_epochs\"] = sup_epochs\n",
    "run[\"Initialization/Arguments/ModelArgs/batch_size\"] = batch_size\n",
    "run[\"Initialization/Arguments/ModelArgs/hidden_dim\"] = hidden_dim\n",
    "run[\"Initialization/Arguments/ModelArgs/num_layers\"] = num_layers\n",
    "run[\"Initialization/Arguments/ModelArgs/discriminator_threshold\"] = dis_thresh\n",
    "run[\"Initialization/Arguments/ModelArgs/learning_rate\"] = lr\n",
    "\n",
    "\n",
    "\n",
    "######################################\n",
    "# Initialize output directories\n",
    "######################################\n",
    "## runtime directory\n",
    "code_dir = os.path.abspath(\".\")\n",
    "if not os.path.exists(code_dir):\n",
    "    run.stop()\n",
    "    raise ValueError(f\"Code directory not found at {code_dir}\")\n",
    "\n",
    "## Data directory\n",
    "data_path = os.path.abspath(\"./Data\")\n",
    "if not os.path.exists(data_path):\n",
    "    run.stop()\n",
    "    raise ValueError(f\"Data directory not found at {data_path}\")\n",
    "data_dir = os.path.dirname(data_path)\n",
    "data__file_name = os.path.basename(data_path)\n",
    "## Output directory\n",
    "model_path = os.path.abspath(f\"./Output/{exp_name}/\")\n",
    "out_dir = os.path.abspath(model_path)\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir,exist_ok=True)\n",
    "\n",
    "print(f\"Code directory:\\t\\t\\t{code_dir}\")\n",
    "print(f\"Data directory:\\t\\t\\t{data_path}\")\n",
    "print(f\"Output directory:\\t\\t{out_dir}\")\n",
    "\n",
    " ######################################\n",
    "# Initialize random seed and CUDA\n",
    "######################################\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if device == \"cuda\" and torch.cuda.is_available():\n",
    "    print(\"Using CUDA\\n\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    print(\"Using CPU\\n\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_limit is bigger than the dataset size, using the whole dataset\n",
      "                                                        Sleeping_stage  \\\n",
      "File name                                                                \n",
      "abc-baseline-900001  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "abc-baseline-900002  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "abc-baseline-900003  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "abc-baseline-900004  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "abc-baseline-900005  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                     length  additional_info  \n",
      "File name                                     \n",
      "abc-baseline-900001    1027              NaN  \n",
      "abc-baseline-900002     985              NaN  \n",
      "abc-baseline-900003    1030              NaN  \n",
      "abc-baseline-900004    1101              NaN  \n",
      "abc-baseline-900005    1016              NaN  \n",
      "Padding data to 1498 length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8341/8341 [00:00<00:00, 16190.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (8341, 1498, 6)\n",
      "Train dataset length is 5004, Test dataset length is 3337\n",
      "\n",
      "Start Embedding Network Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:23, Loss:0.0109:   8%|         | 24/300 [04:55<59:00, 12.83s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################\n",
    "# Load and preprocess data for model\n",
    "######################################\n",
    "X,T,loaded_data_info = preprocess_data(writer=run,len_limit=0.4,padding_value=padding_value,data_limit=data_limit,save_dataset=save_dataset,norm_enable=norm_enable,deconst_enable=True)\n",
    "#print(X)\n",
    "#print(f\"Processed Data: {X.shape} (Idx x Max_Sequence_Length x Features(=1))\")\n",
    "print(f'Original data shape: {X.shape}')\n",
    "#print(f\"Original data preview:\\n{X[:2, :10, :]}\\n\")\n",
    "feature_dim = X.shape[-1]\n",
    "Z_dim = X.shape[-1]\n",
    "#print(f'feature_dim: {feature_dim}, Z_dim: {Z_dim}')\n",
    "\n",
    "max_seq_len = loaded_data_info['max_length']\n",
    " # Train-Test Split data and time\n",
    " # TODO: Same people should be in the same pool at train test split\n",
    " # Make the split on the subject ID's, so also have to att ID to the loaded data\n",
    "train_data, test_data, train_time, test_time = train_test_split(\n",
    "    X, T, test_size=train_rate, random_state=seed\n",
    ")\n",
    "print(f'Train dataset length is {len(train_time)}, Test dataset length is {len(test_data)}')\n",
    "#########################\n",
    "# Initialize and Run model\n",
    "#########################\n",
    " # Log start time\n",
    "start = time.time()\n",
    "#init TimeGAN model\n",
    "model = TimeGAN(device=device,feature_dim=feature_dim,Z_dim=Z_dim,  hidden_dim=hidden_dim, num_layers=num_layers, max_seq_len=max_seq_len,batch_size=batch_size,padding_value=padding_value)\n",
    "\n",
    "#model = TimeGAN(feature_dim=feature_dim,  hidden_dim=hidden_dim, num_layers=num_layers, max_seq_len=max_seq_len,padding_value=padding_value)\n",
    "if is_train == True:\n",
    "    timegan_trainer(model,train_data,train_time,batch_size=batch_size,lr=lr,emb_epochs=emb_epochs,sup_epochs=sup_epochs,max_seq_length=max_seq_len,Z_dim=Z_dim,dis_thresh=dis_thresh,device=device,model_path=model_path,writer=run)\n",
    "\n",
    "# Log end time\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#generated_time=test_time[:64]\n",
    "logger = trange(int(np.ceil(len(test_time)/batch_size)), desc=f\"Batch: 0/{int(np.ceil(len(test_time)/batch_size))}\")\n",
    "for i in logger:\n",
    "    start_pos = i*batch_size\n",
    "    end_pos = start_pos+batch_size\n",
    "    if end_pos > len(test_time):\n",
    "        end_pos = len(test_time)\n",
    "    tmp_time = test_time[start_pos:end_pos]\n",
    "   \n",
    "    tmp_data = timegan_generator(model, tmp_time, model_path=model_path,batch_size=batch_size,max_seq_len=max_seq_len,Z_dim=Z_dim,device=device)\n",
    "    if deconst_enable:\n",
    "      results =reconstruct_array(tmp_data)\n",
    "    else:\n",
    "      results = tmp_data\n",
    "    \n",
    "    if i !=0:\n",
    "        #print(f'gen shape: {generated_data.shape}, res_shape: {results.shape}')\n",
    "        generated_data=np.concatenate((generated_data,results))\n",
    "        generated_time=np.concatenate((generated_time,tmp_time))\n",
    "    else:\n",
    "        generated_data=results\n",
    "        generated_time=tmp_time\n",
    "        #print(generated_data.shape)\n",
    "    logger.set_description(f\"Batch:{i}/{int(np.ceil(len(test_time)/batch_size))}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#generated_data = timegan_generator(model, test_time, model_path=model_path,batch_size=batch_size,max_seq_len=max_seq_len,Z_dim=Z_dim,device=device)\n",
    "print(f'generated data shape:{generated_data.shape}')\n",
    "#recosntruct data if deconst_enabledeconst_enable = true\n",
    "\n",
    "print(f\"Generated data preview:\\n{generated_data[:1, :, :]}\\n\")\n",
    "print(f\"Model Runtime: {(end - start)/60} mins\\n\")\n",
    "# Save splitted data and generated data\n",
    "with open(f\"{model_path}/train_data.pickle\", \"wb\") as fb:\n",
    "    pickle.dump(train_data, fb)\n",
    "    run[\"pickled_train_data\"].upload(File.as_pickle(train_data))\n",
    "with open(f\"{model_path}/train_time.pickle\", \"wb\") as fb:\n",
    "    pickle.dump(train_time, fb)\n",
    "    run[\"pickled_train_time\"].upload(File.as_pickle(train_time))\n",
    "with open(f\"{model_path}/test_data.pickle\", \"wb\") as fb:\n",
    "    pickle.dump(test_data, fb)\n",
    "    run[\"pickled_test_data\"].upload(File.as_pickle(test_data))\n",
    "with open(f\"{model_path}/test_time.pickle\", \"wb\") as fb:\n",
    "    pickle.dump(test_time, fb)\n",
    "    run[\"pickled_test_time\"].upload(File.as_pickle(test_time))\n",
    "with open(f\"{model_path}/fake_data.pickle\", \"wb\") as fb:\n",
    "    pickle.dump(generated_data, fb)\n",
    "    run[\"pickled_fake_data\"].upload(File.as_pickle(generated_data))\n",
    "with open(f\"{model_path}/fake_time.pickle\", \"wb\") as fb:\n",
    "    pickle.dump(generated_time, fb)\n",
    "    run[\"pickled_fake_time\"].upload(File.as_pickle(generated_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#generate a batch of data figs\n",
    "a = generated_data.shape[0]\n",
    "for i in range(a):\n",
    "    data=generated_data[i]\n",
    "    idx = np.where(data <=0.1)[0][0]\n",
    "    generated_data_un_pad= data[:idx]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(generated_data_un_pad)\n",
    "    if i == 1:\n",
    "        plt.savefig(f\"{model_path}/graphs/generated_fig_nr1.png\")\n",
    "    run[f'results/graphs/Generated_example_nr{i}'].upload(fig)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Stop Neptune experiment\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Remake ID x length x 5 array to ID x length x 1 based on x1 values.\"\"\"\n",
    "\n",
    "\n",
    "def fnc(array):\n",
    "    print(f'array shape: {array.shape}')\n",
    "    \n",
    "    new_array = np.zeros((array.shape[0],array.shape[1],1))\n",
    "    i_max = array.shape[0]-1\n",
    "    j_max = array.shape[1]-1\n",
    "    for i in range(i_max):\n",
    "        for j in range(j_max):\n",
    "            new_array[i,j,0] = np.argmax(array[i,j,:])\n",
    "    print(new_array.shape)\n",
    "    return new_array\n",
    "array = fnc(generated_data)\n",
    "print(array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(generated_data[1,:200])\n",
    "Z = torch.rand(64, 1186, 1).to(device)\n",
    "print(Z.shape)\n",
    "print(generated_data.shape)\n",
    "gen_torch = torch.from_numpy(generated_data)\n",
    "#a =model._discriminator_forward(X=gen_torch[0].to(device),T=test_time[0],Z=Z)\n",
    "\n",
    "b=model.embedder.forward(gen_torch.to(device),test_time)\n",
    "c=model.discriminator.forward(H=b,T=test_time)\n",
    "#print(c[0])\n",
    "c_=c.detach().cpu().numpy()\n",
    "plt.plot(c_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "\n",
    "# Evaluation variables:\n",
    "feat_pred_no = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Preprocess data for seeker\n",
    "#########################\n",
    "# Define enlarge data and its labels\n",
    "enlarge_data = np.concatenate((train_data, test_data), axis=0)\n",
    "enlarge_time = np.concatenate((train_time, test_time), axis=0)\n",
    "enlarge_data_label = np.concatenate((np.ones([train_data.shape[0], 1]), np.zeros([test_data.shape[0], 1])), axis=0)\n",
    "# Mix the order\n",
    "idx = np.random.permutation(enlarge_data.shape[0])\n",
    "enlarge_data = enlarge_data[idx]\n",
    "enlarge_data_label = enlarge_data_label[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_prediction(train_data, test_data, index):\n",
    "    \"\"\"Use the other features to predict a certain feature.\n",
    "\n",
    "    Args:\n",
    "    - train_data (train_data, train_time): training time-series\n",
    "    - test_data (test_data, test_data): testing time-series\n",
    "    - index: feature index to be predicted\n",
    "\n",
    "    Returns:\n",
    "    - perf: average performance of feature predictions (in terms of AUC or MSE)\n",
    "    \"\"\"\n",
    "    train_data, train_time = train_data\n",
    "    test_data, test_time = test_data\n",
    "\n",
    "    # Parameters\n",
    "    no, seq_len, dim = train_data.shape\n",
    "\n",
    "    # Set model parameters\n",
    "\n",
    "    args = {}\n",
    "    args[\"device\"] = \"cuda\"\n",
    "    args[\"task\"] = \"regression\"\n",
    "    args[\"model_type\"] = \"gru\"\n",
    "    args[\"bidirectional\"] = False\n",
    "    args[\"epochs\"] = 20\n",
    "    args[\"batch_size\"] = 128\n",
    "    args[\"in_dim\"] = dim-1\n",
    "    args[\"h_dim\"] = dim-1\n",
    "    args[\"out_dim\"] = 1\n",
    "    args[\"n_layers\"] = 3\n",
    "    args[\"dropout\"] = 0.5\n",
    "    args[\"padding_value\"] = -1.0\n",
    "    args[\"max_seq_len\"] = 100\n",
    "    args[\"learning_rate\"] = 1e-3\n",
    "    args[\"grad_clip_norm\"] = 5.0\n",
    "\n",
    "    # Output initialization\n",
    "    perf = list()\n",
    "  \n",
    "    # For each index\n",
    "    for idx in index:\n",
    "        # Set training features and labels\n",
    "        train_dataset = FeaturePredictionDataset(\n",
    "            train_data, \n",
    "            train_time, \n",
    "            idx\n",
    "        )\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=args[\"batch_size\"],\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # Set testing features and labels\n",
    "        test_dataset = FeaturePredictionDataset(\n",
    "            test_data, \n",
    "            test_time,\n",
    "            idx\n",
    "        )\n",
    "        test_dataloader = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=no,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        model = GeneralRNN(args)\n",
    "        model.to(args[\"device\"])\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=args[\"learning_rate\"]\n",
    "        )\n",
    "\n",
    "        logger = trange(args[\"epochs\"], desc=f\"Epoch: 0, Loss: 0\")\n",
    "        for epoch in logger:\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for train_x, train_t, train_y in train_dataloader:\n",
    "                train_x = train_x.to(args[\"device\"])\n",
    "                train_y = train_y.to(args[\"device\"])\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                train_p = model(train_x, train_t)\n",
    "                loss = criterion(train_p, train_y)\n",
    "                # backward\n",
    "                loss.backward()\n",
    "                # optimize\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            logger.set_description(f\"Epoch: {epoch}, Loss: {running_loss:.4f}\")\n",
    "\n",
    "        \n",
    "        # Evaluate the trained model\n",
    "        with torch.no_grad():\n",
    "            temp_perf = 0\n",
    "            for test_x, test_t, test_y in test_dataloader:\n",
    "                test_x = test_x.to(args[\"device\"])\n",
    "                test_p = model(test_x, test_t).cpu().numpy()\n",
    "\n",
    "                test_p = np.reshape(test_p, [-1])\n",
    "                test_y = np.reshape(test_y.numpy(), [-1])\n",
    "        \n",
    "                temp_perf = rmse_error(test_y, test_p)\n",
    "      \n",
    "        perf.append(temp_perf)\n",
    "    \n",
    "    return perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feat_idx = np.random.permutation(train_data.shape[2])[:feat_pred_no]\n",
    "print(\"Running feature prediction using original data...\")\n",
    "ori_feat_pred_perf = feature_prediction(\n",
    "    (train_data, train_time), \n",
    "    (test_data, test_time),\n",
    "    feat_idx\n",
    ")\n",
    "print(\"Running feature prediction using generated data...\")\n",
    "new_feat_pred_perf = feature_prediction(\n",
    "    (generated_data, generated_time),\n",
    "    (test_data, test_time),\n",
    "    feat_idx\n",
    ")\n",
    "feat_pred = [ori_feat_pred_perf, new_feat_pred_perf]\n",
    "print('Feature prediction results:\\n' +\n",
    "      f'(1) Ori: {str(np.round(ori_feat_pred_perf, 4))}\\n' +\n",
    "      f'(2) New: {str(np.round(new_feat_pred_perf, 4))}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset1 = pd.DataFrame()\n",
    "filename1 = \"Data/generated_dataset_SHHS.csv\"\n",
    "main_dataset1 = pd.read_csv(filename1,sep=';',index_col=0)\n",
    "\n",
    "main_dataset2 = pd.DataFrame()\n",
    "filename2 = \"Data/generated_dataset_ABC.csv\"\n",
    "main_dataset2 = pd.read_csv(filename2,sep=';',index_col=0)\n",
    "\n",
    "#main_dataset1= main_dataset1[:len(main_dataset2)]\n",
    "print(f'len1: {len(main_dataset1)},len2: {len(main_dataset2)}')\n",
    "diff = pd.concat([main_dataset1, main_dataset2]).drop_duplicates(keep=False)\n",
    "#print(diff)\n",
    "#print(main_dataset)\n",
    "#print(main_dataset.head())\n",
    "#for i in range(len(main_dataset.Sleeping_stage)):\n",
    "#    print(type(main_dataset.Sleeping_stage[i]))\n",
    "\n",
    "main_dataset2['Sleeping_stage'] = main_dataset2['Sleeping_stage'].apply(string_to_int_array)\n",
    "\n",
    "\n",
    "mask = main_dataset1['Sleeping_stage'].str.contains('nan', na=False)\n",
    "\n",
    "# drop the rows that contain 'NaN' as part of the string\n",
    "main_dataset1 = main_dataset1[~mask]\n",
    "print(len(main_dataset1))\n",
    "\n",
    "main_dataset1['Sleeping_stage'] = main_dataset1['Sleeping_stage'].apply(string_to_float_array)\n",
    "def string_to_int_array(s):\n",
    "    return [int(x) for x in ast.literal_eval(s)]\n",
    "def string_to_float_array(s):\n",
    "    return [float(x) for x in ast.literal_eval(s)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
