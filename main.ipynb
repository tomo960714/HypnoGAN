{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/NTLAB/HypnoGAN/e/HYPNOG-8\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "# Local packages:\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from typing import Dict, Union\n",
    "\n",
    "# 3rd party packages:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm,trange\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilenames\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import neptune\n",
    "run = neptune.init_run(\n",
    "    project=\"NTLAB/HypnoGAN\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhNGRjNDgzOC04OTk5LTQ0YTktYjQ4Ny1hMTE4NzRjNjBiM2EifQ==\",\n",
    ")\n",
    "\n",
    "\n",
    "# TODO: Implement Neptune logger\n",
    "\n",
    "# personal packages:\n",
    "#from Data.preprocess import preprocess_data\n",
    "#from model.timegan import TimeGAN\n",
    "#from model.utils import timegan_trainer, timegan_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeGAN_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"A time series dataset for TimeGAN.\n",
    "    Args:\n",
    "        data(numpy.ndarray): the padded dataset to be fitted. Has to transform to ndarray from DataFrame during initialize\n",
    "        time(numpy.ndarray): the length of each data\n",
    "    Parameters:\n",
    "        - x (torch.FloatTensor): the real value features of the data\n",
    "        - t (torch.LongTensor): the temporal feature of the data\n",
    "    \"\"\"\n",
    "    def __init__(self,data, time=None):\n",
    "        #sanity check data and time\n",
    "        \n",
    "        \n",
    "        \n",
    "        if isinstance(time,type(None)):\n",
    "            time = [len(x) for x in data]\n",
    "            \n",
    "        if len(data) != len(time):\n",
    "            raise ValueError( f\"len(data) `{len(data)}` != len(time) {len(time)}\")\n",
    "        \n",
    "        self.X = torch.FloatTensor(data)\n",
    "        self.T = torch.LongTensor(time)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx],self.T[idx]\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Minibatch sampling\n",
    "        \"\"\"\n",
    "        # Pad sequences to max length\n",
    "        X_mb = [X for X in batch[0]]\n",
    "        \n",
    "        # The actual length of each data\n",
    "        T_mb = [T for T in batch[1]]\n",
    "        \n",
    "        return X_mb, T_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    The embedding network (encoder) that maps the input data to a latent space.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, hidden_dim, num_layers, padding_value=0, max_seq_len=1000):\n",
    "        super(EmbeddingNetwork, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.padding_value = padding_value\n",
    "        self. max_seq_len = max_seq_len\n",
    "\n",
    "        #Embedder Architecture\n",
    "        self.emb_rnn = nn.GRU(\n",
    "            input_size=self.feature_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.emb_linear = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.emb_sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "        # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L61\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.emb_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(1)\n",
    "\n",
    "            for name, param in self.emb_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "    def forward(self,X,T):\n",
    "        \"\"\"Forward pass of the embedding features from original space to latent space.\n",
    "        Args:\n",
    "            X: Input time series feature (B x S x F)\n",
    "            T: INput temporal information (B)\n",
    "        Returns:\n",
    "            H: latent space embeddings (B x S x H)\n",
    "        \"\"\"\n",
    "        # Dynamic RNN input for ignoring paddings\n",
    "\n",
    "        X_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input =X,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # 128*100*71\n",
    "        H_o,H_t = self.emb_rnn(X_pack)\n",
    "\n",
    "        #pad RNN output back to sequence length\n",
    "\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "\n",
    "        #128*100*10\n",
    "        logits = self.emb_linear(H_o)\n",
    "        H = self.emb_sigmoid(logits)\n",
    "\n",
    "        return H\n",
    "    \n",
    "class RecoveryNetwork(nn.Module):\n",
    "    \"\"\"The recovery network (decoder) for TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,hidden_dim,feature_dim,num_layers,padding_value=0,max_seq_len=1000):\n",
    "        super(RecoveryNetwork, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.padding_value = padding_value\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        #Recovery Architecture\n",
    "        self.rec_rnn = nn.GRU(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.rec_linear = nn.Linear(self.hidden_dim, self.feature_dim)\n",
    "\n",
    "        # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name,param in self.rec_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(0)\n",
    "            for name,param in self.rec_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        \n",
    "    def forward(self,H,T):\n",
    "        \"\"\" Forward pass of the recovery features from latent space to original space.\n",
    "        Args:\n",
    "            H: latent representation (B x S x E)\n",
    "            T: input temporal information (B)\n",
    "        Returns:\n",
    "            X_tilde: recovered features (B x S x F)\n",
    "        \"\"\"\n",
    "        #Dynamic RNN input for ignoring paddings\n",
    "        H_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input = H,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        #128 x 100 x 10\n",
    "        H_o,H_t = self.rec_rnn(H_pack)\n",
    "        #pad RNN output back to sequence length\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "        #128 x 100 x 71\n",
    "        X_tilde = self.rec_linear(H_o)\n",
    "        return X_tilde\n",
    "\n",
    "class SupervisorNetwork(nn.Module):\n",
    "        \"\"\"The supervisor network for TimeGAN\n",
    "        \"\"\"\n",
    "        def __init__(self,hidden_dim,num_layers,padding_value=0,max_seq_len=1000):\n",
    "            super(SupervisorNetwork,self).__init__()\n",
    "            self.hidden_dim =hidden_dim\n",
    "            self.num_layers = num_layers\n",
    "            self.padding_value = padding_value\n",
    "            self.max_seq_len = max_seq_len\n",
    "\n",
    "            #supervisor architecture\n",
    "            self.sup_rnn = nn.GRU(\n",
    "                input_size=self.hidden_dim,\n",
    "                hidden_size=self.hidden_dim,\n",
    "                num_layers=self.num_layers-1,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            self.sup_linear = nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "            self.sup_sigmoid = nn.Sigmoid()\n",
    "             # Init weights\n",
    "            # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "            # Reference: \n",
    "            # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "            # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "            with torch.no_grad():\n",
    "                for name, param in self.sup_rnn.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'bias_ih' in name:\n",
    "                        param.data.fill_(1)\n",
    "                    elif 'bias_hh' in name:\n",
    "                        param.data.fill_(0)\n",
    "                for name, param in self.sup_linear.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        param.data.fill_(0)\n",
    "        def forward(self,H,T):\n",
    "            \"\"\"Forward pass for the supervisor for predicting next step\n",
    "            Args:\n",
    "                H: latent representation (B x S x E)\n",
    "                T: input temporal information (B)\n",
    "            Returns:\n",
    "                H_hat: predicted next step data (B x S x E)\n",
    "            \"\"\"\n",
    "\n",
    "            #Dynamic RNN input for ignoring paddings\n",
    "            H_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "                input = H,\n",
    "                lengths=T,\n",
    "                batch_first=True,\n",
    "                enforce_sorted=False,\n",
    "            )\n",
    "\n",
    "            H_o,H_t = self.sup_rnn(H_pack)\n",
    "            #pad RNN output back to sequence length\n",
    "            H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "                sequence=H_o,\n",
    "                batch_first=True,\n",
    "                padding_value=self.padding_value,\n",
    "                total_length=self.max_seq_len,\n",
    "            )\n",
    "            logits = self.sup_linear(H_o)\n",
    "            H_hat = self.sup_sigmoid(logits)\n",
    "            return H_hat\n",
    "\n",
    "class GeneratorNetwork(nn.Module):\n",
    "    \"\"\"The generator network for TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,Z_dim,hidden_dim,num_layers,padding_value=0,max_seq_len=1000):\n",
    "        super(GeneratorNetwork,self).__init__()\n",
    "        self.Z_dim = Z_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.padding_value = padding_value\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        #Generator Architecture\n",
    "        self.gen_rnn = nn.GRU(\n",
    "            input_size=self.Z_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.gen_linear = nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "        self.gen_sigmoid = nn.Sigmoid()\n",
    "                # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.gen_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(0)\n",
    "            for name, param in self.gen_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        \n",
    "    def forward(self,Z,T):\n",
    "        \"\"\" Takes in random noise (features) and generates synthetic features within the last latent space\n",
    "        Args:\n",
    "            Z: input random noise (B x S x Z)\n",
    "            T: input temporal information (B)\n",
    "        Returns:\n",
    "            H: embeddings (B x S x E)\n",
    "        \"\"\"\n",
    "        #Dynamic RNN input for ignoring paddings\n",
    "        Z_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input = Z,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # 128*100*71\n",
    "        H_o,H_t = self.gen_rnn(Z_pack)\n",
    "\n",
    "        #pad RNN output back to sequence length\n",
    "\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "\n",
    "        #128*100*10\n",
    "        logits = self.gen_linear(H_o)\n",
    "        H = self.gen_sigmoid(logits)\n",
    "\n",
    "        return H\n",
    "\n",
    "class DiscriminatorNetwork(nn.Module):\n",
    "    \"\"\"The discriminator network for TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,hidden_dim,num_layers,padding_value=0,max_seq_len=1000   ):\n",
    "        super(DiscriminatorNetwork,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.padding_value = padding_value\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        #Discriminator Architecture\n",
    "        self.dis_rnn = nn.GRU(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dis_linear = nn.Linear(self.hidden_dim,1)\n",
    "\n",
    "        # Init weights\n",
    "        # Default weights of TensorFlow is Xavier Uniform for W and 1 or 0 for b\n",
    "        # Reference: \n",
    "        # - https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable\n",
    "        # - https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py#L484-L614\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.dis_rnn.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    param.data.fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    param.data.fill_(0)\n",
    "            for name, param in self.dis_linear.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    torch.nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "    \n",
    "    def forward(self, H, T):\n",
    "        \"\"\" Forward pass for predicting if the data is real or synthetic\n",
    "        \n",
    "        Args:\n",
    "            H: latent representation (B x S x E)\n",
    "            T: input temporal information (B)\n",
    "        Returns:\n",
    "        logits: prediction logits(B x S x 1)\n",
    "        \"\"\"\n",
    "        # dynamic RNN input for ignoring paddings\n",
    "        H_pack = nn.utils.rnn.pack_padded_sequence(\n",
    "            input = H,\n",
    "            lengths=T,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # 128*100*10\n",
    "        H_o,H_t = self.dis_rnn(H_pack)\n",
    "\n",
    "        # pad RNN output back to sequence length\n",
    "        H_o,T = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=H_o,\n",
    "            batch_first=True,\n",
    "            padding_value=self.padding_value,\n",
    "            total_length=self.max_seq_len,\n",
    "        )\n",
    "\n",
    "        logits = self.dis_linear(H_o).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "class TimeGAN(nn.Module):\n",
    "    \"\"\" Implementation of TimeGan (Yoon et al., 2019) using PyTorch\n",
    "    \n",
    "    Reference:\n",
    "        - Yoon, J., Jarret, D., van der Schaar, M. (2019). Time-series Generative Adversarial Networks. (https://papers.nips.cc/paper/2019/hash/c9efe5f26cd17ba6216bbe2a7d26d490-Abstract.html)\n",
    "        - https://github.com/jsyoon0823/TimeGAN\n",
    "    \"\"\"\n",
    "    def __init__(self,device,feature_dim,Z_dim,hidden_dim,max_seq_len,batch_size,num_layers,padding_value):\n",
    "        super(TimeGAN,self).__init__()\n",
    "        self.device =device\n",
    "        self.feature_dim = feature_dim\n",
    "        self.Z_dim = Z_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.embedder = EmbeddingNetwork(feature_dim=feature_dim,hidden_dim=hidden_dim,num_layers=num_layers,padding_value=padding_value,max_seq_len=max_seq_len)\n",
    "        self.recovery = RecoveryNetwork(feature_dim=feature_dim,hidden_dim=hidden_dim,num_layers=num_layers,padding_value=padding_value,max_seq_len=max_seq_len)\n",
    "        self.generator = GeneratorNetwork(Z_dim=Z_dim,feature_dim=feature_dim,hidden_dim=hidden_dim,num_layers=num_layers,padding_value=padding_value,max_seq_len=max_seq_len)\n",
    "        self.discriminator = DiscriminatorNetwork(feature_dim=feature_dim,hidden_dim=hidden_dim,num_layers=num_layers,padding_value=padding_value,max_seq_len=max_seq_len)\n",
    "        self.supervisor = SupervisorNetwork(feature_dim=feature_dim,hidden_dim=hidden_dim,num_layers=num_layers,padding_value=padding_value,max_seq_len=max_seq_len)\n",
    "\n",
    "    def _recovery_forward(self, X, T):\n",
    "        \"\"\" The embedding network forward pass and the embedder network loss\n",
    "        Args:\n",
    "            X: input features\n",
    "            T: input temporal information\n",
    "        Returns:\n",
    "            E_loss: the reconstruction loss\n",
    "            X_tilde: the reconstructed features\n",
    "        \"\"\"\n",
    "\n",
    "        # FOrward pass\n",
    "        H = self.embedder(X,T)\n",
    "        X_tilde = self.recovery(H,T)\n",
    "\n",
    "        #for Joint training\n",
    "        H_hat_supervise = self.supervisor(H,T)\n",
    "        G_loss_S = F.mse_loss(\n",
    "            H_hat_supervise[:,:-1,:],\n",
    "            H[:,1:,:],\n",
    "        ) #Teacher forcing next output\n",
    "\n",
    "        #Reconstruction loss\n",
    "        E_loss_T0 = F.mse_loss(X_tilde,X)\n",
    "        E_loss0 = 10*torch.sqrt(E_loss_T0)\n",
    "        E_loss = E_loss0 + 0.1*G_loss_S\n",
    "        return E_loss, E_loss0,E_loss_T0\n",
    "    def _supervisor_forward(self, X, T):\n",
    "        \"\"\" The supervisor training forward pass\n",
    "        Args:\n",
    "            X: original input features\n",
    "            T: input temporal information\n",
    "        Returns:\n",
    "            S_loss: the supervisor's loss\n",
    "        \"\"\"\n",
    "        #supervisor forward pass\n",
    "        H = self.embedder(X,T)\n",
    "        H_hat_supervise = self.supervisor(H,T)\n",
    "\n",
    "        #supervised loss\n",
    "        S_loss = F.mse_loss(\n",
    "            H_hat_supervise[:,:-1,:],\n",
    "            H[:,1:,:],\n",
    "        ) #Teacher forcing next output\n",
    "        return S_loss\n",
    "    def _discriminator_forward(self, X, T, Z, gamma=1):\n",
    "        \"\"\" The discriminator forward pass and adversarial loss\n",
    "        Args:\n",
    "            X: input features\n",
    "            T: input temporal information\n",
    "            Z: input noise\n",
    "            gamma: the weight for the adversarial loss\n",
    "        Returns:\n",
    "            D_loss: adversarial loss\n",
    "        \"\"\"\n",
    "        #Real\n",
    "        H = self.embedder(X, T).detach()\n",
    "\n",
    "        #generator\n",
    "        E_hat = self.generator(Z,T).detach()\n",
    "        H_hat = self.supervisor(E_hat,T).detach()\n",
    "        \n",
    "        #forward pass\n",
    "        Y_real = self.discriminator(H,T)        #Encode original data\n",
    "        Y_fake = self.discriminator(H_hat,T)    #Output of generator + supervisor\n",
    "        Y_fake_e = self.discriminator(E_hat,T)  #Output of generator\n",
    "\n",
    "        D_loss_real = F.binary_cross_entropy_with_logits(Y_real, torch.ones_like(Y_real))\n",
    "        D_loss_fake = F.binary_cross_entropy_with_logits(Y_fake, torch.zeros_like(Y_fake))\n",
    "        D_loss_fake_e = F.binary_cross_entropy_with_logits(Y_fake_e, torch.zeros_like(Y_fake_e))\n",
    "\n",
    "        D_loss = D_loss_real + D_loss_fake + gamma * D_loss_fake_e\n",
    "\n",
    "        return D_loss\n",
    "    \n",
    "    def _generator_forward(self, X, T, Z, gamma=1):\n",
    "        \"\"\" The generator forward pass\n",
    "        Args:\n",
    "            X: original input features\n",
    "            T: input temporal information\n",
    "            Z: input noise for the generator\n",
    "            gamma: the weight for the adversarial loss\n",
    "        Returns:\n",
    "            G_loss: the generator loss\n",
    "        \"\"\"\n",
    "        #supervisor forward pass\n",
    "        H = self.embedder(X,T)\n",
    "        H_hat_supervise = self.supervisor(H,T)\n",
    "\n",
    "        #generator forward pass\n",
    "        E_hat = self.generator(Z,T)\n",
    "        H_hat = self.supervisor(E_hat,T)\n",
    "\n",
    "        #synthetic data generated\n",
    "        X_hat = self.recovery(H_hat,T)\n",
    "\n",
    "        #generator loss\n",
    "        #Adversarial loss\n",
    "        Y_fake = self.discriminator(H_hat,T)        #Output of supervisor\n",
    "        Y_fake_e = self.discriminator(E_hat,T)      #Output of generator\n",
    "\n",
    "        G_loss_U = F.binary_cross_entropy_with_logits(Y_fake, torch.ones_like(Y_fake))\n",
    "        G_loss_U_e = F.binary_cross_entropy_with_logits(Y_fake_e, torch.ones_like(Y_fake_e))\n",
    "\n",
    "        #Supervised loss\n",
    "        G_loss_S = F.mse_loss(\n",
    "            H_hat_supervise[:,:-1,:],\n",
    "            H[:,1:,:],\n",
    "        ) #Teacher forcing next output\n",
    "\n",
    "        #Two moments losses\n",
    "        G_loss_V1 = torch.mean(\n",
    "            torch.abs(torch.sqrt(X_hat.var(dim=0,unbiased=False)+1e-6) - torch.sqrt(X.var(dim=0,unbiased=False)+1e-6))\n",
    "        )\n",
    "        G_loss_V2 = torch.mean(torch.abs((X_hat.mean(dim=0)) - (X.mean(dim=0))))\n",
    "        G_loss_V = G_loss_V1 + G_loss_V2\n",
    "        \n",
    "        #sum of losses\n",
    "        G_loss = G_loss_U + gamma * G_loss_U_e + 100 * torch.sqrt(G_loss_S) + 100 * G_loss_V\n",
    "    \n",
    "        return G_loss\n",
    "    \n",
    "    def _inference(self, Z,T):\n",
    "        \"\"\" Inference for generating synthetic data\n",
    "        Args:\n",
    "            Z: input noise\n",
    "            T: temporal information\n",
    "        Returns:\n",
    "            X_hat: the generated data\n",
    "        \"\"\"\n",
    "\n",
    "        #generator forward pass\n",
    "        E_hat = self.generator(Z,T)\n",
    "        H_hat = self.supervisor(E_hat,T)\n",
    "\n",
    "        #synthetic data generated\n",
    "        X_hat = self.recovery(H_hat,T)\n",
    "        return X_hat\n",
    "\n",
    "    def forward(self,X,T,Z, obj, gamma=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: input features (B,H,F)\n",
    "            T: The temporal information (B)\n",
    "            Z: the sampled noise (B,H,Z)\n",
    "            obj: the network to be trained ('autoencoder','supervisor','generator','discriminator')\n",
    "            gamma: loss hyperparameter\n",
    "        Returns:\n",
    "            loss: loss for the forward pass\n",
    "            X_hat: the generated data\n",
    "        \"\"\"\n",
    "\n",
    "        #Move variables to device\n",
    "        if obj !='inference':\n",
    "            if X is None:\n",
    "                raise ValueError('X cannot be empty')\n",
    "            \n",
    "            X = torch.FloatTensor(X)\n",
    "            X = X.to(self.device)\n",
    "\n",
    "        if Z is not None:\n",
    "            Z = torch.FloatTensor(Z)\n",
    "            Z = Z.to(self.device)\n",
    "        \n",
    "        if obj == 'autoencoder':\n",
    "            #embedder and recovery forward\n",
    "            loss = self._recovery_forward(X,T)\n",
    "        elif obj == 'supervisor':\n",
    "            loss = self._supervisor_forward(X,T)\n",
    "        elif obj == 'generator':\n",
    "            if Z is None:\n",
    "                raise ValueError('Z cannot be empty')\n",
    "            loss = self._generator_forward(X,T,Z,gamma)\n",
    "        elif obj == 'discriminator':\n",
    "            if Z is None:\n",
    "                raise ValueError('Z cannot be empty')\n",
    "            loss = self._discriminator_forward(X,T,Z,gamma)\n",
    "            return loss\n",
    "        elif obj == 'inference':\n",
    "            X_hat = self._inference(Z,T)\n",
    "            X_hat = X_hat.cpu.detach()\n",
    "\n",
    "            return X_hat\n",
    "        else:\n",
    "            raise ValueError('obj must be autoencoder, supervisor, generator or discriminator')\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_trainer(\n",
    "        model: torch.nn.Module,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        e_opt: torch.optim.Optimizer,\n",
    "        r_opt: torch.optim.Optimizer,\n",
    "        emb_epochs: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Training loop for embedding and recovery functions.\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader to use\n",
    "        e_opt (torch.optim.Optimizer): The optimizer for the embedding function\n",
    "        r_opt (torch.optim.Optimizer): The optimizer for the recovery function\n",
    "        args (Dict): The model/training configuration\n",
    "        writer (Union[torch.utils.tensorboard.SummaryWriter, type(None)], optional): The tensorboard writer to use. Defaults to None.\n",
    "    \"\"\"\n",
    "    logger = trange(emb_epochs, desc =f\"Epoch:0, Loss:0\")\n",
    "    for epoch in logger:\n",
    "        for X_mb,T_mb in dataloader:\n",
    "\n",
    "            #reset gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            #forward pass\n",
    "            _,E_loss0,E_loss_T0 = model(X=X_mb,T=T_mb,Z=None,obj=\"autoencoder\")\n",
    "            loss = np.sqrt(E_loss_T0.item())\n",
    "\n",
    "            #backward pass\n",
    "            E_loss0.backward()\n",
    "\n",
    "            #update weights\n",
    "            e_opt.step()\n",
    "            r_opt.step()\n",
    "\n",
    "        # Log loss for final batch of each epochs\n",
    "        logger.set_description(f\"Epoch:{epoch}, Loss:{loss:.4f}\")\n",
    "        \"\"\"if writer:\n",
    "            writer.add_scalar(\"Embedding/Loss:\",loss,epoch)\n",
    "            writer.flush()\"\"\"\n",
    "\n",
    "def supervisor_trainer(\n",
    "    model: torch.nn.Module, \n",
    "    dataloader: torch.utils.data.DataLoader, \n",
    "    s_opt: torch.optim.Optimizer, \n",
    "    g_opt: torch.optim.Optimizer,\n",
    "    sup_epochs: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    The training loop for the supervisor function\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader to use\n",
    "        s_opt (torch.optim.Optimizer): The optimizer for the supervisor function\n",
    "        g_opt (torch.optim.Optimizer): The optimizer for the generator function\n",
    "        args (Dict): The model/training configuration\n",
    "        writer (Union[torch.utils.tensorboard.SummaryWriter, type(None)], optional): The tensorboard writer to use. Defaults to None.\n",
    "    \"\"\"\n",
    "    logger = trange(sup_epochs, desc=f\"Epoch: 0, Loss: 0\")\n",
    "    for epoch in logger:\n",
    "        for X_mb, T_mb in dataloader:\n",
    "            # Reset gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Forward Pass\n",
    "            S_loss = model(X=X_mb, T=T_mb, Z=None, obj=\"supervisor\")\n",
    "\n",
    "            # Backward Pass\n",
    "            S_loss.backward()\n",
    "            loss = np.sqrt(S_loss.item())\n",
    "\n",
    "            # Update model parameters\n",
    "            s_opt.step()\n",
    "\n",
    "        # Log loss for final batch of each epoch (29 iters)\n",
    "        logger.set_description(f\"Epoch: {epoch}, Loss: {loss:.4f}\")\n",
    "        \"\"\"if writer:\n",
    "            writer.add_scalar(\n",
    "                \"Supervisor/Loss:\",loss,epoch)\n",
    "            writer.flush()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_trainer(\n",
    "    model: torch.nn.Module, \n",
    "    dataloader: torch.utils.data.DataLoader, \n",
    "    e_opt: torch.optim.Optimizer, \n",
    "    r_opt: torch.optim.Optimizer, \n",
    "    s_opt: torch.optim.Optimizer, \n",
    "    g_opt: torch.optim.Optimizer, \n",
    "    d_opt: torch.optim.Optimizer,\n",
    "    sup_epochs: int,\n",
    "    batch_size: int,\n",
    "    max_seq_len: int,\n",
    "    Z_dim: int,\n",
    "    dis_thresh: float,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    The training loop for training the model altogether\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader to use\n",
    "        e_opt (torch.optim.Optimizer): The optimizer for the embedding function\n",
    "        r_opt (torch.optim.Optimizer): The optimizer for the recovery function\n",
    "        s_opt (torch.optim.Optimizer): The optimizer for the supervisor function\n",
    "        g_opt (torch.optim.Optimizer): The optimizer for the generator function\n",
    "        d_opt (torch.optim.Optimizer): The optimizer for the discriminator function\n",
    "        args (Dict): The model/training configuration\n",
    "        writer (Union[torch.utils.tensorboard.SummaryWriter, type(None)], optional): The tensorboard writer to use. Defaults to None.\n",
    "    \"\"\"\n",
    "    logger = trange(\n",
    "        sup_epochs, \n",
    "        desc=f\"Epoch: 0, E_loss: 0, G_loss: 0, D_loss: 0\"\n",
    "    )\n",
    "    for epoch in logger:\n",
    "        for X_mb, T_mb in dataloader:\n",
    "            ## Generator Training\n",
    "            for _ in range(2):\n",
    "                # Random Generator\n",
    "                Z_mb = torch.rand((batch_size, max_seq_len, Z_dim))\n",
    "\n",
    "                # Forward Pass (Generator)\n",
    "                model.zero_grad()\n",
    "                G_loss = model(X=X_mb, T=T_mb, Z=Z_mb, obj=\"generator\")\n",
    "                G_loss.backward()\n",
    "                G_loss = np.sqrt(G_loss.item())\n",
    "\n",
    "                # Update model parameters\n",
    "                g_opt.step()\n",
    "                s_opt.step()\n",
    "\n",
    "                # Forward Pass (Embedding)\n",
    "                model.zero_grad()\n",
    "                E_loss, _, E_loss_T0 = model(X=X_mb, T=T_mb, Z=Z_mb, obj=\"autoencoder\")\n",
    "                E_loss.backward()\n",
    "                E_loss = np.sqrt(E_loss.item())\n",
    "                \n",
    "                # Update model parameters\n",
    "                e_opt.step()\n",
    "                r_opt.step()\n",
    "\n",
    "            # Random Generator\n",
    "            Z_mb = torch.rand((batch_size, max_seq_len, Z_dim))\n",
    "\n",
    "            ## Discriminator Training\n",
    "            model.zero_grad()\n",
    "            # Forward Pass\n",
    "            D_loss = model(X=X_mb, T=T_mb, Z=Z_mb, obj=\"discriminator\")\n",
    "\n",
    "            # Check Discriminator loss\n",
    "            if D_loss > dis_thresh:\n",
    "                # Backward Pass\n",
    "                D_loss.backward()\n",
    "\n",
    "                # Update model parameters\n",
    "                d_opt.step()\n",
    "            D_loss = D_loss.item()\n",
    "\n",
    "        logger.set_description(\n",
    "            f\"Epoch: {epoch}, E: {E_loss:.4f}, G: {G_loss:.4f}, D: {D_loss:.4f}\"\n",
    "        )\n",
    "        \"\"\"if writer:\n",
    "            writer.add_scalar(\n",
    "                'Joint/Embedding_Loss:',E_loss,epoch)\n",
    "            writer.add_scalar(\n",
    "                'Joint/Generator_Loss:',G_loss,epoch)\n",
    "            writer.add_scalar('Joint/Discriminator_Loss:',D_loss,epoch)\n",
    "            writer.flush()\"\"\"\n",
    "\n",
    "def timegan_trainer(model,train_data,train_time,batch_size,lr,emb_epochs,sup_epochs,max_seq_length,Z_dim,dis_thresh,device,model_path):\n",
    "    \"\"\"\n",
    "    The trainign procedure for TimeGAN.\n",
    "    Args:\n",
    "        model (torch.nn.module): The model that generates synthetic data\n",
    "        loaded_data(pandas.DataFrame): The data to train on, including data and time\n",
    "        args (Dict): The model/training configuration\n",
    "    Returns:\n",
    "        generated_data (np.array): The synthetic data generated by the model\n",
    "    \"\"\"\n",
    "    dataset = TimeGAN_Dataset(data=train_data,time=train_time)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False)\n",
    "    model.to(device)\n",
    "\n",
    "    #initialize optimizers\n",
    "    e_opt = torch.optim.Adam(model.embedder.parameters(), lr=lr)\n",
    "    r_opt = torch.optim.Adam(model.recovery.parameters(), lr=lr)\n",
    "    s_opt = torch.optim.Adam(model.supervisor.parameters(), lr=lr)\n",
    "    g_opt = torch.optim.Adam(model.generator.parameters(), lr=lr)\n",
    "    d_opt = torch.optim.Adam(model.discriminator.parameters(), lr=lr)\n",
    "\n",
    "    #initialize tensorboard writer\n",
    "    #writer = SummaryWriter(os.path.join(f\"tensorboard/{args.exp}\"))\n",
    "\n",
    "    print(\"\\nStart Embedding Network Training\")\n",
    "    embedding_trainer(model=model, dataloader=dataloader, e_opt=e_opt, r_opt=r_opt, emb_epochs=emb_epochs)\n",
    "\n",
    "    print(\"\\nStart Training with Supervised Loss Only\")\n",
    "    supervisor_trainer(model=model, dataloader=dataloader, s_opt=s_opt,g_opt=g_opt, sup_epochs=sup_epochs)\n",
    "\n",
    "    print(\"\\nStart Joint Training\")\n",
    "    joint_trainer(model=model, dataloader=dataloader, e_opt=e_opt, r_opt=r_opt, s_opt=s_opt, g_opt=g_opt, d_opt=d_opt, sup_epochs=sup_epochs, batch_size=batch_size, max_seq_len=max_seq_length, Z_dim=Z_dim, dis_thresh=dis_thresh)\n",
    "\n",
    "\n",
    "    #save model,args, and hyperparameters\n",
    "    #torch.save(args,f\"{args.model_path}/args.pickle\")\n",
    "    torch.save(model.state_dict(),f\"{model_path}/model.pt\")\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def timegan_generator(model,T,model_path,batch_size, max_seq_len, Z_dim ,device):\n",
    "    \"\"\"\n",
    "    The interference procedure for TimeGAN.\n",
    "    Args:\n",
    "        model (torch.nn.module): The model that generates synthetic data\n",
    "        T (List[int]): The time to generate data for\n",
    "        args (Dict): The model/training configuration\n",
    "    returns:\n",
    "        generated_data (np.array): The synthetic data generated by the model\n",
    "    \"\"\"\n",
    "    #load model\n",
    "    if not os.path.exists(model_path):\n",
    "        raise ValueError(f\"Model not found at {model_path}\")\n",
    "    model.load_state_dict(torch.load(f\"{model_path}/model.pt\"))\n",
    "    print(\"\\nStart Generating Synthetic Data\")\n",
    "    #Initialize model to evaluation mode and run without gradients\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Random Generator\n",
    "        Z = torch.rand((batch_size, max_seq_len, Z_dim))\n",
    "        # Forward Pass (Generator)\n",
    "        generated_data = model(X=None, T=T, Z=Z, obj=\"inference\")\n",
    "    return generated_data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_limit=None,save_dataset=None):\n",
    "    \"\"\"\n",
    "    data_limit=None,save_dataset=None\n",
    "    Load and preprocess real life datasets.\n",
    "    \n",
    "    Args:\n",
    "        data_limit (int): The number of data points to load. If None, all data points are loaded. Default: None. Used for testing.\n",
    "        save_dataset (bool): If 'Full', the dataset is saved to a csv file. If it's 'limited', than save the limited dataset if data_limit is not None. Default: None.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        dataset (pandas.DataFrame): The dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    main_dataset = pd.DataFrame()\n",
    "\n",
    "    \n",
    "    Tk().withdraw() # we don't want a full GUI, so keep the root window from appearing\n",
    "    filenames = askopenfilenames() # show an \"Open\" dialog box and return the path to the selected file\n",
    "    print(filenames)\n",
    "    for filename in filenames:\n",
    "        \n",
    "        if filename.endswith('.mat'):\n",
    "            #output df format: [id,value_array]\n",
    "            df = load_mat_as_df(filename)\n",
    "            print(df)\n",
    "        \n",
    "        elif filename.endswith('.csv'):\n",
    "\n",
    "            #use create_dataset_csv.py to create a csv file\n",
    "            if filename.find('dataset') != -1:\n",
    "                df = pd.read_csv(filename,sep=';',index_col=0)\n",
    "\n",
    "            \"\"\" CSV format:\n",
    "            ID|time|Sleeping stage|length|additional_info\n",
    "\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        elif filename.endswith('.xml'):\n",
    "            ## TODO: add xml support\n",
    "            #df =\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Unsupported file format, skipping file:\",filename,\".\")\n",
    "            pass\n",
    "    \n",
    "        main_dataset.append(df)\n",
    "\n",
    "    #Cut df to data_limit size for testing purposes\n",
    "    if data_limit is not None:\n",
    "        if save_dataset == 'Full':\n",
    "            #save dataset to a csv file\n",
    "            main_dataset.to_csv('Full_dataset.csv',sep=';')\n",
    "        \n",
    "        elif save_dataset == 'Limited':\n",
    "            main_dataset = main_dataset[:data_limit]\n",
    "            #save dataset to a csv file\n",
    "            main_dataset.to_csv('limited_dataset.csv',sep=';')\n",
    "        elif save_dataset == 'None':\n",
    "            main_dataset = main_dataset[:data_limit]\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Invalid save_dataset value, valid values are 'Full','Limited','None'.\")\n",
    "\n",
    "    elif data_limit is None:\n",
    "        if save_dataset == 'Full':\n",
    "            #save dataset to a csv file\n",
    "            main_dataset.to_csv('full_dataset.csv',sep=';')\n",
    "        elif save_dataset == 'Limited':\n",
    "            print(\"Warning: data_limit is None, dataset is not limited, saving full dataset.\")\n",
    "            main_dataset.to_csv('full_dataset.csv',sep=';')\n",
    "        elif save_dataset == 'None':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Invalid save_dataset value, valid values are 'Full','Limited','None'.\")\n",
    "\n",
    "    \n",
    "    return main_dataset #dataset as df\n",
    "\n",
    "def load_mat_as_df(mat_file_path, var_name):\n",
    "    mat = sio.loadmat(mat_file_path,simplify_cells=True)\n",
    "\n",
    "    if var_name not in list(mat.keys()):\n",
    "        var_name = get_variable_name(mat)   \n",
    "        \n",
    "\n",
    "    return pd.DataFrame(mat[var_name])\n",
    "\n",
    "def get_variable_name(loaded_mat):\n",
    "\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    root.title('.mat variable selector')\n",
    "    tk.Label(root, text=\"Choose a variable:\").pack()\n",
    "    choices = list(loaded_mat.keys())\n",
    "\n",
    "    variable = tk.StringVar(root)\n",
    "    variable.set(choices[0]) # default value\n",
    "    w = tk.Combobox(root, textvariable=variable,values=choices)\n",
    "\n",
    "    w.pack()\n",
    "    def ok():\n",
    "        print (\"value is:\" + variable.get())\n",
    "        root.destroy()\n",
    "    def cancel():\n",
    "        root.destroy()\n",
    "        raise InterruptedError('User cancelled, invalid variable name')\n",
    "\n",
    "    button1 = tk.Button(root, text=\"OK\", command=ok)\n",
    "    button2 = tk.Button(root, text=\"Cancel\", command=cancel)\n",
    "    button1.pack()\n",
    "    button2.pack()\n",
    "    root.mainloop()\n",
    "    \n",
    "    return variable.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(padding_value,data_limit=None,save_dataset=None,norm_enable=False):\n",
    "    \"\"\"\n",
    "    padding_value: int = -1.0,\n",
    "    data_limit: int = None\"\"\"\n",
    "    # Load and preprocess data\n",
    "    # \n",
    "    # 1. Load data from files (csv,mat,xml)\n",
    "    # 2. Preprocess data:\n",
    "    # 2.1. Remove outliers\n",
    "    # 2.2. Extract sequence length and time\n",
    "    # 2.3. Resample data\n",
    "    # 2.4. Normalize data\n",
    "    # 2.5. Padding \n",
    "    #  \n",
    "    # Args:\n",
    "    #     data_limit (int): The number of lines to load from the data file\n",
    "    #     padding_value (int): The value used for padding\n",
    "    #     \n",
    "    # \n",
    "    # Returns:\n",
    "    #     prep_data (pandas.DataFrame): The processed data\n",
    "\n",
    "    #######################################\n",
    "    # 1. Load data from files (csv,mat,xml)\n",
    "    #######################################\n",
    "\n",
    "    loaded_data = load_data(data_limit=data_limit,save_dataset=save_dataset)\n",
    "    \"\"\"\n",
    "    loaded data =       time_data   , data  , length\n",
    "    (pandas.DataFrame), (np.array)  ,(list) ,(int)\n",
    "    \n",
    "    ()\n",
    "    \"\"\"\n",
    "    #######################################\n",
    "    # 2. Preprocess data:\n",
    "    #######################################\n",
    "    # 2.1. Remove outliers\n",
    "    #######################################\n",
    "    \"\"\"\n",
    "    Remove row's with unacceptable sleep stages values\n",
    "    \"\"\"\n",
    "    sleep_stages = np.array([1,2,3,4,5])\n",
    "    loaded_data[loaded_data['data'].apply(lambda x: all(elem in sleep_stages for elem in x))]\n",
    "\n",
    "    #######################################\n",
    "    # 2.2. Extract sequence length and time\n",
    "    #######################################\n",
    "    \"\"\"\n",
    "    Extract sequence length of all lines and time of each line\n",
    "    \"\"\"\n",
    "    loaded_data['length'] = loaded_data['data'].apply(lambda x: len(x))\n",
    "\n",
    "    \n",
    "    #######################################\n",
    "    # 2.4. Normalize data\n",
    "    #######################################\n",
    "    \"\"\"\n",
    "    Normalize data to [0,1] using MinMaxScaler algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    if norm_enable == True:\n",
    "        loaded_data['data']=MinMaxNormalizer(loaded_data['data'])\n",
    "    \n",
    "\n",
    "    #######################################\n",
    "    # 2.5. Padding\n",
    "    #######################################\n",
    "    \"\"\"\n",
    "    Padding data to given length\n",
    "    \"\"\"\n",
    "   \n",
    "    # Question: Current padding value is 0, is it ok? Do we need it or just resample?\n",
    "    data_info = {\n",
    "        'length' : len(loaded_data),\n",
    "        'max_length' : max(loaded_data['length']),\n",
    "        'paddding_value' : padding_value,\n",
    "\n",
    "    }\n",
    "    \n",
    "    loaded_data['data'] = loaded_data['data'].apply(lambda x: np.transpose(x))\n",
    "    prep_data = pd.DataFrame(columns=['time','data'])\n",
    "    for i in tqdm(range(data_info.length)):\n",
    "        #create empty array with padding value\n",
    "        tmp_array = np.empty([data_info.max_length,1])\n",
    "        tmp_array.fill(padding_value)\n",
    "        #fill array with data\n",
    "        tmp_array[:loaded_data['data'][i].shape[0],:loaded_data['data'][i].shape[1]] = loaded_data['data'][i]\n",
    "        #append to prep_data\n",
    "        prep_data.append(tmp_array)\n",
    "\n",
    "    return prep_data.to_numpy(),loaded_data['time_data'].to_numpy(),data_info\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def MinMaxNormalizer(data,min_value=1,max_value=5):\n",
    "    numerator = data-min_value\n",
    "    denominator = max_value-min_value\n",
    "    norm_data = numerator/denominator\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code directory:\t\t\tc:\\Users\\tomo9\\Documents\\00_School\\00_Thesis\\HypnoGAN\n",
      "Data directory:\t\t\tc:\\Users\\tomo9\\Documents\\00_School\\00_Thesis\\HypnoGAN\\Data\n",
      "Output directory:\t\tc:\\Users\\tomo9\\Documents\\00_School\\00_Thesis\\HypnoGAN\\Output\\test\n",
      "Using CUDA\n",
      "\n",
      "Run HYPNOG-7 received stop signal. Exiting\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "All 0 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/NTLAB/HypnoGAN/e/HYPNOG-7/metadata\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# Experiment Arguments\n",
    "#######################################\n",
    "\n",
    "# select device:\n",
    "device = \"cuda\"\n",
    "\n",
    "#set random seed\n",
    "seed = 0\n",
    "\n",
    "#experiment name\n",
    "exp_name =\"test\"\n",
    "\n",
    "#normalization enable\n",
    "norm_enable = False\n",
    "\n",
    "# padding value\n",
    "padding_value = -1.0\n",
    "\n",
    "# Train\n",
    "is_train = True\n",
    "\n",
    "# dataset save:\n",
    "# Full: save full dataset\n",
    "# Limited: save limited dataset\n",
    "# None: don't save dataset\n",
    "save_dataset = \"None\"\n",
    "\n",
    "# save arguments to neptune\n",
    "run[\"Initialization/Arguments/ExperimentArgs/device\"] = device\n",
    "run[\"Initialization/Arguments/ExperimentArgs/seed\"] = seed\n",
    "run[\"Initialization/Arguments/ExperimentArgs/exp_name\"] = exp_name\n",
    "run[\"Initialization/Arguments/ExperimentArgs/norm_enable\"] = norm_enable\n",
    "run[\"Initialization/Arguments/ExperimentArgs/padding_value\"] = padding_value\n",
    "run[\"Initialization/Arguments/ExperimentArgs/is_train\"] = is_train\n",
    "run[\"Initialization/Arguments/ExperimentArgs/save_dataset\"] = save_dataset\n",
    "\n",
    "\n",
    "#######################################\n",
    "# Data Arguments\n",
    "#######################################\n",
    "\n",
    "#data limit for testing\n",
    "# None: use full dataset\n",
    "# int: use limited dataset\n",
    "data_limit = 2\n",
    "\n",
    "#train test split rate\n",
    "train_rate = 0.6\n",
    "# save arguments to neptune\n",
    "run[\"Initialization/Arguments/DataArgs/data_limit\"] = data_limit\n",
    "run[\"Initialization/Arguments/DataArgs/train_rate\"] = train_rate\n",
    "\n",
    "#######################################\n",
    "# Model Arguments\n",
    "#######################################\n",
    "\n",
    "# embedding model epochs\n",
    "emb_epochs = 600\n",
    "\n",
    "# GAN model epochs\n",
    "gan_epochs = 600\n",
    "\n",
    "# supervised model epochs\n",
    "sup_epochs = 600\n",
    "\n",
    "# batch size\n",
    "batch_size = 64\n",
    "\n",
    "# hidden dimension of RNN\n",
    "hidden_dim = 20\n",
    "\n",
    "# number of layers in RNN\n",
    "num_layers = 3\n",
    "\n",
    "# discriminator threshold\n",
    "dis_thresh = 0.15\n",
    "\n",
    "#learning rate\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "# save arguments to neptune\n",
    "run[\"Initialization/Arguments/ModelArgs/embedding_epochs\"] = emb_epochs\n",
    "run[\"Initialization/Arguments/ModelArgs/gan_epochs\"] = gan_epochs\n",
    "run[\"Initialization/Arguments/ModelArgs/supervised_epochs\"] = sup_epochs\n",
    "run[\"Initialization/Arguments/ModelArgs/batch_size\"] = batch_size\n",
    "run[\"Initialization/Arguments/ModelArgs/hidden_dim\"] = hidden_dim\n",
    "run[\"Initialization/Arguments/ModelArgs/num_layers\"] = num_layers\n",
    "run[\"Initialization/Arguments/ModelArgs/discriminator_threshold\"] = dis_thresh\n",
    "run[\"Initialization/Arguments/ModelArgs/learning_rate\"] = lr\n",
    "\n",
    "\n",
    "\n",
    "######################################\n",
    "# Initialize output directories\n",
    "######################################\n",
    "## runtime directory\n",
    "code_dir = os.path.abspath(\".\")\n",
    "if not os.path.exists(code_dir):\n",
    "    raise ValueError(f\"Code directory not found at {code_dir}\")\n",
    "\n",
    "## Data directory\n",
    "data_path = os.path.abspath(\"./Data\")\n",
    "if not os.path.exists(data_path):\n",
    "    raise ValueError(f\"Data directory not found at {data_path}\")\n",
    "data_dir = os.path.dirname(data_path)\n",
    "data__file_name = os.path.basename(data_path)\n",
    "## Output directory\n",
    "model_path = os.path.abspath(f\"./Output/{exp_name}/\")\n",
    "out_dir = os.path.abspath(model_path)\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir,exist_ok=True)\n",
    "\n",
    "print(f\"Code directory:\\t\\t\\t{code_dir}\")\n",
    "print(f\"Data directory:\\t\\t\\t{data_path}\")\n",
    "print(f\"Output directory:\\t\\t{out_dir}\")\n",
    "\n",
    " ######################################\n",
    "# Initialize random seed and CUDA\n",
    "######################################\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if device == \"cuda\" and torch.cuda.is_available():\n",
    "    print(\"Using CUDA\\n\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    print(\"Using CPU\\n\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "######################################\n",
    "# Load and preprocess data for model\n",
    "######################################\n",
    "X,T,loaded_data_info = preprocess_data(padding_value=padding_value,data_limit=data_limit,save_dataset=save_dataset,norm_enable=norm_enable)\n",
    "print(f\"Processed Data: {X.shape} (Idx x Max_Sequence_Length x Features(=1))\")\n",
    "print(f\"Original data preview:\\n{X[:2, :10, :2]}\\n\")\n",
    "feature_dim = X.shape[-1]\n",
    "Z_dim = X.shape[-1]\n",
    "max_seq_len = loaded_data_info.max_length\n",
    " # Train-Test Split data and time\n",
    " # TODO: Same people should be in the same pool at train test split\n",
    " # Make the split on the subject ID's, so also have to att ID to the loaded data\n",
    "train_data, test_data, train_time, test_time = train_test_split(\n",
    "    X, T, test_size=train_rate, random_state=seed\n",
    ")\n",
    "#########################\n",
    "# Initialize and Run model\n",
    "#########################\n",
    " # Log start time\n",
    "start = time.time()\n",
    "model = TimeGAN(feature_dim=feature_dim,  hidden_dim=hidden_dim, num_layers=num_layers, max_seq_len=max_seq_len,padding=padding_value)\n",
    "if is_train == True:\n",
    "    timegan_trainer(model,train_data,train_time,batch_size=batch_size,lr=lr,emb_epochs=emb_epochs,sup_epochs=sup_epochs,max_seq_length=max_seq_len,Z_dim=Z_dim,dis_thresh=dis_thresh,device=device,model_path=model_path)\n",
    "generated_data = timegan_generator(model, train_time, model_path=model_path,batch_size=batch_size,max_seq_length=max_seq_len,Z_dim=Z_dim,device=device)\n",
    "generated_time = train_time\n",
    "# Log end time\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Generated data preview:\\n{generated_data[:2, -10:, :2]}\\n\")\n",
    "print(f\"Model Runtime: {(end - start)/60} mins\\n\")\n",
    "# Save splitted data and generated data\n",
    "with open(f\"{model_path}/train_data.pickle\", \"wb\") as fb:\n",
    "    pickle.dump(train_data, fb)\n",
    "with open(f\"{model_path}/train_time.pickle\", \"wb\") as fb:\n",
    "    pickle.dump(train_time, fb)\n",
    "with open(f\"{model_path}/test_data.pickle\", \"wb\") as fb:\n",
    "    pickle.dump(test_data, fb)\n",
    "with open(f\"{model_path}/test_time.pickle\", \"wb\") as fb:\n",
    "    pickle.dump(test_time, fb)\n",
    "with open(f\"{model_path}/fake_data.pickle\", \"wb\") as fb:\n",
    "    pickle.dump(generated_data, fb)\n",
    "with open(f\"{model_path}/fake_time.pickle\", \"wb\") as fb:\n",
    "    pickle.dump(generated_time, fb)\n",
    "\n",
    "# Stop Neptune experiment\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
